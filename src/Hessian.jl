"""
Gradient of the joint posterior probability of the latent variables

The gradient is computed for the m-th trial of the i-th trialset, the t-th timestep in that trialet, and for accumulator state iáµƒ and coupling variable state iá¶œ:
    âˆ‡p(a = Î¾(iáµƒ), c = iá¶œ âˆ£ ğ˜áµ¢â‚˜, dáµ¢â‚˜)

The recommended dataset should have a few number of trials

ARGUMENT
-`i`: trialset
-`m`: trial
-`t`: time step
-`iáµƒ`: index of the accumulator
-`iá¶œ`: index of the coupling variable
-`model`: structure containing the data, parameters, and hyperparameters of a factorial hidden-Markov drift-diffusion model

RETURN
-a vector corresponding to the gradient of the posterior probability
"""
function âˆ‡posterior(i::Integer, m::Integer, t::Integer, iáµƒ::Integer, iá¶œ::Integer, model::Model)
    @unpack options, Î¸native, Î¸real, trialsets = model
	@unpack Î, K = options
	trialinvariant = Trialinvariant(model; purpose="gradient")
	pğ˜ğ‘‘=map(model.trialsets) do trialset
			map(trialset.trials) do trial
				map(1:trial.ntimesteps) do t
					ones(T,Î,K)
				end
			end
		end
    likelihood!(pğ˜ğ‘‘, trialsets, Î¸native.Ïˆ[1]) # `pğ˜ğ‘‘` is the conditional likelihood p(ğ˜â‚œ, d âˆ£ aâ‚œ, zâ‚œ)
	âˆ‡posterior(pğ˜ğ‘‘[i][m], mpGLMs, trialinvariant, Î¸native, trialsets[i].trials[m])
end

"""
Compute the gradient of the posterior probabilities for each time step in one trial

RETURN
-a nested array whose element [t][i][j][q] corresponds the t-th time step, j-th accumulator state, j-th coupling state, and q-th parameter
"""
function âˆ‡posterior(glmÎ¸s::Vector{<:GLMÎ¸}, Î¸native::LatentÎ¸, trial::Trial, trialinvariant::Trialinvariant)
	@unpack clicks = trial
	@unpack inputtimesteps, inputindex = clicks
	@unpack Aáµƒsilent, dAáµƒsilentdÎ¼, dAáµƒsilentdÏƒÂ², dAáµƒsilentdB, Aá¶œ, Aá¶œáµ€, Î”t, K, ğ›š, Ï€á¶œáµ€, Î, ğ› = trialinvariant
	dâ„“dk, dâ„“dÎ», dâ„“dÏ•, dâ„“dÏƒÂ²â‚, dâ„“dÏƒÂ²â‚›, dâ„“dB = 0., 0., 0., 0., 0., 0.
	âˆ‘Ï‡á¶œ = zeros(T, K,K)
	Î¼ = Î¸native.Î¼â‚€[1] + trial.previousanswer*Î¸native.wâ‚•[1]
	Ïƒ = âˆšÎ¸native.ÏƒÂ²áµ¢[1]
	Ï€áµƒ, dÏ€áµƒdÎ¼, dÏ€áµƒdÏƒÂ², dÏ€áµƒdB = zeros(T, Î), zeros(T, Î), zeros(T, Î), zeros(T, Î)
	probabilityvector!(Ï€áµƒ, dÏ€áµƒdÎ¼, dÏ€áµƒdÏƒÂ², dÏ€áµƒdB, Î¼, ğ›š, Ïƒ, ğ›)
	n_steps_with_input = length(clicks.inputtimesteps)
	Aáµƒ = map(x->zeros(T, Î,Î), clicks.inputtimesteps)
	dAáµƒdÎ¼ = map(x->zeros(T, Î,Î), clicks.inputtimesteps)
	dAáµƒdÏƒÂ² = map(x->zeros(T, Î,Î), clicks.inputtimesteps)
	dAáµƒdB = map(x->zeros(T, Î,Î), clicks.inputtimesteps)
	Î”c = zeros(T, n_steps_with_input)
	âˆ‘c = zeros(T, n_steps_with_input)
	C, dCdk, dCdÏ• = âˆ‡adapt(clicks, Î¸native.k[1], Î¸native.Ï•[1])
	for i in 1:n_steps_with_input
		t = clicks.inputtimesteps[i]
		cL = sum(C[clicks.left[t]])
		cR = sum(C[clicks.right[t]])
		stochasticmatrix!(Aáµƒ[i], dAáµƒdÎ¼[i], dAáµƒdÏƒÂ²[i], dAáµƒdB[i], cL, cR, trialinvariant, Î¸native)
		Î”c[i] = cR-cL
		âˆ‘c[i] = cL+cR
	end

	pğ˜ğ‘‘, âˆ‚pğ˜ğ‘‘_âˆ‚w, âˆ‚pğ˜ğ‘‘_âˆ‚Ïˆ = âˆ‡conditionalikelihood(choice, glmÎ¸s, Î¸native.Ïˆ[1], trial.spiketrainmodels, trialinvariant) #to write

	D, f = forward(Aáµƒ, inputindex, Ï€áµƒ, pğ˜ğ‘‘, trialinvariant)
end

"""
	âˆ‡conditionallikelihood(glmÎ¸s, spiketrainmodels, trialinvariant)

Gradient of the conditional likelihood of the spiking of simultaneously recorded neurons

ARGUMENT
-`glmÎ¸s`: a vector whose each element contains the parameters of the Poisson mixture generalized linear model of a neuron's spike train. Each element corresponds to a neuron.
-`spiketrainmodels`: a vector whose element contains one trial's data of the Poisson mixture generalized linear model of a neuron's spike train. Each element corresponds to a neuron.
-`trialinvariant`: a structure containing quantities that are used in each trial

RETURN
-`âˆ‡pğ˜`: partial derivatives; element âˆ‡pğ˜[q][t][j,k] corresponds to the partial derivative of the product of the likelihood of all neurons' spike count at time step t conditioned on the accumulator in the j-th state and the coupling in the k-th state, with respect to the q-th parameter. The parameters of the GLMs of all neurons are concatenated.
-`pğ˜`: conditional likelihood; element pğ˜[t][j,k] corresponds to the product of the likelihood of all neurons' spike count at time step t conditioned on the accumulator in the j-th state and the coupling in the k-th state

EXAMPLE
```julia-repl
julia> using FHMDDM
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_04_01_test/data.mat")
julia> trialinvariant = Trialinvariant(model)
julia> glmÎ¸s = map(glm->glm.Î¸, model.trialsets[1].mpGLMs)
julia> âˆ‡pğ˜, pğ˜ = FHMDDM.âˆ‡conditionallikelihood(glmÎ¸s, model.trialsets[1].trials[1].spiketrainmodels, trialinvariant)
```
"""
function âˆ‡conditionallikelihood(glmÎ¸s::Vector{<:GLMÎ¸},
							    spiketrainmodels::Vector{<:SpikeTrainModel},
							    trialinvariant::Trialinvariant)
	@unpack Î”t, K, Î = trialinvariant
	ğ› = (2collect(1:Î) .- Î .- 1)./(Î-1) # normalized
	ntimesteps = length(spiketrainmodels[1].ğ²)
	nneurons = length(spiketrainmodels)
	nğ® = length(glmÎ¸s[1].ğ®)
	nğ¯ = length(glmÎ¸s[1].ğ¯)
	nparameters_per_neuron = nğ®+nğ¯+1
	nparameters = nneurons*nparameters_per_neuron
	pğ˜ = map(t->ones(Î,K), 1:ntimesteps)
	âˆ‡pğ˜ = map(q->map(t->zeros(Î,K), 1:ntimesteps), 1:nparameters)
	for n = 1:nneurons
		fğ› = map(Î¾->transformaccumulator(glmÎ¸s[n].b[1], Î¾), ğ›)
		âˆ‚fğ›_âˆ‚b = map(Î¾->dtransformaccumulator(glmÎ¸s[n].b[1], Î¾), ğ›)
		index1 = (n-1)*nparameters_per_neuron+1
		indicesğ® = index1 : index1+nğ®-1
		indicesğ¯ = index1+nğ® : index1+nğ®+nğ¯-1
		indexb = index1+nğ®+nğ¯
		ğš½ğ¯ = spiketrainmodels[n].ğš½ * glmÎ¸s[n].ğ¯
		for t=1:ntimesteps
			for i = 1:nğ®
				q = indicesğ®[i]
				âˆ‡pğ˜[q][t] .= spiketrainmodels[n].ğ”[t,i]
			end
			for i = 1:nğ¯
				q = indicesğ¯[i]
				âˆ‡pğ˜[q][t][:,1] .= spiketrainmodels[n].ğš½[t,i].*fğ›
			end
			âˆ‡pğ˜[indexb][t][:,1] .= ğš½ğ¯[t].*âˆ‚fğ›_âˆ‚b
		end
		indicesbefore = 1:index1-1
		indices = index1:index1+nparameters_per_neuron-1
		indicesafter = indices[end]+1:nparameters
		for j = 1:Î
			for k = 1:K
				ğ—ğ° = spiketrainmodels[n].ğ” * glmÎ¸s[n].ğ®
				(k == 1) && (ğ—ğ° .+= ğš½ğ¯.*fğ›[j])
				for t=1:ntimesteps
					âˆ‚pğ‘¦â‚™â‚œ_âˆ‚Xw, pğ‘¦â‚™â‚œ = dPoissonlikelihood(Î”t, ğ—ğ°[t], spiketrainmodels[n].ğ²[t])
					pğ˜[t][j,k] *= pğ‘¦â‚™â‚œ
					for q in indicesbefore
						âˆ‡pğ˜[q][t][j,k] *= pğ‘¦â‚™â‚œ
					end
					for q in indices
						âˆ‡pğ˜[q][t][j,k] *= âˆ‚pğ‘¦â‚™â‚œ_âˆ‚Xw
					end
					for q in indicesafter
						âˆ‡pğ˜[q][t][j,k] *= pğ‘¦â‚™â‚œ
					end
				end
			end
		end
	end
	return âˆ‡pğ˜, pğ˜
end

"""
	compareâˆ‡pğ˜(model)

Compare the automatically computed and hand coded gradient of the conditional likelihood of the population spiking

ARGUMENT
-`model`: a structure containing the data, parametes, and hyperparameters of a factorial hidden-Markov drift-diffusion model

RETURN
-a vector whose each element shows the maximum absolute difference between the two partial derivatives with respect to each parameter.

EXAMPLE
```julia-repl
using FHMDDM
model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_04_01_test/data.mat")
maxabsdiff = FHMDDM.compareâˆ‡pğ˜(model)
```
"""
function compareâˆ‡pğ˜(model::Model)
	@unpack trialsets = model
	@unpack Î”t, K, Î = model.options
	trialinvariant = Trialinvariant(model)
	maxbsdiff = 0.0
	glmÎ¸s = map(glm->glm.Î¸, model.trialsets[1].mpGLMs)
	concatenatedÎ¸ = zeros(0)
	for n in eachindex(glmÎ¸s)
		concatenatedÎ¸ = vcat(concatenatedÎ¸, glmÎ¸s[n].ğ®, glmÎ¸s[n].ğ¯, glmÎ¸s[n].b)
	end
	nparameters = length(concatenatedÎ¸)
	automaticgradient = similar(concatenatedÎ¸)
	maxabsdiff = zeros(nparameters)
	for m in eachindex(model.trialsets[1].trials)
		trial = model.trialsets[1].trials[m]
		âˆ‡pğ˜, pğ˜ = âˆ‡conditionallikelihood(glmÎ¸s, trial.spiketrainmodels, trialinvariant)
		for t = 1:trial.ntimesteps
			for j = 1:Î
				for k = 1:K
					f(x) = conditionallikelihood(Î”t,j,k,trial.spiketrainmodels,t,Î,x)
					automaticgradient = ForwardDiff.gradient!(automaticgradient, f, concatenatedÎ¸)
					for q=1:nparameters
						maxabsdiff[q] = max(maxabsdiff[q], abs(automaticgradient[q] - âˆ‡pğ˜[q][t][j,k]))
					end
				end
			end
		end
	end
	maxabsdiff
end
