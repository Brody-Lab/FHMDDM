"""
    test(datapath)

Run a number of tests on the model

ARGUMENT
-`datapath`: full path of the data file
"""
function test(datapath::String; maxabsdiff::Real=1e-8)
	println("testing `"*datapath*"`")
	printseparator()
	println("testing cross-validation and saving results in `cvresults.mat`")
	model = Model(datapath)
	cvresults = crossvalidate(2, model)
	printseparator()
	println("saving cross-validation results in `cvresults.mat`")
	save(cvresults, model.options)
    printseparator()
    println("testing the hessian of the expectation of the log-likelihood of one neuron's spike train")
    println("	- `expectation_of_âˆ‡âˆ‡loglikelihood(Î³, mpGLM, x)`")
    println("	- used for parameter initialization")
    test_expectation_of_âˆ‡âˆ‡loglikelihood!(datapath; maxabsdiff=maxabsdiff)
    printseparator()
    println("testing the gradient of the expectation of the log-likelihood of one neuron's spike train")
    println("	- 'expectation_âˆ‡loglikelihood!(âˆ‡Q, Î³, mpGLM)'")
    println("	- used for learning of the parameters of the full model")
    test_expectation_âˆ‡loglikelihood!(datapath; maxabsdiff=maxabsdiff)
    printseparator()
    println("testing the gradient of log-likelihood of all the data")
    println("	- 'âˆ‡negativeloglikelihood!(âˆ‡nâ„“, memory, model, concatenatedÎ¸)'")
    test_âˆ‡negativeloglikelihood!(datapath; maxabsdiff=maxabsdiff)
	printseparator()
    println("testing the gradient of log-posterior conditioned on all the data")
	println("	- `âˆ‡negativelogposterior(model)`")
	test_âˆ‡negativelogposterior(datapath; maxabsdiff=maxabsdiff)
	printseparator()
	println("testing hessian of the log-likelihood of all the data")
	println("	- `âˆ‡âˆ‡loglikelihood(model)`")
	test_âˆ‡âˆ‡loglikelihood(datapath; maxabsdiff=maxabsdiff)
    printseparator()
    println("testing gradient of log evidence of all the data")
	println("	- `âˆ‡logevidence(model)`")
    test_âˆ‡logevidence(datapath; maxabsdiff=maxabsdiff, simulate=false)
    printseparator()
    println("testing the hessian of the log-likelihood of only the behavioral choices")
	println("	- `âˆ‡âˆ‡choiceLL(model)`")
    test_âˆ‡âˆ‡choiceLL(datapath; maxabsdiff=maxabsdiff)
	printseparator()
	println("testing parameter initialization")
	initializeparameters!(Model(datapath))
	printseparator()
	println("testing maximum likelihood estimation")
	maximizelikelihood!(Model(datapath), Optim.LBFGS(linesearch = LineSearches.BackTracking()))
	printseparator()
	println("testing maximum a posteriori estimation")
	maximizeposterior!(Model(datapath))
	printseparator()
	println("testing evidence optimization")
	maximizeevidence!(Model(datapath))
    printseparator()
    println("testing saving model summary and predictions in `test.mat`")
	analyzeandsave(Model(datapath); prefix="test")
	printseparator()
	println("tests completed")
    return nothing
end

"""
	printseparator()

Write a standardized separator
"""
printseparator() = print("\n---------\n---------\n\n")

"""
    test_expectation_of_âˆ‡âˆ‡loglikelihood(datapath)

Check the hessian and gradient of the expectation of the log-likelihood of one neuron's GLM.

The function being checked is used in parameter initialization.
"""
function test_expectation_of_âˆ‡âˆ‡loglikelihood!(datapath::String; maxabsdiff::Real=1e-8)
    model = Model(datapath)
    mpGLM = model.trialsets[1].mpGLMs[1]
    Î³ = FHMDDM.randomposterior(mpGLM; rng=MersenneTwister(1234))
    xâ‚€ = concatenateparameters(mpGLM.Î¸; initialization=true)
    nparameters = length(xâ‚€)
    fhand, ghand, hhand = fill(NaN,1), fill(NaN,nparameters),
    fill(NaN,nparameters,nparameters)
    FHMDDM.expectation_of_âˆ‡âˆ‡loglikelihood!(fhand, ghand, hhand, Î³, mpGLM)
    f(x) = FHMDDM.expectation_of_loglikelihood(Î³, mpGLM, x; initialization=true);
    fauto = f(xâ‚€);
    gauto = ForwardDiff.gradient(f, xâ‚€);
    hauto = ForwardDiff.hessian(f, xâ‚€);
	absÎ”Q = abs(fauto - fhand[1])
	maxabsÎ”âˆ‡Q = maximum(abs.(gauto .- ghand))
	maxabsÎ”âˆ‡âˆ‡Q = maximum(abs.(hauto .- hhand))
    println("   |Î”Q|: ", absÎ”Q)
    println("   max(|Î”gradient|): ", maxabsÎ”âˆ‡Q)
    println("   max(|Î”hessian|): ", maxabsÎ”âˆ‡âˆ‡Q)
	if (absÎ”Q > maxabsdiff) || (maxabsÎ”âˆ‡Q > maxabsdiff) || (maxabsÎ”âˆ‡âˆ‡Q > maxabsdiff)
		error("Maxmimum absolute difference exceeded")
	end
	return nothing
end

"""
    test_expectation_âˆ‡loglikelihood(datapath)

Check the gradient of the expectation of the log-likelihood of one neuron's GLM.

The function being checked is used in computing the gradient of the log-likelihood of the entire model.
"""
function test_expectation_âˆ‡loglikelihood!(datapath::String; maxabsdiff::Real=1e-8)
    model = Model(datapath)
    mpGLM = model.trialsets[1].mpGLMs[1]
    if length(mpGLM.Î¸.b) > 0
        while abs(mpGLM.Î¸.b[1]) < 1e-4
            mpGLM.Î¸.b[1] = 1 - 2rand()
        end
    end
    Î³ = FHMDDM.randomposterior(mpGLM; rng=MersenneTwister(1234))
    âˆ‡Q = FHMDDM.GLMÎ¸(mpGLM.Î¸, eltype(mpGLM.Î¸.ð®))
    FHMDDM.expectation_âˆ‡loglikelihood!(âˆ‡Q, Î³, mpGLM)
    ghand = FHMDDM.concatenateparameters(âˆ‡Q)
    concatenatedÎ¸ = FHMDDM.concatenateparameters(mpGLM.Î¸)
    f(x) = FHMDDM.expectation_of_loglikelihood(Î³, mpGLM, x)
    gauto = ForwardDiff.gradient(f, concatenatedÎ¸)
	maxabsÎ”âˆ‡Q = maximum(abs.(gauto .- ghand))
    println("   max(|Î”gradient|): ", maxabsÎ”âˆ‡Q)
	if maxabsÎ”âˆ‡Q > maxabsdiff
		error("Maxmimum absolute difference exceeded")
	end
	return nothing
end

"""
    test_âˆ‡negativeloglikelihood!(datapath)

Check the gradient of the negative of the log-likelihood of the model
"""
function test_âˆ‡negativeloglikelihood!(datapath::String; maxabsdiff::Real=1e-8)
    model = Model(datapath)
    for trialset in model.trialsets
        for mpGLM in trialset.mpGLMs
            if length(mpGLM.Î¸.b) > 0
                while abs(mpGLM.Î¸.b[1]) < 1e-4
                    mpGLM.Î¸.b[1] = 1 - 2rand()
                end
            end
        end
    end
    concatenatedÎ¸, indexÎ¸ = concatenateparameters(model)
    âˆ‡nâ„“ = similar(concatenatedÎ¸)
    memory = FHMDDM.Memoryforgradient(model)
    FHMDDM.âˆ‡negativeloglikelihood!(âˆ‡nâ„“, memory, model, concatenatedÎ¸)
    f(x) = -FHMDDM.loglikelihood(x, indexÎ¸, model)
    â„“_auto = f(concatenatedÎ¸)
    âˆ‡nâ„“_auto = ForwardDiff.gradient(f, concatenatedÎ¸)
	absÎ”â„“ = abs(â„“_auto + memory.â„“[1])
	maxabsÎ”âˆ‡â„“ = maximum(abs.(âˆ‡nâ„“_auto .- âˆ‡nâ„“))
    println("   |Î”â„“|: ", absÎ”â„“)
    println("   max(|Î”âˆ‡â„“|): ", maxabsÎ”âˆ‡â„“)
	if (absÎ”â„“ > maxabsdiff) || (maxabsÎ”âˆ‡â„“ > maxabsdiff)
		error("Maxmimum absolute difference exceeded")
	end
end

"""
    test_âˆ‡âˆ‡loglikelihood(datapath)

Check the computation of the hessian of the log-likelihood
"""
function test_âˆ‡âˆ‡loglikelihood(datapath::String; maxabsdiff::Real=1e-8)
    model = Model(datapath)
    for trialset in model.trialsets
        for mpGLM in trialset.mpGLMs
            if length(mpGLM.Î¸.b) > 0
                while abs(mpGLM.Î¸.b[1]) < 1e-4
                    mpGLM.Î¸.b[1] = 1 - 2rand()
                end
            end
        end
    end
	concatenatedÎ¸, indexÎ¸ = FHMDDM.concatenateparameters(model)
	â„“hand, âˆ‡hand, âˆ‡âˆ‡hand = FHMDDM.âˆ‡âˆ‡loglikelihood(model)
	f(x) = FHMDDM.loglikelihood(x, indexÎ¸, model)
	â„“auto = f(concatenatedÎ¸)
	âˆ‡auto = ForwardDiff.gradient(f, concatenatedÎ¸)
	âˆ‡âˆ‡auto = ForwardDiff.hessian(f, concatenatedÎ¸)
	absÎ”â„“ = abs(â„“auto-â„“hand)
	maxabsÎ”âˆ‡â„“ = maximum(abs.(âˆ‡auto .- âˆ‡hand))
	maxabsÎ”âˆ‡âˆ‡â„“ = maximum(abs.(âˆ‡âˆ‡auto .- âˆ‡âˆ‡hand))
	println("   |Î”â„“|: ", absÎ”â„“)
    println("   max(|Î”âˆ‡â„“|): ", maxabsÎ”âˆ‡â„“)
    println("   max(|Î”âˆ‡âˆ‡â„“|): ", maxabsÎ”âˆ‡âˆ‡â„“)
	if (absÎ”â„“ > maxabsdiff) || (maxabsÎ”âˆ‡â„“ > maxabsdiff) || (maxabsÎ”âˆ‡âˆ‡â„“ > maxabsdiff)
		error("Maxmimum absolute difference exceeded")
	end
end

"""
	test_âˆ‡negativelogposterior(model)

Compare the hand-computed and automatically-differentiated gradients

ARGUMENT
-`model`: a structure containing the data, parameters, and hyperparameters of a factorial hidden-Markov drift-diffusion model
```
"""
function test_âˆ‡negativelogposterior(datapath::String; maxabsdiff::Real=1e-8)
	model = Model(datapath)
	for trialset in model.trialsets
        for mpGLM in trialset.mpGLMs
            if length(mpGLM.Î¸.b) > 0
                while abs(mpGLM.Î¸.b[1]) < 1e-4
                    mpGLM.Î¸.b[1] = 1 - 2rand()
                end
            end
        end
    end
	concatenatedÎ¸, indexÎ¸ = FHMDDM.concatenateparameters(model)
	memory = FHMDDM.Memoryforgradient(model)
	â„“hand = FHMDDM.logposterior!(model, memory, concatenatedÎ¸)
	âˆ‡hand = similar(concatenatedÎ¸)
	FHMDDM.âˆ‡negativelogposterior!(âˆ‡hand, model, memory, concatenatedÎ¸)
	f(x) = FHMDDM.logposterior(x, indexÎ¸, model)
	â„“auto = f(concatenatedÎ¸)
	âˆ‡auto = ForwardDiff.gradient(f, concatenatedÎ¸)
	absÎ”ð¿ = abs(â„“auto-â„“hand)
	maxabsÎ”âˆ‡ð¿ = maximum(abs.(âˆ‡auto .+ âˆ‡hand))
	println("   |Î”(ð¿)|: ", absÎ”ð¿)
    println("   max(|Î”(âˆ‡ð¿)|): ", maxabsÎ”âˆ‡ð¿)
	if (absÎ”ð¿ > maxabsdiff) || (maxabsÎ”âˆ‡ð¿ > maxabsdiff)
		error("Maxmimum absolute difference exceeded")
	end
end

"""
	test_âˆ‡logevidence(model)

Check whether the hand-coded gradient of the log-evidence matches the automatic gradient

ARGUMENT
-`datapath`: path to the file containing the data and model settings

OPTIONAL ARGUMENT
-`simulate`: whether to simulate Hessian and MAP solution. If not, the model is first fitted before a Hessian is computed

RETURN
-maximum absolute normalized difference between the gradients
-maximum absolute normalized difference between the log-evidence functions
"""
function test_âˆ‡logevidence(datapath::String; maxabsdiff::Real=1e-8, simulate::Bool=false)
    model = Model(datapath)
	@unpack ð›‚, indexð€, indexðš½, ðš½ = model.gaussianprior
	ð›‰, indexð›‰ = FHMDDM.concatenateparameters(model)
	ð± = 1 .- 2rand(length(ð›‚))
	FHMDDM.real2native!(model.gaussianprior, ð±)
	FHMDDM.precisionmatrix!(model.gaussianprior)
	if simulate
		N_ðš½ = length(indexðš½)
		ð‘ = 1 .- 2rand(N_ðš½,N_ðš½)
		ðâ‚€ = transpose(ð‘)*ð‘ # simulate a positive-definite Hessian of the posterior
		ð‡ = ðš½ - ðâ‚€
		ð°â‚€ = 1 .- 2rand(N_ðš½)
		ð°â‚€ ./= norm(ð°â‚€)
		ðâ‚€ð°â‚€ = ðâ‚€*ð°â‚€
		for i in eachindex(indexðš½)
			ð›‰[indexðš½[i]] = ð°â‚€[i]
		end
		FHMDDM.sortparameters!(model, ð›‰, indexð›‰)
		FHMDDM.real2native!(model.Î¸native, model.options, model.Î¸real)
	else
		FHMDDM.initializeparameters!(model; show_trace=false)
		FHMDDM.maximizeposterior!(model; show_trace=false);
		ð‡ = FHMDDM.âˆ‡âˆ‡loglikelihood(model)[3][indexðš½, indexðš½]
		ð°â‚€ = FHMDDM.concatenateparameters(model)[1][indexðš½]
		ðâ‚€ð°â‚€ = (ðš½-ð‡)*ð°â‚€
		ð± = 1 .- 2rand(length(ð›‚)) # only if we are not simulating because changing the hyperparameters might make the Hessian of the posterior not positive-definite
	end
	memory = FHMDDM.Memoryforgradient(model)
	handcoded_evidence = FHMDDM.logevidence!(memory, model, ð›‰, ðâ‚€ð°â‚€, ð‡, ð±)
	handcoded_gradient = fill(NaN,length(ð±))
	âˆ‡nâ„“ = similar(ð›‰)
	FHMDDM.âˆ‡negativelogevidence!(memory, model, handcoded_gradient, âˆ‡nâ„“, ð›‰, ðâ‚€ð°â‚€, ð‡, ð±)
    f(x) = FHMDDM.logevidence(ðâ‚€ð°â‚€, ð‡, model, x)
	automatic_evidence = f(ð±)
	automatic_gradient = ForwardDiff.gradient(f, ð±)
	absdiffâˆ‡ = abs.((automatic_gradient .+ handcoded_gradient))
	absreldiffâˆ‡ = abs.((automatic_gradient .+ handcoded_gradient))./automatic_gradient
	absÎ”ð¸ = abs(automatic_evidence-handcoded_evidence)
	maxabsÎ”âˆ‡ð¸ = maximum(min.(absdiffâˆ‡, absreldiffâˆ‡))
    println("   |Î”ð¸|: ", absÎ”ð¸)
    println("   max(|Î”âˆ‡ð¸|): ", maxabsÎ”âˆ‡ð¸)
	if (absÎ”ð¸ > maxabsdiff) || (maxabsÎ”âˆ‡ð¸ > maxabsdiff)
		error("Maxmimum absolute difference exceeded")
	end
end

"""
	test_âˆ‡âˆ‡choiceLL(model)

Compare the automatically computed and hand-coded gradients and hessians with respect to the parameters being fitted in their real space

ARGUMENT
-`datapath`: path to the file containing the data and model settings

RETURN
-`absdiffâ„“`: absolute difference in the log-likelihood evaluted using the algorithm bein automatically differentiated and the hand-coded algorithm
-`absdiffâˆ‡`: absolute difference in the gradients
-`absdiffâˆ‡âˆ‡`: absolute difference in the hessians
```
"""
function test_âˆ‡âˆ‡choiceLL(datapath::String; maxabsdiff::Real=1e-8)
	model = Model(datapath)
	â„“hand, âˆ‡hand, âˆ‡âˆ‡hand = FHMDDM.âˆ‡âˆ‡choiceLL(model)
	concatenatedÎ¸, indexÎ¸ = FHMDDM.concatenate_choice_related_parameters(model)
	if isempty(concatenatedÎ¸)
		println("   drift-diffusion parameters are not being fitted")
	else
		indexð›‚ = FHMDDM.choice_related_precisions(model)[2]
		f(x) = FHMDDM.choiceLL(x, indexÎ¸.latentÎ¸, model)
		â„“auto = f(concatenatedÎ¸)
		âˆ‡auto = ForwardDiff.gradient(f, concatenatedÎ¸)
		âˆ‡âˆ‡auto = ForwardDiff.hessian(f, concatenatedÎ¸)
		absÎ”â„“ = abs(â„“auto-â„“hand)
		maxabsÎ”âˆ‡â„“ = maximum(abs.(âˆ‡auto .- âˆ‡hand[indexð›‚]))
		maxabsÎ”âˆ‡âˆ‡â„“ = maximum(abs.(âˆ‡âˆ‡auto .- âˆ‡âˆ‡hand[indexð›‚,indexð›‚]))
		println("   |Î”(â„“)|: ", absÎ”â„“)
	    println("   max(|Î”(âˆ‡â„“)|): ", maxabsÎ”âˆ‡â„“)
	    println("   max(|Î”(âˆ‡âˆ‡â„“)|): ", maxabsÎ”âˆ‡âˆ‡â„“)
		if (absÎ”â„“ > maxabsdiff) || (maxabsÎ”âˆ‡â„“ > maxabsdiff) || (maxabsÎ”âˆ‡âˆ‡â„“ > maxabsdiff)
			error("Maxmimum absolute difference exceeded")
		end
	end
end
