"""
"""
function maximize_choice_evidence!(model;
								iterations::Int = 500,
								max_consecutive_failures::Int=2,
								outer_iterations::Int=10,
								verbose::Bool=true,
								g_tol::Real=1e-6,
								x_reltol::Real=1e-1)
	memory = Memoryforgradient(model)
	bestğ›‰, indexğ›‰ = concatenate_choice_related_parameters(model)
	bestğ¸ = -Inf
	ğ›‚ = drift_diffusion_precisions(model)
	bestğ›‚ = copy(ğ›‚)
	n_consecutive_failures = 0
	posteriorconverged = false
	for i = 1:outer_iterations
	    results = maximize_choice_posterior!(model; iterations=iterations, g_tol=g_tol)[3]
		if !Optim.converged(results)
			if Optim.iteration_limit_reached(results)
				new_Î± = min(100.0, 2geomean(model.gaussianprior.ğ›‚))
				ğ›‚ .= new_Î±
				verbose && println("Outer iteration: ", i, ": because the maximum number of iterations was reached, the values of the precisions are set to be twice the geometric mean of the hyperparameters. New ğ›‚  â†’ ", new_Î±)
			else
				verbose && println("Outer iteration: ", i, ": because of a line search failure, Gaussian noise is added to the parameter values")
				ğ›‰ = concatenate_choice_related_parameters(model)[1]
				ğ›‰ .+= randn(length(ğ›‰))
				sortparameters!(model, ğ›‰, indexğ›‰.latentÎ¸)
			end
		else
			verbose && println("Outer iteration: ", i, ": the MAP values of the parameters converged")
			ğ›‰â‚€ = concatenate_choice_related_parameters(model)[1] # exact posterior mode
			stats = @timed âˆ‡âˆ‡choiceLL(model)[indexğš½, indexğš½] # not sure how to replace `indexğš½` yet; I think it will depend on how I compute the Hessian
			ğ‡ = stats.value
			verbose && println("Outer iteration: ", i, ": computing the Hessian of the log-likelihood took ", stats.time, " seconds")
			ğ¸ = logevidence!(memory, model, ğ‡, ğ›‰â‚€)
			if ğ¸ > bestğ¸
				if verbose
					if posteriorconverged
						println("Outer iteration: ", i, ": the log-evidence (best: ", bestğ¸, "; new:", ğ¸, ") is improved by the new values of the precisions found in the previous outer iteration")
					else
						println("Outer iteration: ", i, ": initial value of log-evidence: ", ğ¸, " is set as the best log-evidence")
					end
				end
				bestğ¸ = ğ¸
				bestğ›‚ .= model.gaussianprior.ğ›‚
			 	bestğ¬ .= model.gaussianprior.ğ¬
				bestğ›‰ .= ğ›‰â‚€
				n_consecutive_failures = 0
			else
				n_consecutive_failures += 1
				verbose && println("Outer iteration: ", i, ": because the log-evidence (best: ", bestğ¸, "; new:", ğ¸, ") was not improved by the new precisions, subsequent learning of the precisions will be begin at the midpoint between the current values of the precisions and the values that gave the best evidence so far.")
				for j in eachindex(model.gaussianprior.ğ›‚)
					model.gaussianprior.ğ›‚[j] = (model.gaussianprior.ğ›‚[j] + bestğ›‚[j])/2
				end
				for j in eachindex(model.gaussianprior.ğ¬)
					model.gaussianprior.ğ¬[j] = (model.gaussianprior.ğ¬[j] + bestğ¬[j])/2
				end
			end
			posteriorconverged = true
			if n_consecutive_failures == max_consecutive_failures
				verbose && println("Outer iteration: ", i, ": optimization halted early due to ", max_consecutive_failures, " consecutive failures in improving evidence")
				break
			end
			normÎ” = maximizeevidence!(memory, model, ğ‡, ğ›‰â‚€)
			if verbose
				println("Outer iteration ", i, ": new ğ›‚ â†’ ", model.gaussianprior.ğ›‚)
				println("Outer iteration ", i, ": new ğ¬ â†’ ", model.gaussianprior.ğ¬)
			end
			if normÎ” < x_reltol
				verbose && println("Outer iteration: ", i, ": optimization halted after relative difference in the norm of the hyperparameters (in real space) decreased below ", x_reltol)
				break
			else
				sortparameters!(model, ğ›‰â‚€, indexğ›‰)
			end
		end
		if (i==outer_iterations) && verbose
			println("Optimization halted after reaching the last of ", outer_iterations, " allowed outer iterations.")
		end
	end
	println("Best log-evidence: ", bestğ¸)
	println("Best shrinkage coefficients: ", bestğ›‚)
	println("Best smoothing coefficients: ", bestğ¬)
	println("Best parameters: ", bestğ›‰)
	precisionmatrix!(model.gaussianprior, bestğ›‚, bestğ¬)
	sortparameters!(model, bestğ›‰, indexğ›‰.latentÎ¸)
	return nothing
end

"""
	drift_diffusion_precisions(model)

Concatenate the precisions of the priors on each drift-diffusion parameter that is being fit

ARGUMENT
-`model`: structure containing the data, hyperparameters, and parameters

RETURN
-a vector concatenating the precisions of the priors on the drift-diffusion parameters that are being fit
"""
function drift_diffusion_precisions(model::Model)
	concatenated_drift_diffusion_Î¸, indexÎ¸ = concatenate_choice_related_parameters(model)
	ğ›‚ = similar(concatenated_drift_diffusion_Î¸)
	k = 0
	for parametername in fieldnames(LatentÎ¸)
		if parametername == :Aá¶œâ‚â‚ || parametername == :Aá¶œâ‚‚â‚‚ || parametername == :Ï€á¶œâ‚
 		elseif getfield(indexÎ¸.latentÎ¸, parametername)[1] > 0
			k = k + 1
			ğ›‚[k] = model.gaussianprior.ğ›‚[k]
		end
	end
	return ğ›‚
end

"""
	maximize_choice_posterior!(model)

Learn the parameters that maximize the L2-regularized log-likelihood of the behavioral choices

OPTIONAL ARGUMENT
-`extended_trace`: save additional information
-`f_tol`: threshold for determining convergence in the objective value
-`g_tol`: threshold for determining convergence in the gradient
-`iterations`: number of inner iterations that will be run before the algorithm gives up
-`outer_iterations`: number of outer iterations that will be run before the algorithm gives up
-`show_every`: trace output is printed every `show_every`th iteration.
-`show_trace`: should a trace of the optimization algorithm's state be shown?
-`x_tol`: threshold for determining convergence in the input vector

RETURN
-results from the optimization assembled by the Optim module


EXAMPLE
```julia-repl
julia> using FHMDDM
julia> datapath = "/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_05_07a/T176_2018_05_03/data.mat"
julia> model = Model(datapath)
julia> FHMDDM.maximize_choice_posterior!(model)
```
"""
function maximize_choice_posterior!(model::Model;
		                 extended_trace::Bool=true,
		                 f_tol::AbstractFloat=1e-8,
		                 g_tol::AbstractFloat=1e-8,
		                 iterations::Integer=1000,
		                 show_every::Integer=10,
		                 show_trace::Bool=true,
		                 x_tol::AbstractFloat=1e-8)
	@unpack Î±â‚€_choices = model.options
	memory = Memoryforgradient(model; choicemodel=true)
    f(concatenatedÎ¸) = -choiceLL!(memory, model, concatenatedÎ¸) + Î±â‚€_choices*dot(concatenatedÎ¸,concatenatedÎ¸)
	function g!(âˆ‡, concatenatedÎ¸)
		âˆ‡negativechoiceLL!(âˆ‡, memory, model, concatenatedÎ¸)
		âˆ‡ .+= Î±â‚€_choices.*concatenatedÎ¸
		return nothing
	end
    Optim_options = Optim.Options(extended_trace=extended_trace,
								  f_tol=f_tol,
                                  g_tol=g_tol,
                                  iterations=iterations,
                                  show_every=show_every,
                                  show_trace=show_trace,
                                  x_tol=x_tol)
	algorithm = LBFGS(linesearch = LineSearches.BackTracking())
	Î¸â‚€ = concatenate_choice_related_parameters(model)[1]
	optimizationresults = Optim.optimize(f, g!, Î¸â‚€, algorithm, Optim_options)
    println(optimizationresults)
	Î¸â‚˜â‚— = Optim.minimizer(optimizationresults)
	sortparameters!(model, Î¸â‚˜â‚—, memory.indexÎ¸.latentÎ¸)
	real2native!(model.Î¸native, model.options, model.Î¸real)
	return optimizationresults
end

"""
	maximizechoiceLL!(model)

Learn the parameters that maximize the log-likelihood of the behavioral choices

OPTIONAL ARGUMENT
-see documentation of 'maximize_choice_posterior'
"""
function maximizechoiceLL!(model::Model;
		                 extended_trace::Bool=true,
		                 f_tol::AbstractFloat=1e-9,
		                 g_tol::AbstractFloat=1e-8,
		                 iterations::Integer=1000,
		                 show_every::Integer=10,
		                 show_trace::Bool=true,
		                 x_tol::AbstractFloat=1e-5)
	memory = Memoryforgradient(model; choicemodel=true)
    f(concatenatedÎ¸) = -choiceLL!(memory, model, concatenatedÎ¸)
    g!(âˆ‡, concatenatedÎ¸) = âˆ‡negativechoiceLL!(âˆ‡, memory, model, concatenatedÎ¸)
    Optim_options = Optim.Options(extended_trace=extended_trace,
								  f_tol=f_tol,
                                  g_tol=g_tol,
                                  iterations=iterations,
                                  show_every=show_every,
                                  show_trace=show_trace,
                                  x_tol=x_tol)
	algorithm = LBFGS(linesearch = LineSearches.BackTracking())
	Î¸â‚€ = concatenate_choice_related_parameters(model)[1]
	optimizationresults = Optim.optimize(f, g!, Î¸â‚€, algorithm, Optim_options)
    println(optimizationresults)
	Î¸â‚˜â‚— = Optim.minimizer(optimizationresults)
	sortparameters!(model, Î¸â‚˜â‚—, memory.indexÎ¸.latentÎ¸)
	real2native!(model.Î¸native, model.options, model.Î¸real)
end

"""
	choiceLL!(memory, model, concatenatedÎ¸)

Log-likelihood of the choices

MODIFIED ARGUMENT
-`model`: an instance of FHM-DDM
-`memory`: a container of variables used by both the log-likelihood and gradient computation

UNMODIFIED ARGUMENT
-`concatenatedÎ¸`: a vector of concatenated parameter values

RETURN
-log-likelihood
"""
function choiceLL!(memory::Memoryforgradient,
					model::Model,
					concatenatedÎ¸::Vector{<:Real})
	if concatenatedÎ¸ != memory.concatenatedÎ¸
		P = update_for_choiceLL!(memory, model, concatenatedÎ¸)
		memory.â„“[1] = 0.0
		@inbounds for trialset in model.trialsets
			for trial in trialset.trials
				choiceLL!(memory, P, model.Î¸native, trial)
			end
		end
	end
	memory.â„“[1]
end

"""
	choiceLL!(memory, P, Î¸native, trial)

Log-likelihood of the choice in a single trial

MODIFIED ARGUMENT
-`memory`: memory allocated for computing the gradient. The log-likelihood is updated.
-`P`: a structure containing allocated memory for computing the accumulator's initial and transition probabilities as well as the partial derivatives of these probabilities

UNMODIFIED ARGUMENT
-`Î¸native`: parameters of the latent variables in native space
-`trial`: a struct containing the stimuli and behavioral response of one trial

RETURN
-log-likelihood
"""
function choiceLL!(memory::Memoryforgradient, P::Probabilityvector, Î¸native::LatentÎ¸, trial::Trial)
	@unpack clicks = trial
	@unpack Aáµƒinput, Aáµƒsilent, â„“, Ï€á¶œáµ€ = memory
	priorprobability!(P, trial.previousanswer)
	f = copy(P.ğ›‘)
	if length(clicks.time) > 0
		adaptedclicks = adapt(clicks, Î¸native.k[1], Î¸native.Ï•[1])
	end
	@inbounds for t = 2:trial.ntimesteps
		if isempty(clicks.inputindex[t])
			Aáµƒ = Aáµƒsilent
		else
			Aáµƒ = Aáµƒinput[clicks.inputindex[t][1]]
			update_for_transition_probabilities!(P, adaptedclicks, clicks, t)
			transitionmatrix!(Aáµƒ, P)
		end
		f = Aáµƒ*f
	end
	conditional_probability_of_choice!(f, trial.choice, Î¸native.Ïˆ[1])
	â„“[1] += log(sum(f))
	return nothing
end

"""
	choiceLL(concatenatedÎ¸, indexÎ¸, model)

ForwardDiff-compatible computation of the log-likelihood of the choices

MODIFIED ARGUMENT
-`model`: an instance of FHM-DDM
-`memory`: a container of variables used by both the log-likelihood and gradient computation

UNMODIFIED ARGUMENT
-`concatenatedÎ¸`: a vector of concatenated parameter values

RETURN
-log-likelihood

EXAMPLE
```julia-repl
julia> using FHMDDM
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_05_05_test/data.mat"; randomize=true);
julia> concatenatedÎ¸, indexÎ¸ = FHMDDM.concatenate_choice_related_parameters(model)
julia> â„“ = FHMDDM.choiceLL(concatenatedÎ¸, indexÎ¸.latentÎ¸, model)
julia> memory = FHMDDM.Memoryforgradient(model; choicemodel=true)
julia> â„“2 = FHMDDM.choiceLL!(memory, model, concatenatedÎ¸) #ForwardDiff-incompatible
julia> abs(â„“2-â„“)
```
"""
function choiceLL(concatenatedÎ¸::Vector{T}, indexÎ¸::LatentÎ¸, model::Model) where {T<:Real}
	model = Model(concatenatedÎ¸, indexÎ¸, model)
	@unpack options, Î¸native, Î¸real, trialsets = model
	@unpack Î”t, minpa, Î = options
	Aáµƒinput, Aáµƒsilent = zeros(T,Î,Î), zeros(T,Î,Î)
	expÎ»Î”t = exp(Î¸native.Î»[1]*Î”t)
	dÎ¼_dÎ”c = differentiate_Î¼_wrt_Î”c(Î”t, Î¸native.Î»[1])
	dğ›_dB = (2 .*collect(1:Î) .- Î .- 1)./(Î-2)
	ğ› = Î¸native.B[1].*dğ›_dB
	transitionmatrix!(Aáµƒsilent, minpa, expÎ»Î”t.*ğ›, âˆš(Î”t*Î¸native.ÏƒÂ²â‚[1]), ğ›)
	â„“ = zero(T)
	@inbounds for s in eachindex(trialsets)
		for m in eachindex(trialsets[s].trials)
			trial = trialsets[s].trials[m]
			f = probabilityvector(minpa, Î¸native.Î¼â‚€[1]+Î¸native.wâ‚•[1]*trial.previousanswer, âˆšÎ¸native.ÏƒÂ²áµ¢[1], ğ›)
			if length(trial.clicks.time) > 0
				adaptedclicks = adapt(trial.clicks, Î¸native.k[1], Î¸native.Ï•[1])
			end
			for t=2:trial.ntimesteps
				if t âˆˆ trial.clicks.inputtimesteps
					cL = sum(adaptedclicks.C[trial.clicks.left[t]])
					cR = sum(adaptedclicks.C[trial.clicks.right[t]])
					ğ› = expÎ»Î”t.*ğ› .+ (cR-cL)*dÎ¼_dÎ”c
					Ïƒ = âˆš((cR+cL)*Î¸native.ÏƒÂ²â‚›[1] + Î”t*Î¸native.ÏƒÂ²â‚[1])
					transitionmatrix!(Aáµƒinput, minpa, ğ›, Ïƒ, ğ›)
					Aáµƒ = Aáµƒinput
				else
					Aáµƒ = Aáµƒsilent
				end
				f = Aáµƒ*f
			end
			conditional_probability_of_choice!(f, trial.choice, Î¸native.Ïˆ[1])
			â„“+=log(sum(f))
		end
	end
	â„“
end


"""
	âˆ‡negativechoiceLL!(âˆ‡nâ„“, memory, model, concatenatedÎ¸)

Update the gradient of the negative log-likelihood of choices

MODIFIED ARGUMENT
-`âˆ‡nâ„“`: the vector representing the gradient
-`memory`: memory allocated for computing the gradient. The log-likelihood is updated.
-`model`: structure containing the data, parameters, and hyperparameters of the model

ARGUMENT
-`concatenatedÎ¸`: values of the model's choice-related parameters concatenated into a vector

EXAMPLE
```julia-repl
julia> using FHMDDM
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_05_21_test/T176_2018_05_03/data.mat");
julia> concatenatedÎ¸, indexÎ¸ = FHMDDM.concatenate_choice_related_parameters(model)
julia> âˆ‡nâ„“ = similar(concatenatedÎ¸)
julia> memory = FHMDDM.Memoryforgradient(model; choicemodel=true)
julia> FHMDDM.âˆ‡negativechoiceLL!(âˆ‡nâ„“, memory, model, concatenatedÎ¸)
julia> using ForwardDiff
julia> f(x) = -FHMDDM.choiceLL(x, indexÎ¸.latentÎ¸, model)
julia> âˆ‡nâ„“_auto = ForwardDiff.gradient(f, concatenatedÎ¸)
julia> maximum(abs.(âˆ‡nâ„“_auto .- âˆ‡nâ„“))

julia> using FHMDDM
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_05_21_test/T176_2018_05_03/data.mat");
julia> concatenatedÎ¸, indexÎ¸ = FHMDDM.concatenate_choice_related_parameters(model)
julia> concatenatedÎ¸ = rand(length(concatenatedÎ¸))
julia> â„“ = FHMDDM.choiceLL(concatenatedÎ¸, indexÎ¸.latentÎ¸, model)
julia> âˆ‡nâ„“ = similar(concatenatedÎ¸)
julia> memory = FHMDDM.Memoryforgradient(model; choicemodel=true)
julia> FHMDDM.âˆ‡negativechoiceLL!(âˆ‡nâ„“, memory, model, concatenatedÎ¸)
julia> â„“ = FHMDDM.choiceLL(concatenatedÎ¸, indexÎ¸.latentÎ¸, model)
julia> abs(â„“ - memory.â„“[1])
```

```compare speeds of automatic and hand-coded gradients
julia> using FHMDDM, ForwardDiff, BenchmarkTools
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_05_07a/T176_2018_05_03/data.mat"; randomize=true);
julia> concatenatedÎ¸, indexÎ¸ = FHMDDM.concatenate_choice_related_parameters(model);
julia> memory = FHMDDM.Memoryforgradient(model; choicemodel=true);
julia> ghand!(âˆ‡, concatenatedÎ¸) = FHMDDM.âˆ‡negativechoiceLL!(âˆ‡, memory, model, concatenatedÎ¸);
julia> f(x) = FHMDDM.choiceLL(x, indexÎ¸.latentÎ¸, model);
julia> gauto!(âˆ‡, x) = ForwardDiff.gradient!(âˆ‡, f, x);
julia> g1, g2 = similar(concatenatedÎ¸), similar(concatenatedÎ¸);
julia> ghand!(g1, concatenatedÎ¸);
julia> ghand!(g2, concatenatedÎ¸);
julia> maximum(abs.(g1.-g2))
julia> @benchmark ghand!(g1, concatenatedÎ¸) #4.6s
julia> @benchmark gauto!(g2, concatenatedÎ¸) #9.2s
```
"""
function âˆ‡negativechoiceLL!(âˆ‡nâ„“::Vector{<:Real},
							memory::Memoryforgradient,
							model::Model,
						    concatenatedÎ¸::Vector{<:Real})
	if concatenatedÎ¸ != memory.concatenatedÎ¸
		P = update_for_choiceLL!(memory, model, concatenatedÎ¸)
	else
		P = Probabilityvector(model.options.Î”t, model.options.minpa, model.Î¸native, model.options.Î)
	end
	âˆ‡choiceLL!(memory,model,P)
	indexall = 0
	indexfit = 0
	for field in fieldnames(LatentÎ¸)
		indexall+=1
		if (getfield(memory.indexÎ¸.latentÎ¸, field)[1] > 0) && (field != :Aá¶œâ‚â‚) && (field != :Aá¶œâ‚‚â‚‚) && (field != :Ï€á¶œâ‚)
			indexfit +=1
			âˆ‡nâ„“[indexfit] = -memory.âˆ‡â„“latent[indexall]
		end
	end
	native2real!(âˆ‡nâ„“, memory.indexÎ¸.latentÎ¸, model)
end

"""
	âˆ‡choiceLL!(memory, model, P)

Update the gradient of the log-likelihood of the choices across trialsets

MODIFIED ARGUMENT
-`memory`: memory allocated for computing the gradient. The log-likelihood is updated.
-`P`: a structure containing allocated memory for computing the accumulator's initial and transition probabilities as well as the partial derivatives of these probabilities

UNMODIFIED ARGUMENT
-`Î¸native`: parameters of the latent variables in native space
-`trial`: a struct containing the stimuli and behavioral response of one trial
"""
function âˆ‡choiceLL!(memory::Memoryforgradient, model::Model, P::Probabilityvector)
	memory.â„“ .= 0.0
	memory.âˆ‡â„“latent .= 0.0
	@inbounds for trialset in model.trialsets
		for trial in trialset.trials
			âˆ‡choiceLL!(memory, P, model.Î¸native, trial)
		end
	end
	return nothing
end

"""
	âˆ‡choiceLL!(memory, P, Î¸native, trial)

Update the gradient of the log-likelihood of the choice in one trial

MODIFIED ARGUMENT
-`memory`: memory allocated for computing the gradient. The log-likelihood is updated.
-`P`: a structure containing allocated memory for computing the accumulator's initial and transition probabilities as well as the partial derivatives of these probabilities

UNMODIFIED ARGUMENT
-`Î¸native`: parameters of the latent variables in native space
-`trial`: a struct containing the stimuli and behavioral response of one trial
"""
function âˆ‡choiceLL!(memory::Memoryforgradient,
					P::Probabilityvector,
					Î¸native::LatentÎ¸,
					trial::Trial)
	@unpack clicks = trial
	@unpack inputtimesteps, inputindex = clicks
	@unpack Aáµƒinput, âˆ‡Aáµƒinput, Aáµƒsilent, âˆ‡Aáµƒsilent, D, f, indexÎ¸_paâ‚, indexÎ¸_paâ‚œaâ‚œâ‚‹â‚, indexÎ¸_Ïˆ, â„“, âˆ‡â„“latent, nÎ¸_paâ‚, nÎ¸_paâ‚œaâ‚œâ‚‹â‚, âˆ‡paâ‚, Î = memory
	t = 1
	âˆ‡priorprobability!(âˆ‡paâ‚, P, trial.previousanswer)
	f[t] .= P.ğ›‘
	if length(clicks.time) > 0
		adaptedclicks = âˆ‡adapt(trial.clicks, Î¸native.k[1], Î¸native.Ï•[1])
	end
	@inbounds for t=2:trial.ntimesteps
		if t âˆˆ clicks.inputtimesteps
			clickindex = clicks.inputindex[t][1]
			Aáµƒ = Aáµƒinput[clickindex]
			âˆ‡Aáµƒ = âˆ‡Aáµƒinput[clickindex]
			update_for_âˆ‡transition_probabilities!(P, adaptedclicks, clicks, t)
			âˆ‡transitionmatrix!(âˆ‡Aáµƒ, Aáµƒ, P)
		else
			Aáµƒ = Aáµƒsilent
		end
		f[t] = Aáµƒ * f[t-1]
	end
	pğ‘‘_a = conditional_probability_of_choice(trial.choice, Î¸native.Ïˆ[1], Î)
	pğ‘‘ = dot(pğ‘‘_a, f[trial.ntimesteps])
	â„“[1] += log(pğ‘‘)
	b = pğ‘‘_a./pğ‘‘ # backward term for the last time step
	Î³ = b.*f[trial.ntimesteps] # posterior probability for the last time step
	âˆ‡â„“latent[indexÎ¸_Ïˆ[1]] += expectation_derivative_logpğ‘‘_wrt_Ïˆ(trial.choice, Î³, Î¸native.Ïˆ[1])
	@inbounds for t = trial.ntimesteps:-1:1
		if t < trial.ntimesteps
			if t+1 âˆˆ clicks.inputtimesteps
				clickindex = clicks.inputindex[t+1][1]
				Aáµƒâ‚œâ‚Šâ‚ = Aáµƒinput[clickindex]
			else
				Aáµƒâ‚œâ‚Šâ‚ = Aáµƒsilent
			end
			b = transpose(Aáµƒâ‚œâ‚Šâ‚) * b
		end
		if t > 1
			if t âˆˆ clicks.inputtimesteps
				clickindex = clicks.inputindex[t][1]
				âˆ‡Aáµƒ = âˆ‡Aáµƒinput[clickindex]
			else
				âˆ‡Aáµƒ = âˆ‡Aáµƒsilent
			end
			for i = 1:nÎ¸_paâ‚œaâ‚œâ‚‹â‚
				âˆ‡â„“latent[indexÎ¸_paâ‚œaâ‚œâ‚‹â‚[i]] += (transpose(b)*âˆ‡Aáµƒ[i]*f[t-1])[1]
			end
		end
	end
	@inbounds for i = 1:nÎ¸_paâ‚
		âˆ‡â„“latent[indexÎ¸_paâ‚[i]] += dot(b, âˆ‡paâ‚[i])
	end
	return nothing
end

"""
    conditional_probability_of_choice(f, choice, Ïˆ)

Probability of a choice conditioned on the accumulator state

ARGUMENT
-`choice`: the observed choice, either right (`choice`=true) or left.
-`Ïˆ`: the prior probability of a lapse state

RETURN
`p`: conditional probability of the choice
"""
function conditional_probability_of_choice(choice::Bool, Ïˆ::T, Î::Integer) where {T<:Real}
	p = zeros(T, Î)
	zeroindex = cld(Î,2)
    p[zeroindex] = 0.5
    if choice
        p[1:zeroindex-1]   .= Ïˆ/2
        p[zeroindex+1:end] .= 1-Ïˆ/2
    else
        p[1:zeroindex-1]   .= 1-Ïˆ/2
        p[zeroindex+1:end] .= Ïˆ/2
    end
	p
end

"""
    conditional_probability_of_choice!(f, choice, Ïˆ)

Probability of a choice conditioned on the accumulator state

MODIFIED ARGUMENT
-`f`: the forward term

ARGUMENT
-`choice`: the observed choice, either right (`choice`=true) or left.
-`Ïˆ`: the prior probability of a lapse state
"""
function conditional_probability_of_choice!(f::Array{<:Real}, choice::Bool, Ïˆ::Real)
	Î = length(f)
	zeroindex = cld(Î,2)
    f[zeroindex] *= 0.5
    if choice
        f[1:zeroindex-1]   .*= Ïˆ/2
        f[zeroindex+1:end] .*= 1-Ïˆ/2
    else
        f[1:zeroindex-1]   .*= 1-Ïˆ/2
        f[zeroindex+1:end] .*= Ïˆ/2
    end
    return nothing
end

"""
	update_for_choiceLL!(model, memory, concatenatedÎ¸)

Update the model and the memory quantities according to new parameter values

MODIFIED ARGUMENT
-`memory`: structure containing variables memory between computations of the model's log-likelihood and its gradient
-`model`: structure with information concerning a factorial hidden Markov drift-diffusion model

ARGUMENT
-`concatenatedÎ¸`: newest values of the model's parameters

RETURN
-`P`: an instance of `Probabilityvector`

EXAMPLE
```julia-repl
julia> using FHMDDM
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_04_27_test/data.mat"; randomize=true);
julia> memory, P = FHMDDM.Memoryforgradient(model; choicemodel=true)
julia> P = update_for_choiceLL!(model, memory, rand(length(memory.concatenatedÎ¸)))
"""
function update_for_choiceLL!(memory::Memoryforgradient,
							 model::Model,
							 concatenatedÎ¸::Vector{<:Real})
	memory.concatenatedÎ¸ .= concatenatedÎ¸
	sortparameters!(model, memory.concatenatedÎ¸, memory.indexÎ¸.latentÎ¸)
	real2native!(model.Î¸native, model.options, model.Î¸real)
	@unpack options, Î¸native = model
	@unpack Î”t, K, minpa, Î = options
	P = Probabilityvector(Î”t, minpa, Î¸native, Î)
	update_for_âˆ‡transition_probabilities!(P)
	âˆ‡transitionmatrix!(memory.âˆ‡Aáµƒsilent, memory.Aáµƒsilent, P)
	return P
end


"""
	update_drift_diffusion_transformation(model)

Update the transformation of the drift-diffusion parameters between real and native spaces.

Specifically, the hyperparameter that specifies the value of each drift-diffusion parameter in native space that corresponds to its value of zero in real space is updated.

ARGUMENT
-`model`: structure containing, the settings and hyperparameters of the model

RETURN
-`model`: structure containing the new settings and hyperparameters of the model

EXAMPLE
```julia-repl
julia> using FHMDDM
julia> datapath = "/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_06_28b_test/T176_2018_05_03_b5K1K1/data.mat"
julia> model = Model(datapath)
julia> newmodel = FHMDDM.update_drift_diffusion_transformation(model)
```
"""
function update_drift_diffusion_transformation(model::Model)
	if model.options.updateDDtransformation
		dict = dictionary(model.options)
		dict["lqu_B"][2] = model.Î¸native.B[1]
		dict["lqu_k"][2] = model.Î¸native.k[1]
		dict["lqu_lambda"][2] = model.Î¸native.Î»[1]
		dict["lqu_mu0"][2] = model.Î¸native.Î¼â‚€[1]
		dict["lqu_phi"][2] = model.Î¸native.Ï•[1]
		dict["lqu_psi"][2] = model.Î¸native.Ïˆ[1]
		dict["lqu_sigma2_a"][2] = model.Î¸native.ÏƒÂ²â‚[1]
		dict["lqu_sigma2_i"][2] = model.Î¸native.ÏƒÂ²áµ¢[1]
		dict["lqu_sigma2_s"][2] = model.Î¸native.ÏƒÂ²â‚›[1]
		dict["lqu_w_h"][2] = model.Î¸native.wâ‚•[1]
		Model(options=Options(dict),
			gaussianprior=model.gaussianprior,
			Î¸native = model.Î¸native,
			Î¸real = model.Î¸real,
			Î¸â‚€native = model.Î¸â‚€native,
			trialsets = model.trialsets)
	else
		model
	end
end

"""
	check_âˆ‡âˆ‡choiceLL(model)

Compare the automatically computed and hand-coded gradients and hessians with respect to the parameters being fitted in their real space

ARGUMENT
-`model`: a structure containing the data, parameters, and hyperparameters of a factorial hidden-Markov drift-diffusion model

RETURN
-`absdiffâ„“`: absolute difference in the log-likelihood evaluted using the algorithm bein automatically differentiated and the hand-coded algorithm
-`absdiffâˆ‡`: absolute difference in the gradients
-`absdiffâˆ‡âˆ‡`: absolute difference in the hessians

EXAMPLE
```julia-repl
julia> using FHMDDM, ForwardDiff
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_07_06a_test/T176_2018_05_03_b5K1K1/data.mat")
julia> absdiffâ„“, absdiffâˆ‡, absdiffâˆ‡âˆ‡ = FHMDDM.check_âˆ‡âˆ‡choiceLL(model)
julia> println("   max(|Î”loss|): ", absdiffâ„“)
julia> println("   max(|Î”gradient|): ", maximum(absdiffâˆ‡))
julia> println("   max(|Î”hessian|): ", maximum(absdiffâˆ‡âˆ‡))
julia>
```
"""
function check_âˆ‡âˆ‡choiceLL(model::Model)
	concatenatedÎ¸, indexÎ¸ = concatenate_choice_related_parameters(model)
	â„“hand, âˆ‡hand, âˆ‡âˆ‡hand = âˆ‡âˆ‡choiceLL(model)
	f(x) = choiceLL(x, indexÎ¸.latentÎ¸, model)
	â„“auto = f(concatenatedÎ¸)
	âˆ‡auto = ForwardDiff.gradient(f, concatenatedÎ¸)
	âˆ‡âˆ‡auto = ForwardDiff.hessian(f, concatenatedÎ¸)
	return abs(â„“auto-â„“hand), abs.(âˆ‡auto .- âˆ‡hand), abs.(âˆ‡âˆ‡auto .- âˆ‡âˆ‡hand)
end

"""
	âˆ‡âˆ‡choiceLL(model)

Hessian of the log-likelihood of only the choices

ARGUMENT
-`model`: a structure containing the data and hyperparameters of the factorial hidden drift-diffusion model

RETURN
-`â„“`: log-likelihood
-`âˆ‡â„“`: gradient of the log-likelihood with respect to fitted parameters in real space
-`âˆ‡âˆ‡â„“`: Hessian matrix of the log-likelihood with respect to fitted parameters in real space
"""
function âˆ‡âˆ‡choiceLL(model::Model)
	@unpack trialsets = model
	memory = Memory_for_hessian_choiceLL(model)
	for trialset in trialsets
		for trial in trialset.trials
			âˆ‡âˆ‡choiceLL!(memory, model.Î¸native, trial)
		end
	end
	@unpack â„“, âˆ‡â„“, âˆ‡âˆ‡â„“ = memory
	for i = 1:size(âˆ‡âˆ‡â„“,1)
		for j = i+1:size(âˆ‡âˆ‡â„“,2)
			âˆ‡âˆ‡â„“[j,i] = âˆ‡âˆ‡â„“[i,j]
		end
	end
	firstderivatives = differentiate_native_wrt_real(model)
	secondderivatives = differentiate_twice_native_wrt_real(model)
	for i = 1:memory.nÎ¸
		d1 = getfield(firstderivatives, memory.parameternames[i])[1]
		d2 = getfield(secondderivatives, memory.parameternames[i])[1]
		âˆ‡âˆ‡â„“[i,:] .*= d1
		âˆ‡âˆ‡â„“[:,i] .*= d1
		âˆ‡âˆ‡â„“[i,i] += d2*âˆ‡â„“[i]
		âˆ‡â„“[i] *= d1
	end
	return â„“[1], âˆ‡â„“, âˆ‡âˆ‡â„“
end

"""
	 âˆ‡âˆ‡choiceLL!(memory, Î¸native, trial)

Compute the hessian of the log-likelihood of the choice in one trial

MODIFIED ARGUMENT
-`memory`: a structure containing pre-allocated memory for in-place computation and also pre-computed quantities that are identical across trials. The hessian corresponds to the field `âˆ‡âˆ‡â„“` within `memory.`

UNMODIFIED ARGUMENT
-`Î¸native`: values of the parameters controlling the latent varables in their native space
-`trial`: structure containing the auditory stimuli and behavioral choice of one trial
"""
function âˆ‡âˆ‡choiceLL!(memory::Memory_for_hessian_choiceLL, Î¸native::LatentÎ¸, trial::Trial)
	@unpack clicks = trial
	@unpack â„“, âˆ‡â„“, âˆ‡âˆ‡â„“, f, âˆ‡f, âˆ‡D, âˆ‡b, âˆ‚pğ‘‘_âˆ‚Ïˆ, P, âˆ‡paâ‚, âˆ‡âˆ‡paâ‚, indexÎ¸_paâ‚, indexÎ¸_paâ‚œaâ‚œâ‚‹â‚, indexÎ¸_Ïˆ, nÎ¸, nÎ¸_paâ‚, nÎ¸_paâ‚œaâ‚œâ‚‹â‚, nÎ¸_Ïˆ, index_paâ‚_in_Î¸, index_paâ‚œaâ‚œâ‚‹â‚_in_Î¸, index_Ïˆ_in_Î¸, pğ‘‘, âˆ‚pğ‘‘_âˆ‚Ïˆ = memory
	âˆ‡âˆ‡priorprobability!(âˆ‡âˆ‡paâ‚, âˆ‡paâ‚, P, trial.previousanswer)
	f[1] .= P.ğ›‘
	for q = 1:nÎ¸
		i = index_paâ‚_in_Î¸[q]
		if i == 0
			âˆ‡f[1][q] .= 0
		else
			âˆ‡f[1][q] = âˆ‡paâ‚[i]
		end
	end
	adaptedclicks = âˆ‡âˆ‡adapt(clicks, Î¸native.k[1], Î¸native.Ï•[1])
	for t=2:trial.ntimesteps-1
		Aáµƒ, âˆ‡Aáµƒ, âˆ‡âˆ‡Aáµƒ = âˆ‡âˆ‡transitionmatrices!(memory, adaptedclicks, clicks, t)
		f[t] = Aáµƒ * f[t-1]
		for q in eachindex(âˆ‡â„“)
			i = index_paâ‚œaâ‚œâ‚‹â‚_in_Î¸[q]
			if i != 0
				âˆ‡f[t][q] = âˆ‡Aáµƒ[i] * f[t-1] .+ Aáµƒ * âˆ‡f[t-1][q]
			else
				âˆ‡f[t][q] = Aáµƒ * âˆ‡f[t-1][q]
			end
		end
	end
	t = trial.ntimesteps
	Aáµƒ, âˆ‡Aáµƒ, âˆ‡âˆ‡Aáµƒ = âˆ‡âˆ‡transitionmatrices!(memory, adaptedclicks, clicks, t)
	conditional_choice_likelihood!(pğ‘‘, trial.choice, Î¸native.Ïˆ[1])
	differentiate_conditional_choice_likelihood_wrt_Ïˆ!(âˆ‚pğ‘‘_âˆ‚Ïˆ, trial.choice)
	f[t] = pğ‘‘.* (Aáµƒ * f[t-1])
	D = sum(f[t])
	â„“[1] += log(D)
	f[t] ./= D
	for q in eachindex(âˆ‡â„“)
		i_aâ‚œ = index_paâ‚œaâ‚œâ‚‹â‚_in_Î¸[q]
		i_Ïˆ = index_Ïˆ_in_Î¸[q]
		if i_aâ‚œ > 0
			âˆ‡f[t][q] = pğ‘‘ .* (âˆ‡Aáµƒ[i_aâ‚œ] * f[t-1] .+ Aáµƒ * âˆ‡f[t-1][q])
		elseif i_Ïˆ > 0
			âˆ‡f[t][q] = âˆ‚pğ‘‘_âˆ‚Ïˆ .* (Aáµƒ * f[t-1]) .+ pğ‘‘ .* (Aáµƒ * âˆ‡f[t-1][q])
		else
			âˆ‡f[t][q] = pğ‘‘ .* (Aáµƒ * âˆ‡f[t-1][q])
		end
	end
	for i in eachindex(âˆ‡f[t])
		âˆ‡D[i] = sum(âˆ‡f[t][i])
		for j in eachindex(âˆ‡f[t][i])
			âˆ‡f[t][i][j] = (âˆ‡f[t][i][j] - f[t][j]*âˆ‡D[i])/D
		end
	end
	Î³ = f[trial.ntimesteps]
	âˆ‡Î³ = âˆ‡f[trial.ntimesteps]
	q = indexÎ¸_Ïˆ[1]
	âˆ‡â„“[q] += expectation_derivative_logpğ‘‘_wrt_Ïˆ(trial.choice, Î³, Î¸native.Ïˆ[1])
	âˆ‡âˆ‡â„“[q,q] += expectation_second_derivative_logpğ‘‘_wrt_Ïˆ(trial.choice, Î³, Î¸native.Ïˆ[1])
	for r = q:nÎ¸
		âˆ‡âˆ‡â„“[q,r] += expectation_derivative_logpğ‘‘_wrt_Ïˆ(trial.choice, âˆ‡Î³[r], Î¸native.Ïˆ[1])
	end
	for q in eachindex(âˆ‡b)
		âˆ‡b[q] .= 0
	end
	Aáµƒ, âˆ‡Aáµƒ, âˆ‡âˆ‡Aáµƒ = âˆ‡âˆ‡transitionmatrices(memory, adaptedclicks, clicks, t)
	for i = 1:nÎ¸_paâ‚œaâ‚œâ‚‹â‚
		q = indexÎ¸_paâ‚œaâ‚œâ‚‹â‚[i]
		Î· = (pğ‘‘'*âˆ‡Aáµƒ[i]*f[t-1])[1]/D
		âˆ‡â„“[q] += Î·
		for r = q:nÎ¸
			âˆ‡âˆ‡â„“[q,r] += (pğ‘‘'*âˆ‡Aáµƒ[i]*âˆ‡f[t-1][r])[1]/D - Î·/D*âˆ‡D[r]
			j = index_paâ‚œaâ‚œâ‚‹â‚_in_Î¸[r]
			if j > 0
				âˆ‡âˆ‡â„“[q,r] += (pğ‘‘'*âˆ‡âˆ‡Aáµƒ[i,j]*f[t-1])[1]/D
			end
			j = index_Ïˆ_in_Î¸[r]
			if j > 0
				âˆ‡âˆ‡â„“[q,r] += (âˆ‚pğ‘‘_âˆ‚Ïˆ'*âˆ‡Aáµƒ[i]*f[t-1])[1]/D
			end
		end
	end
	b = nothing # so that updates of b in inside the for loop is accessible outside of the loop
	for t = trial.ntimesteps-1:-1:1
		Aáµƒâ‚œâ‚Šâ‚, âˆ‡Aáµƒâ‚œâ‚Šâ‚, âˆ‡âˆ‡Aáµƒâ‚œâ‚Šâ‚ = âˆ‡âˆ‡transitionmatrices(memory, adaptedclicks, clicks, t+1)
		Aáµƒâ‚œâ‚Šâ‚áµ€ = transpose(Aáµƒâ‚œâ‚Šâ‚)
		if t == trial.ntimesteps-1
			for q = 1:nÎ¸
				i_aâ‚œ = index_paâ‚œaâ‚œâ‚‹â‚_in_Î¸[q]
				i_Ïˆ = index_Ïˆ_in_Î¸[q]
				if i_Ïˆ != 0
					âˆ‡b[q] = Aáµƒâ‚œâ‚Šâ‚áµ€*(âˆ‚pğ‘‘_âˆ‚Ïˆ./D .-  pğ‘‘./D^2 .*âˆ‡D[q])
				elseif i_aâ‚œ != 0
					âˆ‡b[q] = transpose(âˆ‡Aáµƒâ‚œâ‚Šâ‚[i_aâ‚œ])*(pğ‘‘./D) .-  Aáµƒâ‚œâ‚Šâ‚áµ€*(pğ‘‘./D^2 .*âˆ‡D[q])
				else
					âˆ‡b[q] = -Aáµƒâ‚œâ‚Šâ‚áµ€*(pğ‘‘./D^2 .*âˆ‡D[q])
				end
			end
			b = Aáµƒâ‚œâ‚Šâ‚áµ€*(pğ‘‘./D)
		else
			for q in eachindex(âˆ‡â„“)
				i = index_paâ‚œaâ‚œâ‚‹â‚_in_Î¸[q]
				if i > 0
					âˆ‡b[q] = (transpose(âˆ‡Aáµƒâ‚œâ‚Šâ‚[i])*b) .+ (Aáµƒâ‚œâ‚Šâ‚áµ€*âˆ‡b[q])
				else
					âˆ‡b[q] = Aáµƒâ‚œâ‚Šâ‚áµ€*âˆ‡b[q]
				end
			end
			b = Aáµƒâ‚œâ‚Šâ‚áµ€ * b
		end
		if t > 1
			Aáµƒ, âˆ‡Aáµƒ, âˆ‡âˆ‡Aáµƒ = âˆ‡âˆ‡transitionmatrices(memory, adaptedclicks, clicks, t)
			báµ€ = transpose(b)
			for i = 1:nÎ¸_paâ‚œaâ‚œâ‚‹â‚
				q = indexÎ¸_paâ‚œaâ‚œâ‚‹â‚[i]
				âˆ‡â„“[q] += (báµ€*âˆ‡Aáµƒ[i]*f[t-1])[1]
				for r = q:nÎ¸
					âˆ‡âˆ‡â„“[q,r] += (transpose(âˆ‡b[r])*âˆ‡Aáµƒ[i]*f[t-1])[1] + (báµ€*âˆ‡Aáµƒ[i]*âˆ‡f[t-1][r])[1]
					j = index_paâ‚œaâ‚œâ‚‹â‚_in_Î¸[r]
					if j > 0
						âˆ‡âˆ‡â„“[q,r] += (báµ€*âˆ‡âˆ‡Aáµƒ[i,j]*f[t-1])[1]
					end
				end
			end
		end
	end
	for i = 1:nÎ¸_paâ‚
		q = indexÎ¸_paâ‚[i]
		âˆ‡â„“[q] += dot(b, âˆ‡paâ‚[i])
		for r = q:nÎ¸
			âˆ‡âˆ‡â„“[q,r] += dot(âˆ‡b[r], âˆ‡paâ‚[i])
			j = index_paâ‚_in_Î¸[r]
			if j > 0
				âˆ‡âˆ‡â„“[q,r] += dot(b, âˆ‡âˆ‡paâ‚[i,j])
			end
		end
	end
	return nothing
end

"""
	âˆ‡âˆ‡transitionmatrices!(memory, adaptedclicks, clicks, t)

Obtain the transition matrices and their first and second partial derivatives for a time step

If the time step has input, the transition matrix and its derivatives are computed in-place

INPUT
-`memory`: a structure containing the memory used for computing the hessian of the log-likelihood of only the choices
-`adaptedclicks`: a structure containing the information on the post-adaptation strengths of the clicks as well as their first and second derivatives
-`clicks`: a structure containing information on the timing of the auditory inputs
-`t`: time step

OUTPUT
-`Aáµƒ`: transition matrix. Element Aáµƒ[i,j] corresponds to `p(aâ‚œ = Î¾áµ¢ âˆ£ aâ‚œâ‚‹â‚ = Î¾â±¼)`
-`âˆ‡Aáµƒ`: first deriative of the transition matrix. Element âˆ‡Aáµƒ[m][i,j] corresponds to `âˆ‚{p(aâ‚œ = Î¾áµ¢ âˆ£ aâ‚œâ‚‹â‚ = Î¾â±¼)}/âˆ‚Î¸â‚˜`
-`âˆ‡Aáµƒ`: first deriative of the transition matrix. Element âˆ‡âˆ‡Aáµƒ[m,n][i,j] corresponds to `âˆ‚Â²{p(aâ‚œ = Î¾áµ¢ âˆ£ aâ‚œâ‚‹â‚ = Î¾â±¼)}/âˆ‚Î¸â‚˜âˆ‚Î¸â‚™`
"""
function âˆ‡âˆ‡transitionmatrices!(memory::Memory_for_hessian_choiceLL, adaptedclicks::Adaptedclicks, clicks::Clicks, t::Integer)
	@unpack P, Aáµƒsilent, âˆ‡Aáµƒsilent, âˆ‡âˆ‡Aáµƒsilent, Aáµƒinput, âˆ‡Aáµƒinput, âˆ‡âˆ‡Aáµƒinput = memory
	if t âˆˆ clicks.inputtimesteps
		update_for_âˆ‡âˆ‡transition_probabilities!(P, adaptedclicks, clicks, t)
		clickindex = clicks.inputindex[t][1]
		Aáµƒ = Aáµƒinput[clickindex]
		âˆ‡Aáµƒ = âˆ‡Aáµƒinput[clickindex]
		âˆ‡âˆ‡Aáµƒ = âˆ‡âˆ‡Aáµƒinput[clickindex]
		âˆ‡âˆ‡transitionmatrix!(âˆ‡âˆ‡Aáµƒ, âˆ‡Aáµƒ, Aáµƒ, P)
	else
		Aáµƒ = Aáµƒsilent
		âˆ‡Aáµƒ = âˆ‡Aáµƒsilent
		âˆ‡âˆ‡Aáµƒ = âˆ‡âˆ‡Aáµƒsilent
	end
	return Aáµƒ, âˆ‡Aáµƒ, âˆ‡âˆ‡Aáµƒ
end

"""
	âˆ‡âˆ‡transitionmatrices(memory, adaptedclicks, clicks, t)

Obtain the transition matrices and their first and second partial derivatives for a time step

INPUT
-`memory`: a structure containing the memory used for computing the hessian of the log-likelihood of only the choices
-`adaptedclicks`: a structure containing the information on the post-adaptation strengths of the clicks as well as their first and second derivatives
-`clicks`: a structure containing information on the timing of the auditory inputs
-`t`: time step

OUTPUT
-`Aáµƒ`: transition matrix. Element Aáµƒ[i,j] corresponds to `p(aâ‚œ = Î¾áµ¢ âˆ£ aâ‚œâ‚‹â‚ = Î¾â±¼)`
-`âˆ‡Aáµƒ`: first deriative of the transition matrix. Element âˆ‡Aáµƒ[m][i,j] corresponds to `âˆ‚{p(aâ‚œ = Î¾áµ¢ âˆ£ aâ‚œâ‚‹â‚ = Î¾â±¼)}/âˆ‚Î¸â‚˜`
-`âˆ‡Aáµƒ`: first deriative of the transition matrix. Element âˆ‡âˆ‡Aáµƒ[m,n][i,j] corresponds to `âˆ‚Â²{p(aâ‚œ = Î¾áµ¢ âˆ£ aâ‚œâ‚‹â‚ = Î¾â±¼)}/âˆ‚Î¸â‚˜âˆ‚Î¸â‚™`
"""
function âˆ‡âˆ‡transitionmatrices(memory::Memory_for_hessian_choiceLL, adaptedclicks::Adaptedclicks, clicks::Clicks, t::Integer)
	@unpack P, Aáµƒsilent, âˆ‡Aáµƒsilent, âˆ‡âˆ‡Aáµƒsilent, Aáµƒinput, âˆ‡Aáµƒinput, âˆ‡âˆ‡Aáµƒinput = memory
	if t âˆˆ clicks.inputtimesteps
		clickindex = clicks.inputindex[t][1]
		Aáµƒ = Aáµƒinput[clickindex]
		âˆ‡Aáµƒ = âˆ‡Aáµƒinput[clickindex]
		âˆ‡âˆ‡Aáµƒ = âˆ‡âˆ‡Aáµƒinput[clickindex]
	else
		Aáµƒ = Aáµƒsilent
		âˆ‡Aáµƒ = âˆ‡Aáµƒsilent
		âˆ‡âˆ‡Aáµƒ = âˆ‡âˆ‡Aáµƒsilent
	end
	return Aáµƒ, âˆ‡Aáµƒ, âˆ‡âˆ‡Aáµƒ
end

"""
    conditional_choice_likelihood!(p, choice, Ïˆ)

In-place computation of the condition likelihood a choice given the accumulator state

MODIFIED ARGUMENT
-`p`: after modidication, element `p[i]` corresponds to `p(choice âˆ£ a=Î¾áµ¢)`

ARGUMENT
-`choice`: the observed choice, either right (`choice`=true) or left.
-`Ïˆ`: the prior probability of a lapse state
"""
function conditional_choice_likelihood!(p::Vector{<:Real}, choice::Bool, Ïˆ::Real)
	Î = length(p)
	zeroindex = cld(Î,2)
    p[zeroindex] = 0.5
    if choice
        p[1:zeroindex-1]   .= Ïˆ/2
        p[zeroindex+1:end] .= 1-Ïˆ/2
    else
        p[1:zeroindex-1]   .= 1-Ïˆ/2
        p[zeroindex+1:end] .= Ïˆ/2
    end
    return nothing
end

"""
	differentiate_conditional_choice_likelihood_wrt_Ïˆ!(âˆ‚pğ‘‘_âˆ‚Ïˆ, ğ‘‘)

Derivative of the conditional likelihood of the choice with respect to the lapse rate

ARGUMENT
-`ğ‘‘`: left (false) or right (true) choice of the animal

MODIFIED ARGUMENT
-`âˆ‚pğ‘‘_âˆ‚Ïˆ`: derivative of the conditional likelihood of the choice with respect to the lapse rate. Element `âˆ‚pğ‘‘_âˆ‚Ïˆ[i,j]` represents:
	âˆ‚p{ğ‘‘ âˆ£ a(T)=Î¾(i)}/âˆ‚Ïˆ
"""
function differentiate_conditional_choice_likelihood_wrt_Ïˆ!(âˆ‚pğ‘‘_âˆ‚Ïˆ::Vector{<:Real}, ğ‘‘::Bool)
	if ğ‘‘
		âˆ‚pğ‘‘_Î¾â»_âˆ‚Ïˆ = 0.5
		âˆ‚pğ‘‘_Î¾âº_âˆ‚Ïˆ = -0.5
	else
		âˆ‚pğ‘‘_Î¾â»_âˆ‚Ïˆ = -0.5
		âˆ‚pğ‘‘_Î¾âº_âˆ‚Ïˆ = 0.5
	end
	Î = length(âˆ‚pğ‘‘_âˆ‚Ïˆ)
	zeroindex = cld(Î,2)
	for i = 1:zeroindex-1
		âˆ‚pğ‘‘_âˆ‚Ïˆ[i] = âˆ‚pğ‘‘_Î¾â»_âˆ‚Ïˆ
	end
	âˆ‚pğ‘‘_âˆ‚Ïˆ[zeroindex] = 0.0
	for i = zeroindex+1:Î
		âˆ‚pğ‘‘_âˆ‚Ïˆ[i] = âˆ‚pğ‘‘_Î¾âº_âˆ‚Ïˆ
	end
end

"""
	Memory_for_hessian_choiceLL(model)

Create a structure for computing the hessian of the log-likelihood of the choices

ARGUMENT
-`model`: structure containing the data, hyperparameters, and parameters of a factorial hidden-Markov drift-diffusion model

OUTPUT
-a structure containing the memory and pre-computed quantities
"""
function Memory_for_hessian_choiceLL(model::Model)
	@unpack options, Î¸native, Î¸real, trialsets = model
	@unpack Î”t, minpa, Î = options
	# B, k, Î», Î¼â‚€, Ï•, Ïˆ, ÏƒÂ²â‚, ÏƒÂ²áµ¢, ÏƒÂ²â‚›, wâ‚•
	parameternames = [:B, :k, :Î», :Î¼â‚€, :Ï•, :Ïˆ, :ÏƒÂ²â‚, :ÏƒÂ²áµ¢, :ÏƒÂ²â‚›, :wâ‚•]
	nÎ¸ = length(parameternames)
	indexÎ¸_paâ‚ = [1,4,8,10]
	indexÎ¸_paâ‚œaâ‚œâ‚‹â‚ = [1,2,3,5,7,9]
	indexÎ¸_Ïˆ = [6]
	nÎ¸_paâ‚ = length(indexÎ¸_paâ‚)
	nÎ¸_paâ‚œaâ‚œâ‚‹â‚ = length(indexÎ¸_paâ‚œaâ‚œâ‚‹â‚)
	P = Probabilityvector(Î”t, minpa, Î¸native, Î)
	update_for_âˆ‡âˆ‡transition_probabilities!(P)
	âˆ‡âˆ‡Aáµƒsilent = map(i->zeros(Î,Î), CartesianIndices((nÎ¸_paâ‚œaâ‚œâ‚‹â‚,nÎ¸_paâ‚œaâ‚œâ‚‹â‚)))
	âˆ‡Aáµƒsilent = map(i->zeros(Î,Î), 1:nÎ¸_paâ‚œaâ‚œâ‚‹â‚)
	Aáµƒsilent = ones(typeof(Î¸native.B[1]), Î, Î).*minpa
	one_minus_Îminpa = 1.0-Î*minpa
	Aáµƒsilent[1,1] += one_minus_Îminpa
	Aáµƒsilent[Î, Î] += one_minus_Îminpa
	âˆ‡âˆ‡transitionmatrix!(âˆ‡âˆ‡Aáµƒsilent, âˆ‡Aáµƒsilent, Aáµƒsilent, P)
	maxclicks = maximum_number_of_clicks(model)
	maxtimesteps = maximum_number_of_time_steps(model)
	f = collect(zeros(Î) for t=1:maxtimesteps)
	âˆ‡f = collect(collect(zeros(Î) for q=1:nÎ¸) for t=1:maxtimesteps)
	Aáµƒinput=map(1:maxclicks) do t
				A = ones(Î,Î).*minpa
				A[1,1] += one_minus_Îminpa
				A[Î,Î] += one_minus_Îminpa
				return A
			end
	âˆ‡Aáµƒinput = collect(collect(zeros(Î,Î) for q=1:nÎ¸_paâ‚œaâ‚œâ‚‹â‚) for t=1:maxclicks)
	âˆ‡âˆ‡Aáµƒinput = map(1:maxclicks) do t
					map(CartesianIndices((nÎ¸_paâ‚œaâ‚œâ‚‹â‚,nÎ¸_paâ‚œaâ‚œâ‚‹â‚))) do ij
						zeros(Î,Î)
					end
				end
	âˆ‡paâ‚ = collect(zeros(Î) for q=1:nÎ¸_paâ‚)
	âˆ‡âˆ‡paâ‚ = map(CartesianIndices((nÎ¸_paâ‚,nÎ¸_paâ‚))) do q
				zeros(Î)
			end
	Memory_for_hessian_choiceLL(Î=Î,
								parameternames=parameternames,
								indexÎ¸_paâ‚=indexÎ¸_paâ‚,
								indexÎ¸_paâ‚œaâ‚œâ‚‹â‚=indexÎ¸_paâ‚œaâ‚œâ‚‹â‚,
								indexÎ¸_Ïˆ=indexÎ¸_Ïˆ,
								P=P,
								Aáµƒsilent=Aáµƒsilent,
								âˆ‡Aáµƒsilent=âˆ‡Aáµƒsilent,
								âˆ‡âˆ‡Aáµƒsilent=âˆ‡âˆ‡Aáµƒsilent,
								Aáµƒinput=Aáµƒinput,
								âˆ‡Aáµƒinput=âˆ‡Aáµƒinput,
								âˆ‡âˆ‡Aáµƒinput=âˆ‡âˆ‡Aáµƒinput,
								âˆ‡paâ‚=âˆ‡paâ‚,
								âˆ‡âˆ‡paâ‚=âˆ‡âˆ‡paâ‚,
								f=f,
								âˆ‡f=âˆ‡f)
end
