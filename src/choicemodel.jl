"""
	maximizechoiceLL!(model)

Learn the parameters that maximize the log-likelihood of the behavioral choices

OPTIONAL ARGUMENT
-`extended_trace`: save additional information
-`f_tol`: threshold for determining convergence in the objective value
-`g_tol`: threshold for determining convergence in the gradient
-`iterations`: number of inner iterations that will be run before the algorithm gives up
-`outer_iterations`: number of outer iterations that will be run before the algorithm gives up
-`show_every`: trace output is printed every `show_every`th iteration.
-`show_trace`: should a trace of the optimization algorithm's state be shown?
-`x_tol`: threshold for determining convergence in the input vector

"""
function maximizechoiceLL!(model::Model;
		                 extended_trace::Bool=true,
		                 f_tol::AbstractFloat=1e-9,
		                 g_tol::AbstractFloat=1e-8,
		                 iterations::Integer=1000,
						 outer_iterations::Integer=10,
		                 show_every::Integer=1,
		                 show_trace::Bool=true,
		                 x_tol::AbstractFloat=1e-5)
	concatenatedÎ¸, indexÎ¸ = concatenate_choice_related_parameters(model)
    f(concatenatedÎ¸) = -loglikelihood!(model, concatenatedÎ¸, indexÎ¸)
    g!(âˆ‡, concatenatedÎ¸) = âˆ‡negativeloglikelihood!(âˆ‡, model, concatenatedÎ¸, indexÎ¸)
	# lowerbounds, upperbounds = concatenatebounds(indexÎ¸, model.options)
    Optim_options = Optim.Options(extended_trace=extended_trace,
								  f_tol=f_tol,
                                  g_tol=g_tol,
                                  iterations=iterations,
								  outer_iterations=outer_iterations,
                                  show_every=show_every,
                                  show_trace=show_trace,
                                  x_tol=x_tol)
	# algorithm = Fminbox(LBFGS(linesearch = LineSearches.BackTracking()))
	algorithm = LBFGS(linesearch = LineSearches.BackTracking())
	Î¸â‚€ = deepcopy(concatenatedÎ¸)
	# optimizationresults = Optim.optimize(f, g!, lowerbounds, upperbounds, Î¸â‚€, algorithm, Optim_options)
	optimizationresults = Optim.optimize(f, g!, Î¸â‚€, algorithm, Optim_options)
    println(optimizationresults)
    maximumlikelihoodÎ¸ = Optim.minimizer(optimizationresults)
	sortparameters!(model, maximumlikelihoodÎ¸, indexÎ¸)
end

"""
    concatenate_choice_related_parameters(model)

Concatenate values of parameters being fitted into a vector of floating point numbers

ARGUMENT
-`model`: the factorial hidden Markov drift-diffusion model

RETURN
-`concatenatedÎ¸`: a vector of the concatenated values of the parameters being fitted
-`indexÎ¸`: a structure indicating the index of each model parameter in the vector of concatenated values
"""
function concatenate_choice_related_parameters(model::Model)
    @unpack options, Î¸real, trialsets = model
	concatenatedÎ¸ = zeros(0)
    counter = 0
	latentÎ¸ = LatentÎ¸(collect(zeros(Int64,1) for i in fieldnames(LatentÎ¸))...)
	tofit = true
	for field in fieldnames(LatentÎ¸)
		if field == :Aá¶œâ‚â‚ || field == :Aá¶œâ‚‚â‚‚ || field == :Ï€á¶œâ‚
			tofit = false
		else
			options_field = Symbol("fit_"*String(field))
			if hasfield(typeof(options), options_field)
				tofit = getfield(options, options_field)
			else
				error("Unrecognized field: "*String(field))
			end
		end
		if tofit
			counter += 1
			getfield(latentÎ¸, field)[1] = counter
			concatenatedÎ¸ = vcat(concatenatedÎ¸, getfield(Î¸real, field)[1])
		else
			getfield(latentÎ¸, field)[1] = 0
		end
	end
	emptyindex = map(trialset->map(mpGLM->zeros(Int, 0), trialset.mpGLMs), model.trialsets)
    indexÎ¸ = IndexÎ¸(latentÎ¸=latentÎ¸,
					ğ® = emptyindex,
					ğ¥ = emptyindex,
					ğ« = emptyindex)
    return concatenatedÎ¸, indexÎ¸
end


"""
    loglikelihood!(model, concatenatedÎ¸)

Compute the log-likelihood of the choices

ARGUMENT
-`model`: an instance of FHM-DDM

UNMODIFIED ARGUMENT
-`concatenatedÎ¸`: a vector of concatenated parameter values
-`indexÎ¸`: index of each parameter after if all parameters being fitted are concatenated

RETURN
-log-likelihood
"""
function loglikelihood!(model::Model,
					    concatenatedÎ¸::Vector{<:Real},
						indexÎ¸::IndexÎ¸)
	sortparameters!(model, concatenatedÎ¸, indexÎ¸)
	trialinvariant = Trialinvariant(model; purpose="loglikelihood")
	â„“ = map(model.trialsets) do trialset
			map(trialset.trials) do trial
				loglikelihood(model.Î¸native, trial, trialinvariant)
			end
		end
	return sum(sum(â„“))
end

"""
	loglikelihood(pğ˜ğ‘‘, trialinvariant, Î¸native, trial)

Compute the log-likelihood of the choice from one trial

ARGUMENT
-`Î¸native`: model parameters in their native space
-`trial`: stimulus and behavioral information of one trial
-`trialinvariant`: a structure containing quantities that are used in each trial

RETURN
-`â„“`: log-likelihood of the data from one trial
"""
function loglikelihood(Î¸native::LatentÎ¸,
					   trial::Trial,
					   trialinvariant::Trialinvariant)
	@unpack clicks = trial
	@unpack Aáµƒsilent, Î”t, ğ›, Î = trialinvariant
	C = adapt(clicks, Î¸native.k[1], Î¸native.Ï•[1])
	Î¼ = Î¸native.Î¼â‚€[1] + trial.previousanswer*Î¸native.wâ‚•[1]
	Ïƒ = âˆšÎ¸native.ÏƒÂ²áµ¢[1]
	f = probabilityvector(Î¼, Ïƒ, ğ›)
	D = sum(f)
	f /= D
	â„“ = log(D)
	T = eltype(Î¸native.Î»[1])
	Aáµƒ = zeros(T, Î, Î)
	pğ‘‘ = conditional_probability_of_choice(trial.choice, Î¸native.Ïˆ[1], Î)
	@inbounds for t = 2:trial.ntimesteps
		if isempty(clicks.inputindex[t])
			f = Aáµƒsilent * f
		else
			cL = sum(C[clicks.left[t]])
			cR = sum(C[clicks.right[t]])
			stochasticmatrix!(Aáµƒ, cL, cR, trialinvariant, Î¸native)
			f = Aáµƒ * f
		end
		if t == trial.ntimesteps
			f .*= pğ‘‘
		end
		D = sum(f)
		f /= D
		â„“ += log(D)
	end
	return â„“
end

"""
    âˆ‡negativeloglikelihood!(âˆ‡, Î³, model, concatenatedÎ¸, indexÎ¸)

Gradient of the negative log-likelihood of the factorial hidden Markov drift-diffusion model (FHMDDM)

MODIFIED INPUT
-`âˆ‡`: a vector of partial derivatives
-`model`: a structure containing information of the factorial hidden Markov drift-diffusion model

UNMODIFIED ARGUMENT
-`concatenatedÎ¸`: parameter values concatenated into a vector
-`indexÎ¸`: index of each parameter after if all parameters being fitted are concatenated

"""
function âˆ‡negativeloglikelihood!(âˆ‡::Vector{<:AbstractFloat},
								 model::Model,
								 concatenatedÎ¸::Vector{<:AbstractFloat},
								 indexÎ¸::IndexÎ¸)
	sortparameters!(model, concatenatedÎ¸, indexÎ¸)
	@unpack options, Î¸native, Î¸real, trialsets = model
	@unpack K = options
	trialinvariant = Trialinvariant(model; purpose="gradient")
	gradients=map(trialsets) do trialset
				pmap(trialset.trials) do trial
					âˆ‡loglikelihood(Î¸native, trial, trialinvariant)
				end
			end
	g = gradients[1][1] # reuse this memory
	for field in fieldnames(LatentÎ¸)
		latentâˆ‚ = getfield(g, field)
		for i in eachindex(gradients)
			start = i==1 ? 2 : 1
			for m in start:length(gradients[i])
				latentâˆ‚[1] += getfield(gradients[i][m], field)[1]
			end
		end
	end
	g.B[1] *= Î¸native.B[1]*logistic(-Î¸real.B[1])
	g.k[1] *= Î¸native.k[1]
	g.Ï•[1] *= Î¸native.Ï•[1]*(1.0 - Î¸native.Ï•[1])
	tmpÏˆ = logistic(Î¸real.Ïˆ[1] + logit(options.q_Ïˆ))
	g.Ïˆ[1] *= (1.0-options.bound_Ïˆ)*tmpÏˆ*(1.0 - tmpÏˆ)
	g.ÏƒÂ²â‚[1] *= Î¸native.ÏƒÂ²â‚[1]
	g.ÏƒÂ²áµ¢[1] *= options.q_ÏƒÂ²áµ¢*exp(Î¸real.ÏƒÂ²áµ¢[1])
	g.ÏƒÂ²â‚›[1] *= Î¸native.ÏƒÂ²â‚›[1]
	for field in fieldnames(LatentÎ¸)
		index = getfield(indexÎ¸.latentÎ¸, field)[1]
		if index != 0
			âˆ‡[index] = -getfield(g,field)[1] # note the negative sign
		end
	end
end

"""
	âˆ‡loglikelihood(trialinvariant, Î¸native, trial)

Compute quantities needed for the gradient of the log-likelihood of the data observed in one trial

ARGUMENT
-`Î¸native`: parameters for the latent variables in their native space
-`trial`: information on the stimuli and behavioral choice of one trial
-`trialinvariant`: structure containing quantities used across trials

RETURN
-`latentâˆ‡`: gradient of the log-likelihood of the data observed in one trial with respect to the parameters specifying the latent variables
"""
function âˆ‡loglikelihood(Î¸native::LatentÎ¸,
						trial::Trial,
						trialinvariant::Trialinvariant)
	@unpack choice, clicks = trial
	@unpack inputtimesteps, inputindex = clicks
	@unpack Aáµƒsilent, dAáµƒsilentdÎ¼, dAáµƒsilentdÏƒÂ², dAáµƒsilentdB, Aá¶œ, Î”t, K, ğ›š, Î, ğ› = trialinvariant
	dâ„“dk, dâ„“dÎ», dâ„“dÏ•, dâ„“dÏƒÂ²â‚, dâ„“dÏƒÂ²â‚›, dâ„“dB = 0., 0., 0., 0., 0., 0.
	Î¼ = Î¸native.Î¼â‚€[1] + trial.previousanswer*Î¸native.wâ‚•[1]
	Ïƒ = âˆšÎ¸native.ÏƒÂ²áµ¢[1]
	Ï€áµƒ, dÏ€áµƒdÎ¼, dÏ€áµƒdÏƒÂ², dÏ€áµƒdB = zeros(Î), zeros(Î), zeros(Î), zeros(Î)
	probabilityvector!(Ï€áµƒ, dÏ€áµƒdÎ¼, dÏ€áµƒdÏƒÂ², dÏ€áµƒdB, Î¼, ğ›š, Ïƒ, ğ›)
	n_steps_with_input = length(clicks.inputtimesteps)
	Aáµƒ = map(x->zeros(Î,Î), clicks.inputtimesteps)
	dAáµƒdÎ¼ = map(x->zeros(Î,Î), clicks.inputtimesteps)
	dAáµƒdÏƒÂ² = map(x->zeros(Î,Î), clicks.inputtimesteps)
	dAáµƒdB = map(x->zeros(Î,Î), clicks.inputtimesteps)
	Î”c = zeros(n_steps_with_input)
	âˆ‘c = zeros(n_steps_with_input)
	C, dCdk, dCdÏ• = âˆ‡adapt(clicks, Î¸native.k[1], Î¸native.Ï•[1])
	for i in 1:n_steps_with_input
		t = clicks.inputtimesteps[i]
		cL = sum(C[clicks.left[t]])
		cR = sum(C[clicks.right[t]])
		stochasticmatrix!(Aáµƒ[i], dAáµƒdÎ¼[i], dAáµƒdÏƒÂ²[i], dAáµƒdB[i], cL, cR, trialinvariant, Î¸native)
		Î”c[i] = cR-cL
		âˆ‘c[i] = cL+cR
	end
	D, f = forward(Aáµƒ, trial.choice, inputindex, Ï€áµƒ, Î¸native.Ïˆ[1], trialinvariant)
	b = ones(Î)
	Î»Î”t = Î¸native.Î»[1]*Î”t
	expÎ»Î”t = exp(Î»Î”t)
	dÎ¼dÎ”c = (expÎ»Î”t - 1.0)/Î»Î”t
	Î· = (expÎ»Î”t - dÎ¼dÎ”c)/Î¸native.Î»[1]
	ğ›áµ€Î”texpÎ»Î”t = transpose(ğ›)*Î”t*expÎ»Î”t
	pğ‘‘ = conditional_probability_of_choice(choice, Î¸native.Ïˆ[1], Î)
	@inbounds for t = trial.ntimesteps:-1:1
		if t == trial.ntimesteps-1
			b .*= pğ‘‘
		end
		if t < trial.ntimesteps # backward step
			Aáµƒâ‚œâ‚Šâ‚ = isempty(inputindex[t+1]) ? Aáµƒsilent : Aáµƒ[inputindex[t+1][1]]
			b = transpose(Aáµƒâ‚œâ‚Šâ‚) * b / D[t+1]
		end
		if t > 1 # joint posterior over consecutive time bins, computations involving the transition matrix
			if isempty(inputindex[t])
				Aáµƒâ‚œ = Aáµƒsilent
				dAáµƒâ‚œdÎ¼ = dAáµƒsilentdÎ¼
				dAáµƒâ‚œdÏƒÂ² = dAáµƒsilentdÏƒÂ²
				dAáµƒâ‚œdB = dAáµƒsilentdB
			else
				i = inputindex[t][1]
				Aáµƒâ‚œ = Aáµƒ[i]
				dAáµƒâ‚œdÎ¼ = dAáµƒdÎ¼[i]
				dAáµƒâ‚œdÏƒÂ² = dAáµƒdÏƒÂ²[i]
				dAáµƒâ‚œdB = dAáµƒdB[i]
			end
			if t == trial.ntimesteps
				Ï‡áµƒ_Aáµƒ = pğ‘‘.*b .* transpose(f[t-1]) ./ D[t]
			else
				Ï‡áµƒ_Aáµƒ = b .* transpose(f[t-1]) ./ D[t]
			end
			Ï‡áµƒ_dlogAáµƒdÎ¼ = Ï‡áµƒ_Aáµƒ .* dAáµƒâ‚œdÎ¼ # Ï‡áµƒâŠ™ d/dÎ¼{log(Aáµƒ)} = Ï‡áµƒâŠ˜ AáµƒâŠ™ d/dÎ¼{Aáµƒ}
			âˆ‘_Ï‡áµƒ_dlogAáµƒdÎ¼ = sum(Ï‡áµƒ_dlogAáµƒdÎ¼)
			âˆ‘_Ï‡áµƒ_dlogAáµƒdÏƒÂ² = sum(Ï‡áµƒ_Aáµƒ .* dAáµƒâ‚œdÏƒÂ²) # similarly, Ï‡áµƒâŠ™ d/dÏƒÂ²{log(Aáµƒ)} = Ï‡áµƒâŠ˜ AáµƒâŠ™ d/dÏƒÂ²{Aáµƒ}
			dâ„“dÏƒÂ²â‚ += âˆ‘_Ï‡áµƒ_dlogAáµƒdÏƒÂ² # the Î”t is multiplied after summing across time steps
			dâ„“dB += sum(Ï‡áµƒ_Aáµƒ .* dAáµƒâ‚œdB)
			if isempty(inputindex[t])
				dÎ¼dÎ» = ğ›áµ€Î”texpÎ»Î”t
			else
				dÎ¼dÎ» = ğ›áµ€Î”texpÎ»Î”t .+ Î”c[i].*Î·
				dâ„“dÏƒÂ²â‚› += âˆ‘_Ï‡áµƒ_dlogAáµƒdÏƒÂ²*âˆ‘c[i]
				dcLdÏ• = sum(dCdÏ•[clicks.left[t]])
				dcRdÏ• = sum(dCdÏ•[clicks.right[t]])
				dcLdk = sum(dCdk[clicks.left[t]])
				dcRdk = sum(dCdk[clicks.right[t]])
				dÏƒÂ²dÏ• = Î¸native.ÏƒÂ²â‚›[1]*(dcLdÏ• + dcRdÏ•)
				dÏƒÂ²dk = Î¸native.ÏƒÂ²â‚›[1]*(dcLdk + dcRdk)
				dâ„“dÏ• += âˆ‘_Ï‡áµƒ_dlogAáµƒdÎ¼*dÎ¼dÎ”c*(dcRdÏ• - dcLdÏ•) + âˆ‘_Ï‡áµƒ_dlogAáµƒdÏƒÂ²*dÏƒÂ²dÏ•
				dâ„“dk += âˆ‘_Ï‡áµƒ_dlogAáµƒdÎ¼*dÎ¼dÎ”c*(dcRdk - dcLdk) + âˆ‘_Ï‡áµƒ_dlogAáµƒdÏƒÂ²*dÏƒÂ²dk
			end
			dâ„“dÎ» += sum(Ï‡áµƒ_dlogAáµƒdÎ¼.*dÎ¼dÎ»)
		end
	end
	dâ„“dÏƒÂ²â‚ *= Î”t
	Î³áµƒâ‚_oslash_Ï€áµƒ = b # reuse memory
	Î³áµƒâ‚_oslash_Ï€áµƒ ./= D[1]
	âˆ‘_Î³áµƒâ‚_dlogÏ€áµƒdÎ¼ = Î³áµƒâ‚_oslash_Ï€áµƒ â‹… dÏ€áµƒdÎ¼ # similar to above, Î³áµƒâ‚âŠ™ d/dÎ¼{log(Ï€áµƒ)} = Î³áµƒâ‚âŠ˜ Ï€áµƒâŠ™ d/dÎ¼{Ï€áµƒ}
	dâ„“dÎ¼â‚€ = âˆ‘_Î³áµƒâ‚_dlogÏ€áµƒdÎ¼
	dâ„“dwâ‚• = âˆ‘_Î³áµƒâ‚_dlogÏ€áµƒdÎ¼ * trial.previousanswer
	dâ„“dÏƒÂ²áµ¢ = Î³áµƒâ‚_oslash_Ï€áµƒ â‹… dÏ€áµƒdÏƒÂ²
	dâ„“dB += Î³áµƒâ‚_oslash_Ï€áµƒ â‹… dÏ€áµƒdB
	dâ„“dÏˆ = differentiateâ„“_wrt_Ïˆ(trial.choice, f[end], Î¸native.Ïˆ[1])
	LatentÎ¸(B	= [dâ„“dB],
			k	= [dâ„“dk],
			Î»	= [dâ„“dÎ»],
			Î¼â‚€	= [dâ„“dÎ¼â‚€],
			Ï•	= [dâ„“dÏ•],
			Ïˆ	= [dâ„“dÏˆ],
			ÏƒÂ²â‚	= [dâ„“dÏƒÂ²â‚],
			ÏƒÂ²áµ¢	= [dâ„“dÏƒÂ²áµ¢],
			ÏƒÂ²â‚›	 = [dâ„“dÏƒÂ²â‚›],
			wâ‚•	 = [dâ„“dwâ‚•])
end

"""
	forward(Aáµƒ, inputindex, Ï€áµƒ, pğ˜d, trialinvariant)

Forward pass of the forward-backward algorithm

ARGUMENT
-`Aáµƒ`: transition probabilities of the accumulator variable. Aáµƒ[t][j,k] â‰¡ p(aâ‚œ=Î¾â±¼ âˆ£ aâ‚œâ‚‹â‚=Î¾â‚–)
`inputindex`: index of the time steps with auditory input. For time step `t`, if the element `inputindex[t]` is nonempty, then `Aáµƒ[inputindex[t][1]]` is the transition matrix for that time step. If `inputindex[t]` is empty, then the corresponding transition matrix is `Aáµƒsilent`.
-`Ï€áµƒ`: a vector of floating-point numbers specifying the prior probability of each accumulator state
-`trialinvariant`: a structure containing quantities that are used in each trial

RETURN
-`D`: scaling factors with element `D[t]` â‰¡ p(ğ˜â‚œ âˆ£ ğ˜â‚, ... ğ˜â‚œâ‚‹â‚)
-`f`: Forward recursion terms. `f[t][j,k]` â‰¡ p(aâ‚œ=Î¾â±¼, zâ‚œ=k âˆ£ ğ˜â‚, ... ğ˜â‚œ) where ğ˜ refers to all the spike trains

"""
function forward(Aáµƒ::Vector{<:Matrix{<:AbstractFloat}},
				 choice::Bool,
 				 inputindex::Vector{<:Vector{<:Integer}},
				 Ï€áµƒ::Vector{<:AbstractFloat},
				 Ïˆ::Real,
				 trialinvariant::Trialinvariant)
	@unpack Aáµƒsilent, Î, ğ› = trialinvariant
	ntimesteps = length(inputindex)
	f = map(x->zeros(Î), 1:ntimesteps)
	D = zeros(ntimesteps)
	f[1] = Ï€áµƒ
	D[1] = sum(f[1])
	f[1] /= D[1]
	pğ‘‘ = conditional_probability_of_choice(choice, Ïˆ, Î)
	@inbounds for t = 2:ntimesteps
		if isempty(inputindex[t])
			Aáµƒâ‚œ = Aáµƒsilent
		else
			i = inputindex[t][1]
			Aáµƒâ‚œ = Aáµƒ[i]
		end
		f[t] = Aáµƒâ‚œ * f[t-1]
		if t == ntimesteps
			f[t] .*= pğ‘‘
		end
		D[t] = sum(f[t])
		f[t] /= D[t]
	end
	return D,f
end

"""
    conditional_probability_of_choice(choice, Ïˆ, Î)

Probability of a choice conditioned on the accumulator state

ARGUMENT
-`choice`: the observed choice, either right (`choice`=true) or left.
-`Ïˆ`: the prior probability of a lapse state
-`Î`: number of accumulator states

RETURN
-a vector whose length is equal to the number of accumulator states
"""
function conditional_probability_of_choice(choice::Bool, Ïˆ::Real, Î::Integer)
	p = zeros(typeof(Ïˆ), Î)
	zeroindex = cld(Î,2)
    p[zeroindex] = 0.5
    if choice
        p[1:zeroindex-1]   .= Ïˆ/2
        p[zeroindex+1:end] .= 1-Ïˆ/2
    else
        p[1:zeroindex-1]   .= 1-Ïˆ/2
        p[zeroindex+1:end] .= Ïˆ/2
    end
    return p
end
