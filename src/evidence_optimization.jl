"""
	maximizeevidence!(model)

Learn both the parameters and hyperparameters by maximizing the evidence

The optimization procedure alternately fixes the hyperparameters and learn the parameters by maximizing the posterior and fixes the parameters and learn the hyperparameters by maximing the evidence.

MODIFIED ARGUMENT
-`model`: structure containing the parameters, hyperparameters, and data
"""
function maximizeevidence!(model::Model)
    maximizeposterior!(model)
    ğ‡ = âˆ‡âˆ‡loglikelihood(model)
    ğ›‰â‚˜â‚â‚š = concatenateparameters(model)[1]
    ğ€â‚€ = copy(model.priorprecision)
	memory = Memoryforgradient(model)
    maximizeevidence!(model, ğ‡)
	return nothing
end

"""
	maximizeevidence!(model, ...)

Learn hyperparameters by fixing the parameters of the model and maximizing the evidence

MODIFIED ARGUMENT
-`model`: structure containing the parameters, hyperparameters, and data
"""
function maximizeevidence!(memory::Memoryforgradient,
						model::Model,
						ğ€â‚€::Diagonal,
						ğ‡::Symmetric,
						ğâ‚€ğ›‰â‚˜â‚â‚š::Vector{<:Real};
						optimizationoptions::Optim.Options=Optim.Options(),
						optimizer::Optim.FirstOrderOptimizer=LBFGS(linesearch=LineSearches.BackTracking()))
	ğ€â‚€ = copy(model.precisionmatrix)
	ğ›‰â‚˜â‚â‚š = concatenateparameters(model)[1]
    ğ‡ = âˆ‡âˆ‡loglikelihood(model)
	ğâ‚€ğ›‰â‚˜â‚â‚š = (ğ€â‚€-ğ‡)*ğ›‰â‚˜â‚â‚š
    f(ğ›‚) = -logevidence!(memory, model, ğ€â‚€, ğ›‚, ğ‡, ğâ‚€ğ›‰â‚˜â‚â‚š)
	g!(âˆ‡nğ¸, ğ›‚) = âˆ‡negativelogevidence!(âˆ‡nğ¸, memory, model, ğ€â‚€, ğ›‚, ğ‡, ğâ‚€ğ›‰â‚˜â‚â‚š)
	optimizationresults = Optim.optimize(f, g!, ğ€â‚€.diag, optimizer, optimizationoptions)
end

"""
	logevidence!(memory, model, ğ›‚, ğ‡, ğâ‚€ğ›‰â‚˜â‚â‚š)

Log of the marginal likelihood

MODIFIED ARGUMENT
-`memory`: a structure containing memory for in-place computation
-`model`: structure containing the parameters, hyperparameters, and data

UNMODIFIED ARGUMENT
-`ğ€â‚€`: precision matrix that was fixed and used to find the MAP values of the parameters
-`ğ›‚`: diagonal of the precision matrix; these are the values that are being learned
-`ğ‡`: Hessian of the log-likelihood evaluated at the MAP values of the parameters
-`ğâ‚€ğ›‰â‚˜â‚â‚š`: Hessian of the log-posterior evalued at the MAP values of the parameters multiplied by the MAP value of the parameters

RETURN
-log of the evidence
"""
function logevidence!(memory::Memoryforgradient,
					model::Model,
					ğ›‚::Vector{<:Real},
					ğ‡::Symmetric,
					ğâ‚€ğ›‰â‚˜â‚â‚š::Vector{<:Real})
	ğ€ = Diagonal(ğ›‚)
	ğ = ğ€-ğ‡
    ğ° = ğ \ ğâ‚€ğ›‰â‚˜â‚â‚š
	loglikelihood!(model, memory, ğ°)
	logevidence(ğ€, ğ‡, memory.â„“[1], ğ°)
end

"""
	logevidence(ğ€, ğ‡, â„“, ğ°)

Evaluate the log-evidence

ARGUMENT
-`ğ€`: precision matrix
-`ğ‡`: Hessian of the log-likelihood evaluated at the MAP values of the parameters
-`â„“`: log-likelihood evaluated at the approximate posterior mode ğ°
-`ğ°`: approximate posterior mode as a function of ğ€

RETURN
-log evidence
"""
function logevidence(ğ€::Diagonal, ğ‡::Symmetric, â„“::Real, ğ°::Vector{<:Real})
	â„“ - 0.5ğ°'*ğ€*ğ° - 0.5logdet(I - ğ€^-1*ğ‡)
end

"""
	âˆ‡negativelogevidence!(âˆ‡nğ¸, memory, model, ğ›‚, ğ‡, ğâ‚€ğ›‰â‚˜â‚â‚š)

gradient of the negative log of the marginal likelihood

MODIFIED ARGUMENT
-`âˆ‡nğ¸`: gradient of the negative of the log-evidence
-`ğ€`: precision matrix
-`memory`: a structure containing memory for in-place computation
-`model`: structure containing the parameters, hyperparameters, and data
-`ğ°`: posterior mode as a function of the hyperparameters

UNMODIFIED ARGUMENT
-`ğ€â‚€`: precision matrix that was fixed and used to find the MAP values of the parameters
-`ğ›‚`: diagonal of the precision matrix; these are the values that are being learned
-`ğ‡`: Hessian of the log-likelihood evaluated at the MAP values of the parameters
-`ğâ‚€ğ›‰â‚˜â‚â‚š`: Hessian of the log-posterior evalued at the MAP values of the parameters multiplied by the MAP value of the parameters
"""
function âˆ‡negativelogevidence!(âˆ‡nğ¸::Vector{<:Real},
								memory::Memoryforgradient,
								model::Model,
								ğ›‚::Vector{<:Real},
								ğ‡::Symmetric,
								ğâ‚€ğ›‰â‚˜â‚â‚š::Vector{<:Real})
	ğ€ = Diagonal(ğ›‚)
	ğ = ğ€-ğ‡
    ğ° = ğ \ ğâ‚€ğ›‰â‚˜â‚â‚š
	âˆ‡nâ„“ = âˆ‡nğ¸ # reuse memory
	FHMDDM.âˆ‡negativeloglikelihood!(âˆ‡nâ„“, memory, model, ğ°)
	ğ‰ = -ğ \ Diagonal(ğ°) #Jacobian matrix of the posterior mode ğ° with respect to the precisions ğ›‚
	âˆ‡nğ¸ .= ğ‰'*(âˆ‡nâ„“ - 0.5ğâ‚€ğ›‰â‚˜â‚â‚š + ğ€*ğ°)
	ğš² = I - ğ€^-1*ğ‡
	ğ = zeros(size(ğš²));
	for i in eachindex(ğ›‚)
		if i > 1
	    	ğ[i-1,:] .= 0.0
		end
	    ğ[i,:] = ğ›‚[i]^-2 .* ğ‡[i,:]
	    âˆ‡nğ¸[i] += 0.5tr(ğš² \ ğ)
	end
	return nothing
end

"""
	check_âˆ‡logevidence(model)

Check whether the hand-coded gradient of the log-evidence matches the automatic gradient

ARGUMENT
-`model`: structure containing the parameters and hyperparameters

OPTIONAL ARGUMENT
-`simulate`: whether to simulate Hessian and MAP solution. If not, the model is first fitted before a Hessian is computed

RETURN
-absolute difference between the gradients
-absolute difference between the log-evidence functions

EXAMPLE
```julia-repl
julia> using FHMDDM
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_05_21_test/T176_2018_05_03/data.mat")
julia> absdiffâˆ‡ğ¸, absidffğ¸ = FHMDDM.check_âˆ‡logevidence(model; simulate=true)
```
"""
function check_âˆ‡logevidence(model::Model; simulate::Bool=true)
	D = size(model.precisionmatrix,1)
	ğ›‚ = rand(D)
	if simulate
		ğ‘ = 1 .- 2rand(D,D)
		ğ‡ = Symmetric(Diagonal(ğ›‚) - transpose(ğ‘)*ğ‘)
		ğ€ = Diagonal(ğ›‚)
		ğ = ğ€-ğ‡
		ğ° = 1 .- 2rand(D)
		ğ° ./= norm(ğ°)
		ğâ‚€ğ›‰â‚˜â‚â‚š = ğ*ğ°
		ğ€â‚€ = model.precisionmatrix
		ğ›‰â‚˜â‚â‚š = (ğ€â‚€-ğ‡) \ ğâ‚€ğ›‰â‚˜â‚â‚š
		indexÎ¸ = FHMDDM.concatenateparameters(model)[2]
		FHMDDM.sortparameters!(model, ğ›‰â‚˜â‚â‚š, indexÎ¸)
		FHMDDM.real2native!(model.Î¸native, model.options, model.Î¸real)
	else
		FHMDDM.initializeparameters!(model)
		FHMDDM.maximizeposterior!(model)
		ğ‡ = Symmetric(FHMDDM.âˆ‡âˆ‡loglikelihood(model))
		ğ›‰â‚˜â‚â‚š, indexÎ¸ = FHMDDM.concatenateparameters(model)
		ğ€â‚€ = model.precisionmatrix
		ğâ‚€ğ›‰â‚˜â‚â‚š = (ğ€â‚€-ğ‡)*ğ›‰â‚˜â‚â‚š
	end
	memory = FHMDDM.Memoryforgradient(model)
	handcoded_evidence = FHMDDM.logevidence!(memory, model, ğ›‚, ğ‡, ğâ‚€ğ›‰â‚˜â‚â‚š)
	handcoded_gradient = fill(NaN,D)
	FHMDDM.âˆ‡negativelogevidence!(handcoded_gradient, memory, model, ğ›‚, ğ‡, ğâ‚€ğ›‰â‚˜â‚â‚š)
    f(x) = FHMDDM.logevidence(x, ğâ‚€ğ›‰â‚˜â‚â‚š, ğ‡, indexÎ¸, model)
	automatic_evidence = f(ğ›‚)
	automatic_gradient = ForwardDiff.gradient(f, ğ›‚)
	return abs.(automatic_gradient .+ handcoded_gradient), abs(automatic_evidence-handcoded_evidence)
end

"""
	logevidence(ğ›‚, ğâ‚€ğ›‰â‚˜â‚â‚š, ğ‡, indexÎ¸, model)

ForwardDiff-computation evaluation of the log-evidence

ARGUMENT
-`ğ›‚`: diagonal of the precision matrix; these are the values that are being learned
-`ğâ‚€ğ›‰â‚˜â‚â‚š`: Hessian of the log-posterior evalued at the MAP values of the parameters multiplied by the MAP value of the parameters
-`ğ‡`: Hessian of the log-likelihood evaluated at the MAP values of the parameters
-`indexÎ¸`: index of the parameters
-`model`: structure containing the parameters, hyperparameters, and data

RETURN
-log of the marginal likelihood
"""
function logevidence(ğ›‚::Vector{<:Real},
					ğâ‚€ğ›‰â‚˜â‚â‚š::Vector{<:Real},
					ğ‡::Symmetric,
					indexÎ¸::IndexÎ¸,
					model::Model)
	ğ€ = Diagonal(ğ›‚)
	ğ = ğ€-ğ‡
    ğ° = ğ \ ğâ‚€ğ›‰â‚˜â‚â‚š
	â„“ = FHMDDM.loglikelihood(ğ°, indexÎ¸, model)
	logevidence(ğ€, ğ‡, â„“, ğ°)
end
