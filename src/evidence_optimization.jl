"""
	maximizeevidence!(model)

Learn both the parameters and hyperparameters by maximizing the evidence

The optimization procedure alternately fixes the hyperparameters and learn the parameters by maximizing the posterior and fixes the parameters and learn the hyperparameters by maximing the evidence.

MODIFIED ARGUMENT
-`model`: structure containing the parameters, hyperparameters, and data
"""
function maximizeevidence!(model::Model)
    maximizeposterior!(model)
    ğ‡ = âˆ‡âˆ‡loglikelihood(model)
    ğ›‰â‚˜â‚â‚š = concatenateparameters(model)[1]
    ğ€â‚€ = copy(model.priorprecision)
	memory = Memoryforgradient(model)
    maximizeevidence!(model, ğ‡)
	return nothing
end

"""
	maximizeevidence!(model, ...)

Learn hyperparameters by fixing the parameters of the model and maximizing the evidence

MODIFIED ARGUMENT
-`model`: structure containing the parameters, hyperparameters, and data
"""
function maximizeevidence!(memory::Memoryforgradient,
						model::Model,
						ğ€â‚€::Diagonal{<:Real, <:Vector{<:Real}},
						ğ‡::Matrix{<:Real},
						ğâ‚€ğ›‰â‚˜â‚â‚š::Vector{<:Real};
						optimizationoptions::Optim.Options=Optim.Options(),
						optimizer::Optim.FirstOrderOptimizer=LBFGS(linesearch=LineSearches.BackTracking()))
	ğ€â‚€ = copy(model.precisionmatrix)
	ğ›‰â‚˜â‚â‚š = concatenateparameters(model)[1]
    ğ‡ = âˆ‡âˆ‡loglikelihood(model)
	ğâ‚€ğ›‰â‚˜â‚â‚š = (ğ€â‚€-ğ‡)*ğ›‰â‚˜â‚â‚š
    f(ğ›‚) = -logevidence!(memory, model, ğ€â‚€, ğ›‚, ğ‡, ğâ‚€ğ›‰â‚˜â‚â‚š)
	g!(âˆ‡nğ¸, ğ›‚) = âˆ‡negativelogevidence!(âˆ‡nğ¸, memory, model, ğ€â‚€, ğ›‚, ğ‡, ğâ‚€ğ›‰â‚˜â‚â‚š)
	optimizationresults = Optim.optimize(f, g!, ğ€â‚€.diag, optimizer, optimizationoptions)
end

"""
	logevidence!(memory, model, ğ›‚, ğ‡, ğâ‚€ğ›‰â‚˜â‚â‚š)

Log of the marginal likelihood

MODIFIED ARGUMENT
-`memory`: a structure containing memory for in-place computation
-`model`: structure containing the parameters, hyperparameters, and data

UNMODIFIED ARGUMENT
-`ğ€â‚€`: precision matrix that was fixed and used to find the MAP values of the parameters
-`ğ›‚`: diagonal of the precision matrix; these are the values that are being learned
-`ğ‡`: Hessian of the log-likelihood evaluated at the MAP values of the parameters
-`ğâ‚€ğ›‰â‚˜â‚â‚š`: Hessian of the log-posterior evalued at the MAP values of the parameters multiplied by the MAP value of the parameters

RETURN
-log of the evidence
"""
function logevidence!(memory::Memoryforgradient,
					model::Model,
					ğ›‚::Vector{<:Real},
					ğ‡::Matrix{<:Real},
					ğâ‚€ğ›‰â‚˜â‚â‚š::Vector{<:Real})
	ğ€ = model.precisionmatrix
	if ğ›‚ != ğ€.diag
		ğ€.diag .= ğ›‚
	end
	ğ = ğ€-ğ‡
    ğ° = ğ \ ğâ‚€ğ›‰â‚˜â‚â‚š
	FHMDDM.loglikelihood!(model, memory, ğ°)
	memory.â„“[1] - 0.5*(transpose(ğ°)*ğ€*ğ°) - 0.5*logdet(ğ€) + 0.5*logdet(ğ)
end

"""
	âˆ‡negativelogevidence!(âˆ‡nğ¸, memory, model, ğ›‚, ğ‡, ğâ‚€ğ›‰â‚˜â‚â‚š)

gradient of the negative log of the marginal likelihood

MODIFIED ARGUMENT
-`âˆ‡nğ¸`: gradient of the negative of the log-evidence
-`ğ€`: precision matrix
-`memory`: a structure containing memory for in-place computation
-`model`: structure containing the parameters, hyperparameters, and data
-`ğ°`: posterior mode as a function of the hyperparameters

UNMODIFIED ARGUMENT
-`ğ€â‚€`: precision matrix that was fixed and used to find the MAP values of the parameters
-`ğ›‚`: diagonal of the precision matrix; these are the values that are being learned
-`ğ‡`: Hessian of the log-likelihood evaluated at the MAP values of the parameters
-`ğâ‚€ğ›‰â‚˜â‚â‚š`: Hessian of the log-posterior evalued at the MAP values of the parameters multiplied by the MAP value of the parameters
"""
function âˆ‡negativelogevidence!(âˆ‡nğ¸::Vector{<:Real},
								memory::Memoryforgradient,
								model::Model,
								ğ›‚::Vector{<:Real},
								ğ‡::Matrix{<:Real},
								ğâ‚€ğ›‰â‚˜â‚â‚š::Vector{<:Real})
	ğ€ = model.precisionmatrix
	if ğ›‚ != ğ€.diag
		ğ€.diag .= ğ›‚
	end
	ğ = ğ€-ğ‡
    ğ° = ğ \ ğâ‚€ğ›‰â‚˜â‚â‚š
	âˆ‡nâ„“ = âˆ‡nğ¸ # reuse memory
	âˆ‡negativeloglikelihood!(âˆ‡nâ„“, memory, model, ğ°)
	Jğ° = -ğ*Diagonal(ğ*ğâ‚€ğ›‰â‚˜â‚â‚š) #Jacobian matrix of the posterior mode ğ° with respect to the precisions ğ›‚
	âˆ‡nğ¸ .= Jğ°*(-âˆ‡nâ„“) - 0.5*ğ°.^2 + Jğ°*ğ€*ğ° - 0.5*(1.0./ğ›‚) + 0.5*diag(inv(ğ))
	return nothing
end

"""
	check_âˆ‡logevidence(model)

Check whether the hand-coded gradient of the log-evidence matches the automatic gradient

ARGUMENT
-`model`: structure containing the parameters and hyperparameters

RETURN
-absolute difference between the gradients
-absolute difference between the log-evidence functions

EXAMPLE
```julia-repl
julia> using FHMDDM
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_05_21_test/T176_2018_05_03/data.mat")
julia> FHMDDM.initializemodel!(model)
julia> FHMDDM.maximizeposterior!(model)
julia> absdiffâˆ‡ğ¸, absidffğ¸ = FHMDDM.check_âˆ‡logevidence(model)
```
"""
function check_âˆ‡logevidence(model::Model)
	memory = FHMDDM.Memoryforgradient(model)
	ğ€â‚€ = copy(model.precisionmatrix)
    ğ‡ = FHMDDM.âˆ‡âˆ‡loglikelihood(model)
	ğ›‰â‚˜â‚â‚š, indexÎ¸ = FHMDDM.concatenateparameters(model)
	ğâ‚€ğ›‰â‚˜â‚â‚š = (ğ€â‚€-ğ‡)*ğ›‰â‚˜â‚â‚š
	ğ›‚ = ones(length(ğ›‰â‚˜â‚â‚š))
	handcoded_gradient = similar(ğ›‰â‚˜â‚â‚š)
	handcoded_evidence = FHMDDM.logevidence!(memory, model, ğ›‚, ğ‡, ğâ‚€ğ›‰â‚˜â‚â‚š)
	FHMDDM.âˆ‡negativelogevidence!(handcoded_gradient, memory, model, ğ›‚, ğ‡, ğâ‚€ğ›‰â‚˜â‚â‚š)
    f(x) = FHMDDM.logevidence(x, ğâ‚€ğ›‰â‚˜â‚â‚š, ğ‡, indexÎ¸, model)
	automatic_evidence = f(ğ›‚)
	automatic_gradient = ForwardDiff.gradient(f, ğ›‚)
	return abs.(automatic_gradient .+ handcoded_gradient), abs(automatic_evidence-handcoded_evidence)
end

"""
	logevidence(ğ›‚, ğâ‚€ğ›‰â‚˜â‚â‚š, ğ‡, indexÎ¸, model)

ForwardDiff-computation evaluation of the log-evidence

ARGUMENT
-`ğ›‚`: diagonal of the precision matrix; these are the values that are being learned
-`ğâ‚€ğ›‰â‚˜â‚â‚š`: Hessian of the log-posterior evalued at the MAP values of the parameters multiplied by the MAP value of the parameters
-`ğ‡`: Hessian of the log-likelihood evaluated at the MAP values of the parameters
-`indexÎ¸`: index of the parameters
-`model`: structure containing the parameters, hyperparameters, and data

RETURN
-log of the marginal likelihood
"""
function logevidence(ğ›‚::Vector{<:Real},
					ğâ‚€ğ›‰â‚˜â‚â‚š::Vector{<:Real},
					ğ‡::Matrix{<:Real},
					indexÎ¸::IndexÎ¸,
					model::Model)
	ğ€ = Diagonal(ğ›‚)
	ğ = ğ€-ğ‡
    ğ° = ğ \ ğâ‚€ğ›‰â‚˜â‚â‚š
	FHMDDM.loglikelihood(ğ°, indexÎ¸, model) - 0.5*(transpose(ğ°)*ğ€*ğ°) - 0.5*logdet(ğ€) + 0.5*logdet(ğ)
end
