"""
	maximizeevidence!(model)

Learn both the parameters and hyperparameters by maximizing the evidence

The optimization procedure alternately fixes the hyperparameters and learn the parameters by maximizing the posterior and fixes the parameters and learn the hyperparameters by maximing the evidence.

MODIFIED ARGUMENT
-`model`: structure containing the parameters, hyperparameters, and data. The parameters and the precision matrix are updated maximize the evidence of the data

OPTIONAL ARGUMET
-`iterations`: maximum number of iterations for optimizing the posterior probability of the parameters
-`outer_iterations`: maximum number of iterations for alternating between maximizing the posterior probability and maximizing the evidence
-`verbose`: whether to display messages

EXAMPLE
```julia-repl
julia> using FHMDDM
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_06_05b_test/T176_2018_05_03/data.mat")
julia> initializeparameters!(model)
julia> maximizeevidence!(model)
```
"""
function maximizeevidence!(model::Model;
						iterations::Int = 500,
						max_consecutive_failures::Int=2,
						outer_iterations::Int=10,
						verbose::Bool=true,
						g_tol::Real=1e-3,
						x_reltol::Real=1e-1)
	memory = FHMDDM.Memoryforgradient(model)
	indexğ›‚ = FHMDDM.indexprecisions(model)
	ğ›‚â‚€ = model.precisionmatrix.diag[indexğ›‚]
	ğ€â‚€ = Diagonal(ğ›‚â‚€)
	bestğ¸ = -Inf
	bestğ›‚ = copy(ğ›‚â‚€)
	ğ›Ÿ, indexÎ¸ = FHMDDM.concatenateparameters(model)
	bestğ›Ÿ = copy(ğ›Ÿ)
	n_consecutive_failures = 0
	posteriorconverged = false
	for i = 1:outer_iterations
		sortparameters!(model, ğ›Ÿ, indexÎ¸)
	    gradientnorms = maximizeposterior!(model; iterations=iterations, g_tol=g_tol)[2]
		concatenatedÎ¸ = FHMDDM.concatenateparameters(model)[1]
		ğ›Ÿ .= concatenatedÎ¸
		if gradientnorms[findlast(x->!isnan(x), gradientnorms)] > g_tol
			two_times_geomean = 2geomean(model.precisionmatrix.diag[indexğ›‚])
			model.precisionmatrix.diag[indexğ›‚] .= two_times_geomean
			verbose && println("Outer iteration: ", i, ": because a critical point could not be found, the values of the precisions are set to be twice the geometric mean of the hyperparameters. New ğ›‚ â†’ ", two_times_geomean)
		else
			verbose && println("Outer iteration: ", i, ": the MAP values of the parameters converged")
			for j in eachindex(indexğ›‚)
				ğ›‚â‚€[j] = model.precisionmatrix.diag[indexğ›‚[j]]
			end
			stats = @timed âˆ‡âˆ‡loglikelihood(model)[indexğ›‚, indexğ›‚]
			ğ‡ = stats.value
			verbose && println("Outer iteration: ", i, ": computing the Hessian of the log-likelihood took ", stats.time, " seconds")
			ğ›‰â‚˜â‚â‚š = concatenatedÎ¸[indexğ›‚]
			ğâ‚€ğ›‰â‚˜â‚â‚š = (ğ€â‚€-ğ‡)*ğ›‰â‚˜â‚â‚š
			ğ¸ = logevidence!(concatenatedÎ¸, memory, model, ğ›‚â‚€, ğâ‚€ğ›‰â‚˜â‚â‚š, ğ‡, indexğ›‚)
			if ğ¸ > bestğ¸
				if verbose
					if posteriorconverged
						println("Outer iteration: ", i, ": the log-evidence (best: ", bestğ¸, "; new:", ğ¸, ") is improved by the new values of the precisions found in the previous outer iteration")
					else
						println("Outer iteration: ", i, ": initial value of log-evidence: ", ğ¸, " is set as the best log-evidence")
					end
				end
				bestğ¸ = ğ¸
				bestğ›‚ .= ğ›‚â‚€
				bestğ›Ÿ .= ğ›Ÿ
				n_consecutive_failures = 0
			else
				n_consecutive_failures += 1
				verbose && println("Outer iteration: ", i, ": because the log-evidence (best: ", bestğ¸, "; new:", ğ¸, ") was not improved by the new precisions, subsequent learning of the precisions will be begin at the midpoint between the current values of the precisions and the values that gave the best evidence so far.")
				for j in eachindex(indexğ›‚)
					model.precisionmatrix.diag[indexğ›‚[j]] = (model.precisionmatrix.diag[indexğ›‚[j]] + bestğ›‚[j])/2
				end
			end
			posteriorconverged = true
			if n_consecutive_failures == max_consecutive_failures
				verbose && println("Outer iteration: ", i, ": optimization halted early due to ", max_consecutive_failures, " consecutive failures in improving evidence")
				break
			end
		    normÎ” = maximizeevidence!(memory, model, ğ€â‚€, ğâ‚€ğ›‰â‚˜â‚â‚š, ğ‡, indexğ›‚, ğ›‰â‚˜â‚â‚š)
			if verbose
				println("Outer iteration ", i, ": new ğ›‚ â†’ ", model.precisionmatrix.diag[indexğ›‚])
			end
			if normÎ” < x_reltol
				if verbose
					println("Outer iteration: ", i, ": optimization halted after relative difference in the norm of the hyperparameters (in real space) decreased below ", x_reltol)
				end
				break
			end
		end
		if (i==outer_iterations) && verbose
			println("Optimization halted after reaching the last of ", outer_iterations, " allowed outer iterations.")
		end
	end
	println("Best log-evidence: ", bestğ¸)
	println("Best hyperparameters: ", bestğ›‚)
	println("Best parameters: ", bestğ›Ÿ)
	for j in eachindex(indexğ›‚)
		model.precisionmatrix.diag[indexğ›‚[j]] = bestğ›‚[j]
	end
	sortparameters!(model, bestğ›Ÿ, indexÎ¸)
	return nothing
end

"""
	maximizeevidence!(memory, model, ğ€â‚€, ğ‡, indexğ›‚, ğ›‰â‚˜â‚â‚š)

Learn hyperparameters by fixing the parameters of the model and maximizing the evidence

MODIFIED ARGUMENT
-`memory`: structure containing variables to be modified during computations
-`model`: structure containing the parameters, hyperparameters, and data. The parameter values are modified, but the hyperparameters are not modified
-`ğ€â‚€`: the precision matrix used to compute the MAP solution `ğ›‰â‚˜â‚â‚š`
-`ğ‡`: Hessian of the log-likelihood evaluated at the MAP solution `ğ›‰â‚˜â‚â‚š`
-`ğ›‰â‚˜â‚â‚š`: MAP solution computed using the precision matrix ğ€â‚€

RETURN
-Euclidean of the normalized difference in the log-precisions being optimized
"""
function maximizeevidence!(memory::Memoryforgradient,
						model::Model,
						ğ€â‚€::Diagonal,
						ğâ‚€ğ›‰â‚˜â‚â‚š::Vector{<:Real},
						ğ‡::Matrix{<:Real},
						indexğ›‚::Vector{<:Integer},
						ğ›‰â‚˜â‚â‚š::Vector{<:Real};
						Î±range::Vector{<:Real}=[1e-1, 1e2],
						optimizationoptions::Optim.Options=Optim.Options(iterations=15, show_trace=true, show_every=1),
						optimizer::Optim.FirstOrderOptimizer=LBFGS(linesearch=LineSearches.BackTracking()))
	concatentatedÎ¸ = FHMDDM.concatenateparameters(model)[1]
	function f(ğ±)
		ğ›‚ = real2native.(ğ±, Î±range[1], Î±range[2])
		-FHMDDM.logevidence!(concatentatedÎ¸, memory, model, ğ›‚, ğâ‚€ğ›‰â‚˜â‚â‚š, ğ‡, indexğ›‚)
	end
	function g!(âˆ‡nğ¸, ğ±)
		ğ›‚ = real2native.(ğ±, Î±range[1], Î±range[2])
		FHMDDM.âˆ‡negativelogevidence!(concatentatedÎ¸, memory, model, âˆ‡nğ¸, ğ›‚, ğâ‚€ğ›‰â‚˜â‚â‚š, ğ‡, indexğ›‚)
		for i in eachindex(âˆ‡nğ¸)
			âˆ‡nğ¸[i] *= differentiate_native_wrt_real(ğ±[i], Î±range[1], Î±range[2])
		end
		return nothing
	end
	ğ±â‚€ = native2real.(model.precisionmatrix.diag[indexğ›‚], Î±range[1], Î±range[2])
	optimizationresults = Optim.optimize(f, g!, ğ±â‚€, optimizer, optimizationoptions)
	ğ±Ì‚ = Optim.minimizer(optimizationresults)
	normÎ” = 0.0
	for i in eachindex(ğ±Ì‚)
		xâ‚€áµ¢ = native2real(ğ€â‚€.diag[i], Î±range[1], Î±range[2])
		normÎ” += (ğ±Ì‚[i]/xâ‚€áµ¢ - 1.0)^2
		model.precisionmatrix.diag[indexğ›‚[i]] = real2native(ğ±Ì‚[i], Î±range[1], Î±range[2])
	end
	return âˆšnormÎ”
end

"""
	logevidence!(memory, model, ğ›‚, ğâ‚€ğ›‰â‚˜â‚â‚š, ğ‡, indexğ›‚)

Log of the marginal likelihood

MODIFIED ARGUMENT
-`memory`: a structure containing memory for in-place computation
-`model`: structure containing the parameters, hyperparameters, and data

UNMODIFIED ARGUMENT
-`ğ€â‚€`: precision matrix that was fixed and used to find the MAP values of the parameters
-`ğ›‚`: precisions being learned
-`ğ‡`: Hessian of the log-likelihood evaluated at the MAP values of the parameters
-`indexğ›‚`: index of the precisions being fit within the full vector of concatenated precisions
-`ğâ‚€ğ›‰â‚˜â‚â‚š`: Hessian of the log-posterior evalued at the MAP values of the parameters multiplied by the MAP value of the parameters

RETURN
-log of the evidence
"""
function logevidence!(concatenatedÎ¸::Vector{<:Real},
					memory::Memoryforgradient,
					model::Model,
					ğ›‚::Vector{<:Real},
					ğâ‚€ğ›‰â‚˜â‚â‚š::Vector{<:Real},
					ğ‡::Matrix{<:Real},
					indexğ›‚::Vector{<:Integer})
	ğ€ = Diagonal(ğ›‚)
	ğ = ğ€-ğ‡
    ğ° = ğ \ ğâ‚€ğ›‰â‚˜â‚â‚š
	for i in eachindex(indexğ›‚)
		concatenatedÎ¸[indexğ›‚[i]] = ğ°[i]
	end
	loglikelihood!(model, memory, concatenatedÎ¸)
	logevidence(ğ€, ğ‡, memory.â„“[1], ğ°)
end

"""
	logevidence(ğ€, ğ‡, â„“, ğ°)

Evaluate the log-evidence

ARGUMENT
-`ğ€`: precision matrix, containing only the precisions being optimized
-`ğ‡`: Hessian of the log-likelihood evaluated at the MAP values of the parameters
-`â„“`: log-likelihood evaluated at the approximate posterior mode ğ°
-`ğ°`: approximate posterior mode as a function of ğ€â‚’â‚šâ‚œ for the precisions being optimized

RETURN
-log evidence
"""
function logevidence(ğ€::Diagonal, ğ‡::Matrix{<:Real}, â„“::Real, ğ°::Vector{<:Real})
	ğŒ = I - ğ€^-1*ğ‡
	if det(ğŒ) < 0
		println("negative determinant") # try/catch block is much slower than conditional branching
		-Inf
	else
		â„“ - 0.5dot(ğ°, ğ€, ğ°) - 0.5logdet(I - ğ€^-1*ğ‡)
	end
end

"""
	âˆ‡negativelogevidence!(âˆ‡nğ¸, memory, model, ğ›‚, ğ‡, ğâ‚€ğ›‰â‚˜â‚â‚š)

gradient of the negative log of the marginal likelihood

MODIFIED ARGUMENT
-`âˆ‡nğ¸`: gradient of the negative of the log-evidence
-`ğ€`: precision matrix
-`memory`: a structure containing memory for in-place computation
-`model`: structure containing the parameters, hyperparameters, and data
-`ğ°`: posterior mode as a function of the hyperparameters

UNMODIFIED ARGUMENT
-`ğ›‚`: precisions being learned
-`ğ‡`: Hessian of the log-likelihood evaluated at the MAP values of the parameters, including only rows and columns corresponding to the hyperparameters being optimized
-`indexğ›‚`: index of the precisions being fit within the full vector of concatenated precisions
-`ğâ‚€ğ›‰â‚˜â‚â‚š`: Hessian of the log-posterior evalued at the MAP values of the parameters multiplied by the MAP value of the parameters
"""
function âˆ‡negativelogevidence!(concatenatedÎ¸::Vector{<:Real},
								memory::Memoryforgradient,
								model::Model,
								âˆ‡nğ¸::Vector{<:Real},
								ğ›‚::Vector{<:Real},
								ğâ‚€ğ›‰â‚˜â‚â‚š::Vector{<:Real},
								ğ‡::Matrix{<:Real},
								indexğ›‚::Vector{<:Integer})
	ğ€ = Diagonal(ğ›‚)
	ğ = ğ€-ğ‡
    ğ° = ğ \ ğâ‚€ğ›‰â‚˜â‚â‚š
	for i in eachindex(indexğ›‚)
		concatenatedÎ¸[indexğ›‚[i]] = ğ°[i]
	end
	âˆ‡nâ„“ = similar(concatenatedÎ¸)
	âˆ‡negativeloglikelihood!(âˆ‡nâ„“, memory, model, concatenatedÎ¸)
	ğ‰ = -ğ \ Diagonal(ğ°) #Jacobian matrix of the posterior mode ğ° with respect to the precisions ğ›‚
	âˆ‡nğ¸ .= ğ‰'*(âˆ‡nâ„“[indexğ›‚] .- 0.5.*ğâ‚€ğ›‰â‚˜â‚â‚š .+ ğ€*ğ°)
	ğš² = I - ğ€^-1*ğ‡
	ğ = zeros(size(ğš²));
	@inbounds for i in eachindex(ğ›‚)
		if i > 1
	    	ğ[i-1,:] .= 0.0
		end
	    ğ[i,:] = ğ›‚[i]^-2 .* ğ‡[i,:]
	    âˆ‡nğ¸[i] += 0.5tr(ğš² \ ğ)
	end
	return nothing
end

"""
	check_âˆ‡logevidence(model)

Check whether the hand-coded gradient of the log-evidence matches the automatic gradient

ARGUMENT
-`model`: structure containing the parameters and hyperparameters

OPTIONAL ARGUMENT
-`simulate`: whether to simulate Hessian and MAP solution. If not, the model is first fitted before a Hessian is computed

RETURN
-maximum absolute normalized difference between the gradients
-absolute normalized difference between the log-evidence functions

EXAMPLE
```julia-repl
julia> using FHMDDM
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_06_05b_test/T176_2018_05_03/data.mat")
julia> max_abs_norm_diff_âˆ‡ğ¸, abs_norm_diff_ğ¸ = FHMDDM.check_âˆ‡logevidence(model; simulate=true)
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_06_05b_test/T176_2018_05_03/data.mat")
julia> max_abs_norm_diff_âˆ‡ğ¸, abs_norm_diff_ğ¸ = FHMDDM.check_âˆ‡logevidence(model; simulate=false)
```
"""
function check_âˆ‡logevidence(model::Model; simulate::Bool=true)
	indexğ›‚ = FHMDDM.indexprecisions(model)
	D = length(indexğ›‚)
	ğ›‚ = rand(D)
	concatenatedÎ¸, indexÎ¸ = FHMDDM.concatenateparameters(model)
	memory = FHMDDM.Memoryforgradient(model)
	ğ›‚â‚€ = model.precisionmatrix.diag[indexğ›‚]
	ğ€â‚€ = Diagonal(ğ›‚â‚€)
	if simulate
		ğ‘ = 1 .- 2rand(D,D)
		ğ‡ = Diagonal(ğ›‚) - transpose(ğ‘)*ğ‘
		ğ€ = Diagonal(ğ›‚)
		ğ = ğ€-ğ‡
		ğ° = 1 .- 2rand(D)
		ğ° ./= norm(ğ°)
		ğâ‚€ğ›‰â‚˜â‚â‚š = ğ*ğ°
		ğ›‰â‚˜â‚â‚š = (ğ€â‚€-ğ‡) \ ğâ‚€ğ›‰â‚˜â‚â‚š
		for i in eachindex(indexğ›‚)
			concatenatedÎ¸[indexğ›‚[i]] = ğ°[i]
		end
		FHMDDM.sortparameters!(model, concatenatedÎ¸, indexÎ¸)
		FHMDDM.real2native!(model.Î¸native, model.options, model.Î¸real)
	else
		FHMDDM.initializeparameters!(memory, model)
		FHMDDM.maximizeposterior!(model)
		ğ‡ = âˆ‡âˆ‡loglikelihood(model)[indexğ›‚, indexğ›‚]
		ğ›‰â‚˜â‚â‚š = concatenateparameters(model)[1][indexğ›‚]
		ğâ‚€ğ›‰â‚˜â‚â‚š = (ğ€â‚€-ğ‡)*ğ›‰â‚˜â‚â‚š
	end
	handcoded_evidence = FHMDDM.logevidence!(concatenatedÎ¸, memory, model, ğ›‚, ğâ‚€ğ›‰â‚˜â‚â‚š, ğ‡, indexğ›‚)
	handcoded_gradient = fill(NaN,D)
	FHMDDM.âˆ‡negativelogevidence!(concatenatedÎ¸, memory, model, handcoded_gradient, ğ›‚, ğâ‚€ğ›‰â‚˜â‚â‚š, ğ‡, indexğ›‚)
    f(x) = FHMDDM.logevidence(x, ğâ‚€ğ›‰â‚˜â‚â‚š, ğ‡, indexğ›‚, model)
	automatic_evidence = f(ğ›‚)
	automatic_gradient = ForwardDiff.gradient(f, ğ›‚)
	return maximum(abs.((automatic_gradient .+ handcoded_gradient)./automatic_gradient)), abs((automatic_evidence-handcoded_evidence)/automatic_evidence)
end

"""
	logevidence(ğ›‚, ğâ‚€ğ›‰â‚˜â‚â‚š, ğ‡, indexÎ¸, model)

ForwardDiff-computation evaluation of the log-evidence

ARGUMENT
-`ğ›‚`: precisions being learned
-`ğâ‚€ğ›‰â‚˜â‚â‚š`: Hessian of the log-posterior evalued at the MAP values of the parameters multiplied by the MAP value of the parameters
-`ğ‡`: Hessian of the log-likelihood evaluated at the MAP values of the parameters
-`indexğ›‚`: index of the precisions being fit within the full vector of concatenated precisions
-`indexÎ¸`: index of the parameters
-`model`: structure containing the parameters, hyperparameters, and data

RETURN
-log of the marginal likelihood
"""
function logevidence(ğ›‚::Vector{type},
					ğâ‚€ğ›‰â‚˜â‚â‚š::Vector{<:Real},
					ğ‡::Matrix{<:Real},
					indexğ›‚::Vector{<:Integer},
					model::Model) where{type<:Real}
	ğ€ = Diagonal(ğ›‚)
	ğ = ğ€-ğ‡
    ğ° = ğ \ ğâ‚€ğ›‰â‚˜â‚â‚š
	concatenatedÎ¸, indexÎ¸ = concatenateparameters(model)
	concatenatedÎ¸ = concatenatedÎ¸ .- zero(type)
	@inbounds for i in eachindex(indexğ›‚)
		concatenatedÎ¸[indexğ›‚[i]] = ğ°[i]
	end
	â„“ = FHMDDM.loglikelihood(concatenatedÎ¸, indexÎ¸, model)
	logevidence(ğ€, ğ‡, â„“, ğ°)
end

"""
    geomean(a)
Return the geometric mean of a real-valued vector.
"""
function geomean(a::Vector{<:Real})
    s = 0.0
    n = length(a)
    for i = 1 : n
        @inbounds s += log(a[i])
    end
    return exp(s / n)
end
