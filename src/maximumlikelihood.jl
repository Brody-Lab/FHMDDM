"""
	functions

-maximizelikelihood!(model, optimizer::Optim.FirstOrderOptimizer)
-maximizelikelihood!(model, optimizer::Flux.Optimise.AbstractOptimiser)
-loglikelihood(model)
-loglikelihood!(model, memory, concatenatedparameters)
"""

"""
    maximizelikelihood!(model, optimizer)

Optimize the parameters of the factorial hidden Markov drift-diffusion model using a first-order optimizer in Optim

MODIFIED ARGUMENT
- a structure containing information for a factorial hidden Markov drift-diffusion model

UNMODIFIED ARGUMENT
-`optimizer`: an optimizer implemented by Optim.jl. The limited memory quasi-Newton algorithm `LBFGS()` does pretty well, and when using L-BFGS the `HagerZhang()` line search seems to do better than `BackTracking()`

OPTIONAL ARGUMENT
-`extended_trace`: save additional information
-`f_tol`: threshold for determining convergence in the objective value
-`g_tol`: threshold for determining convergence in the gradient
-`iterations`: number of inner iterations that will be run before the optimizer gives up
-`show_every`: trace output is printed every `show_every`th iteration.
-`show_trace`: should a trace of the optimizer's state be shown?
-`x_tol`: threshold for determining convergence in the input vector

RETURN
`losses`: value of the loss function (negative of the model's log-likelihood) across iterations. If `store_trace` were set to false, then these are NaN's
`gradientnorms`: 2-norm of the gradient of  of the loss function across iterations. If `store_trace` were set to false, then these are NaN's
"""
function maximizelikelihood!(model::Model,
							 optimizer::Optim.FirstOrderOptimizer;
			                 extended_trace::Bool=false,
			                 f_tol::AbstractFloat=0.0,
			                 iterations::Integer=500,
			                 show_every::Integer=10,
			                 show_trace::Bool=true,
							 store_trace::Bool=true,
			                 x_tol::AbstractFloat=0.0)
	memory = Memoryforgradient(model)
    f(concatenatedŒ∏) = -loglikelihood!(model, memory, concatenatedŒ∏)
    g!(‚àá, concatenatedŒ∏) = ‚àánegativeloglikelihood!(‚àá, memory, model, concatenatedŒ∏)
    Optim_options = Optim.Options(extended_trace=extended_trace,
								  f_tol=f_tol,
                                  g_tol=model.options.g_tol,
                                  iterations=iterations,
                                  show_every=show_every,
                                  show_trace=show_trace,
								  store_trace=store_trace,
                                  x_tol=x_tol)
	Œ∏‚ÇÄ = concatenateparameters(model)
	optimizationresults = Optim.optimize(f, g!, Œ∏‚ÇÄ, optimizer, Optim_options)
    Œ∏‚Çò‚Çó = Optim.minimizer(optimizationresults)
	sortparameters!(model, Œ∏‚Çò‚Çó, memory.indexŒ∏)
	real2native!(model.Œ∏native, model.options, model.Œ∏real)
	println(optimizationresults)
	losses, gradientnorms = fill(NaN, iterations+1), fill(NaN, iterations+1)
	if store_trace
		traces = Optim.trace(optimizationresults)
		for i in eachindex(traces)
			gradientnorms[i] = traces[i].g_norm
			losses[i] = traces[i].value
		end
	end
    return losses, gradientnorms
end

"""
    loglikelihood!(model, memory, concatenatedŒ∏)

Compute the log-likelihood

MODIFIED ARGUMENT
-`model`: an instance of FHM-DDM
-`memory`: a container of variables used by both the log-likelihood and gradient computation

UNMODIFIED ARGUMENT
-`concatenatedŒ∏`: a vector of concatenated parameter values

RETURN
-log-likelihood
```
"""
function loglikelihood!(model::Model, memory::Memoryforgradient, concatenatedŒ∏::Vector{<:Real})
	if (concatenatedŒ∏ != memory.concatenatedŒ∏) || isnan(memory.‚Ñì[1])
		P = update!(memory, model, concatenatedŒ∏)
		memory.‚Ñì[1] = 0.0
		log_s = log(model.options.sf_y)
		for trialset in model.trialsets
			N = length(trialset.mpGLMs)
			for trial in trialset.trials
				T = trial.ntimesteps
				memory.‚Ñì[1] -= N*T*log_s
				forward!(memory, P, model.Œ∏native, trial)
			end
		end
	end
	memory.‚Ñì[1]
end

"""
    loglikelihood(concatenatedŒ∏, indexŒ∏, model)

ForwardDiff-compatible computation of the log-likelihood

ARGUMENT
-`concatenatedŒ∏`: a vector of concatenated parameter values
-`indexŒ∏`: struct indexing of each parameter in the vector of concatenated values
-`model`: an instance of FHM-DDM

RETURN
-log-likelihood
"""
function loglikelihood(concatenatedŒ∏::Vector{type}, indexŒ∏::IndexŒ∏, model::Model) where {type<:Real}
	model = Model(concatenatedŒ∏, indexŒ∏, model)
	@unpack options, Œ∏native, Œ∏real, trialsets = model
	@unpack Œît, minpa, sf_y, Œû = options
	pùêòùëë=map(model.trialsets) do trialset
			map(trialset.trials) do trial
				map(1:trial.ntimesteps) do t
					ones(type,Œû)
				end
			end
		end
    scaledlikelihood!(pùêòùëë, trialsets, Œ∏native.œà[1])
	choiceLLscaling = scale_factor_choiceLL(model)
	A·µÉinput = ones(type,Œû,Œû).*minpa
	one_minus_Œûminpa = 1.0-Œû*minpa
	A·µÉinput[1,1] += one_minus_Œûminpa
	A·µÉinput[Œû,Œû] += one_minus_Œûminpa
	A·µÉsilent = copy(A·µÉinput)
	expŒªŒît = exp(Œ∏native.Œª[1]*Œît)
	dŒº_dŒîc = differentiate_Œº_wrt_Œîc(Œît, Œ∏native.Œª[1])
	dùõè_dB = (2 .*collect(1:Œû) .- Œû .- 1)./(Œû-2)
	ùõè = Œ∏native.B[1].*dùõè_dB
	transitionmatrix!(A·µÉsilent, minpa, expŒªŒît.*ùõè, ‚àö(Œît*Œ∏native.œÉ¬≤‚Çê[1]), ùõè)
	log_s = log(sf_y)
	‚Ñì = zero(type)
	for s in eachindex(trialsets)
		nneurons = length(trialsets[s].mpGLMs)
		for m in eachindex(trialsets[s].trials)
			trial = trialsets[s].trials[m]
			‚Ñì-=nneurons*trial.ntimesteps*log_s
			pùêö‚Çú = probabilityvector(minpa, Œ∏native.Œº‚ÇÄ[1]+Œ∏native.w‚Çï[1]*trial.previousanswer, ‚àöŒ∏native.œÉ¬≤·µ¢[1], ùõè)
			f = pùêòùëë[s][m][1] .* pùêö‚Çú
			D = sum(f)
			D = max(D, nextfloat(0.0))
			f./=D
			‚Ñì+=log(D)
			adaptedclicks = adapt(trial.clicks, Œ∏native.k[1], Œ∏native.œï[1])
			for t=2:trial.ntimesteps
				if t ‚àà trial.clicks.inputtimesteps
					cL = sum(adaptedclicks.C[trial.clicks.left[t]])
					cR = sum(adaptedclicks.C[trial.clicks.right[t]])
					ùõç = expŒªŒît.*ùõè .+ (cR-cL).*dŒº_dŒîc
					œÉ = ‚àö((cR+cL)*Œ∏native.œÉ¬≤‚Çõ[1] + Œît*Œ∏native.œÉ¬≤‚Çê[1])
					transitionmatrix!(A·µÉinput, minpa, ùõç, œÉ, ùõè)
					A·µÉ = A·µÉinput
				else
					A·µÉ = A·µÉsilent
				end
				f = pùêòùëë[s][m][t] .* (A·µÉ * f)
				D = sum(f)
				D = max(D, nextfloat(0.0))
				f./=D
				‚Ñì+=log(D)
				if choiceLLscaling > 1
					pùêö‚Çú = A·µÉ*pùêö‚Çú
				end
			end
			if choiceLLscaling > 1
				pùëë_a = ones(type, Œû)
				conditionallikelihood!(pùëë_a, trial.choice, Œ∏native.œà[1])
				‚Ñì += (choiceLLscaling-1)*log(dot(pùëë_a, pùêö‚Çú))
			end
		end
	end
	‚Ñì
end

"""
	loglikelihood(model)

Log of the likelihood of the data given the parameters

This function is called when summarizing the model
"""
loglikelihood(model::Model) = loglikelihood!(model, Memoryforgradient(model), concatenateparameters(model))

"""
	‚àánegativeloglikelihood!(‚àán‚Ñì, memory, model, concatenatedŒ∏)

Update the gradient of the negative log-likelihood of the model

MODIFIED ARGUMENT
-`‚àán‚Ñì`: the vector representing the gradient
-`memory`: memory allocated for computing the gradient. The log-likelihood is updated.
-`model`: structure containing the data, parameters, and hyperparameters of the model

ARGUMENT
-`concatenatedŒ∏`: values of the model's parameters concatenated into a vector
"""
function ‚àánegativeloglikelihood!(‚àán‚Ñì::Vector{<:Real}, memory::Memoryforgradient, model::Model, concatenatedŒ∏::Vector{<:AbstractFloat})
	if concatenatedŒ∏ != memory.concatenatedŒ∏
		P = update!(memory, model, concatenatedŒ∏)
	else
		P = Probabilityvector(model.options.Œît, model.options.minpa, model.Œ∏native, model.options.Œû)
	end
	‚àáloglikelihood!(memory,model,P)
	indexall = 0
	indexfit = 0
	for field in fieldnames(LatentŒ∏)
		indexall+=1
		if getfield(memory.indexŒ∏.latentŒ∏, field)[1] > 0
			indexfit +=1
			‚àán‚Ñì[indexfit] = -memory.‚àá‚Ñìlatent[indexall]
		end
	end
	native2real!(‚àán‚Ñì, memory.indexŒ∏.latentŒ∏, model)
	‚àá‚Ñìglm = vcat((vcat((FHMDDM.concatenateparameters(‚àá) for ‚àá in ‚àás)...) for ‚àás in memory.‚àá‚Ñìglm)...)
	for i in eachindex(‚àá‚Ñìglm)
		‚àán‚Ñì[indexfit+i] = -‚àá‚Ñìglm[i]
	end
	return nothing
end

"""
	‚àáloglikelihood!(memory, model, P)

Compute the gradient of the log-likelihood within the fields of an object of composite type `Memoryforgradient`

MODIFIED ARGUMENT
-`memory`: memory allocated for computing the gradient. The log-likelihood is updated.
-`model`: structure containing the data, parameters, and hyperparameters of the model
-`P`: a structure containing allocated memory for computing the accumulator's initial and transition probabilities as well as the partial derivatives of these probabilities
"""
function ‚àáloglikelihood!(memory::Memoryforgradient, model::Model, P::Probabilityvector)
	memory.‚Ñì .= 0.0
	memory.‚àá‚Ñìlatent .= 0.0
	for trialset in model.trialsets
		for trial in trialset.trials
			‚àáloglikelihood!(memory, model, P, trial)
		end
	end
	for s in eachindex(model.trialsets)
		for n = 1:length(model.trialsets[s].mpGLMs)
			expectation_‚àáloglikelihood!(memory.‚àá‚Ñìglm[s][n], memory.Œ≥[s], model.trialsets[s].mpGLMs[n])
		end
	end
	return nothing
end

"""
	‚àáloglikelihood!(memory, model, P, trial)

Update the gradient of the log-likelihood of the model

MODIFIED ARGUMENT
-`memory`: memory allocated for computing the gradient. The log-likelihood is updated.
-`model`: structure containing the data, parameters, and hyperparameters of the model
-`P`: a structure containing allocated memory for computing the accumulator's initial and transition probabilities as well as the partial derivatives of these probabilities
-`trial`: an object containing of the data of one trial
"""
function ‚àáloglikelihood!(memory::Memoryforgradient, model::Model, P::Probabilityvector, trial::Trial)
	pùêòùëë = memory.pùêòùëë[trial.trialsetindex][trial.index_in_trialset]
	trialset = model.trialsets[trial.trialsetindex]
	@unpack Œ∏native = model
	@unpack clicks = trial
	@unpack inputtimesteps, inputindex = clicks
	@unpack A·µÉinput, ‚àáA·µÉinput, A·µÉsilent, ‚àáA·µÉsilent, choiceLLscaling, D, f, f·∂ú, indexŒ∏_pa‚ÇÅ, indexŒ∏_pa‚Çúa‚Çú‚Çã‚ÇÅ, indexŒ∏_œà, ‚Ñì, ‚àá‚Ñìlatent, nŒ∏_pa‚ÇÅ, nŒ∏_pa‚Çúa‚Çú‚Çã‚ÇÅ, ‚àápa‚ÇÅ, Œû = memory
	adaptedclicks = ‚àáadapt(trial.clicks, Œ∏native.k[1], Œ∏native.œï[1])
	‚Ñì[1] -= length(trialset.mpGLMs)*trial.ntimesteps*log(model.options.sf_y)
	t = 1
	‚àápriorprobability!(‚àápa‚ÇÅ, P, trial.previousanswer)
	f·∂ú[t] = copy(P.ùõë)
	pa‚ÇÅ = f·∂ú[t]
	for i=1:Œû
		f[t][i] = pùêòùëë[t][i] * pa‚ÇÅ[i]
	end
	D[t] = max(sum(f[t]), nextfloat(0.0))
	f[t] ./= D[t]
	‚Ñì[1] += log(D[t])
	for t=2:trial.ntimesteps
		if t ‚àà clicks.inputtimesteps
			clickindex = clicks.inputindex[t][1]
			A·µÉ = A·µÉinput[clickindex]
			‚àáA·µÉ = ‚àáA·µÉinput[clickindex]
			update_for_‚àátransition_probabilities!(P, adaptedclicks, clicks, t)
			‚àátransitionmatrix!(‚àáA·µÉ, A·µÉ, P)
		else
			A·µÉ = A·µÉsilent
		end
		f[t] = pùêòùëë[t] .* (A·µÉ * f[t-1])
		D[t] = sum(f[t])
		D[t] = max(D[t], nextfloat(0.0))
		f[t] ./= D[t]
		‚Ñì[1] += log(D[t])
		if choiceLLscaling > 1
			f·∂ú[t] = A·µÉ*f·∂ú[t-1]
		end
	end
	b = ones(Œû)
	f‚®Äb = f # reuse memory
	‚àá‚Ñìlatent[indexŒ∏_œà[1]] += expectation_derivative_logpùëë_wrt_œà(trial.choice, f‚®Äb[trial.ntimesteps], Œ∏native.œà[1])
	if choiceLLscaling > 1
		pùëë_a = ones(Œû)
		conditionallikelihood!(pùëë_a, trial.choice, Œ∏native.œà[1])
		f·∂ú[trial.ntimesteps] .*= pùëë_a
		D·∂ú = sum(f·∂ú[trial.ntimesteps])
		D·∂ú = max(D·∂ú, nextfloat(0.0))
		‚Ñì[1] += (choiceLLscaling-1)*log(D·∂ú)
		f·∂ú[trial.ntimesteps] ./= D·∂ú
		b·∂ú = pùëë_a./D·∂ú # backward term for the last time step
		‚àá‚Ñìlatent[indexŒ∏_œà[1]] += (choiceLLscaling-1)*expectation_derivative_logpùëë_wrt_œà(trial.choice, f·∂ú[trial.ntimesteps], Œ∏native.œà[1])
	end
	for t = trial.ntimesteps:-1:1
		if t < trial.ntimesteps
			if t+1 ‚àà clicks.inputtimesteps
				clickindex = clicks.inputindex[t+1][1]
				A·µÉ‚Çú‚Çä‚ÇÅ = A·µÉinput[clickindex]
			else
				A·µÉ‚Çú‚Çä‚ÇÅ = A·µÉsilent
			end
			b = transpose(A·µÉ‚Çú‚Çä‚ÇÅ) * (b.*pùêòùëë[t+1]./D[t+1])
			f‚®Äb[t] .*= b
			if choiceLLscaling > 1
				b·∂ú = transpose(A·µÉ‚Çú‚Çä‚ÇÅ) * b·∂ú
			end
		end
		if t > 1
			if t ‚àà clicks.inputtimesteps
				clickindex = clicks.inputindex[t][1]
				A·µÉ = A·µÉinput[clickindex]
				‚àáA·µÉ = ‚àáA·µÉinput[clickindex]
			else
				A·µÉ = A·µÉsilent
				‚àáA·µÉ = ‚àáA·µÉsilent
			end
			for i = 1:nŒ∏_pa‚Çúa‚Çú‚Çã‚ÇÅ
				‚àá‚Ñìlatent[indexŒ∏_pa‚Çúa‚Çú‚Çã‚ÇÅ[i]] += sum_product_over_states(D[t], f[t-1], b, pùêòùëë[t], ‚àáA·µÉ[i])
			end
			if choiceLLscaling > 1
				for i = 1:nŒ∏_pa‚Çúa‚Çú‚Çã‚ÇÅ
					‚àá‚Ñìlatent[indexŒ∏_pa‚Çúa‚Çú‚Çã‚ÇÅ[i]] += (choiceLLscaling-1)*(transpose(b·∂ú)*‚àáA·µÉ[i]*f·∂ú[t-1])[1]
				end
			end
		end
	end
	t = 1
	for i = 1:nŒ∏_pa‚ÇÅ
		‚àá‚Ñìlatent[indexŒ∏_pa‚ÇÅ[i]] += sum_product_over_states(D[t], b, pùêòùëë[t], ‚àápa‚ÇÅ[i])
	end
	if choiceLLscaling > 1
		for i = 1:nŒ∏_pa‚ÇÅ
			‚àá‚Ñìlatent[indexŒ∏_pa‚ÇÅ[i]] += (choiceLLscaling-1)*dot(b·∂ú, ‚àápa‚ÇÅ[i])
		end
	end
	for t = 1:trial.ntimesteps
		œÑ = trial.œÑ‚ÇÄ+t
		for i = 1:Œû
			memory.Œ≥[trial.trialsetindex][i][œÑ] = f‚®Äb[t][i]
		end
	end
	return nothing
end

"""
	Memoryforgradient(model)

Create variables that are memory by the computations of the log-likelihood and its gradient

ARGUMENT
-`model`: structure with information about the factorial hidden Markov drift-diffusion model

OUTPUT
-an instance of the custom type `Memoryforgradient`, which contains the memory quantities
```
"""
function Memoryforgradient(model::Model; choicemodel::Bool=false)
	@unpack options, Œ∏native = model
	@unpack Œît, minpa, Œû = options
	maxclicks = maximum_number_of_clicks(model)
	maxtimesteps = maximum_number_of_time_steps(model)
	if choicemodel
		concatenatedŒ∏, indexŒ∏ = concatenate_choice_related_parameters(model)
	else
		concatenatedŒ∏ = concatenateparameters(model)
		indexŒ∏ = indexparameters(model)
	end
	‚àá‚Ñìglm = map(model.trialsets) do trialset
				map(trialset.mpGLMs) do mpGLM
					GLMŒ∏(eltype(mpGLM.Œ∏.ùêÆ), mpGLM.Œ∏)
				end
			end
	one_minus_Œûminpa = 1.0 - Œû*minpa
	A·µÉinput=map(1:maxclicks) do t
				A = fill(minpa,Œû,Œû)
				A[1,1] += one_minus_Œûminpa
				A[Œû,Œû] += one_minus_Œûminpa
				return A
			end
	pùêòùëë = map(model.trialsets) do trialset
			map(trialset.trials) do trial
				map(1:trial.ntimesteps) do t
					ones(Œû)
				end
			end
		end
	Œ≥ =	map(model.trialsets) do trialset
			map(1:Œû) do index
				zeros(trialset.ntimesteps)
			end
		end
	memory = Memoryforgradient(A·µÉinput=A·µÉinput,
								choiceLLscaling = scale_factor_choiceLL(model),
								concatenatedŒ∏ = similar(concatenatedŒ∏),
								indexŒ∏=indexŒ∏,
								Œ≥=Œ≥,
								maxclicks=maxclicks,
								maxtimesteps=maxtimesteps,
								‚àá‚Ñìglm=‚àá‚Ñìglm,
								pùêòùëë=pùêòùëë,
								Œû=Œû)
	return memory
end

"""
	update!(memory, model, concatenatedŒ∏)

Update the model and the memory quantities according to new parameter values

MODIFIED ARGUMENT
-`memory`: structure containing variables memory between computations of the model's log-likelihood and its gradient
-`model`: structure with information concerning a factorial hidden Markov drift-diffusion model

ARGUMENT
-`concatenatedŒ∏`: newest values of the model's parameters

RETURN
-`P`: an instance of `Probabilityvector`
```
"""
function update!(memory::Memoryforgradient, model::Model, concatenatedŒ∏::Vector{<:Real})
	@unpack options, Œ∏native, Œ∏real = model
	memory.concatenatedŒ∏ .= concatenatedŒ∏
	sortparameters!(model, memory.concatenatedŒ∏, memory.indexŒ∏)
	real2native!(Œ∏native, options, Œ∏real)
	if !isempty(memory.pùêòùëë[1][1][1])
	    scaledlikelihood!(memory.pùêòùëë, model.trialsets, Œ∏native.œà[1])
	end
	P = update_for_‚àálatent_dynamics!(memory, options, Œ∏native)
	return P
end

"""
	update!(memory, model)

"""
function update!(memory::Memoryforgradient, model::Model)
	P = update!(memory, model, concatenateparameters(model))
	return P
end

"""
	update_for_‚àálatent_dynamics!(memory, options, Œ∏native)

Update quantities for computing the gradient of the prior and transition probabilities of the latent variables

MODIFIED ARGUMENT
-`memory`: structure containing variables memory between computations of the model's log-likelihood and its gradient

UNMODIFIED ARGUMENT
-`options`: settings of the model
-`Œ∏native`: values of the parameters that control the latent variables, in the parameters' native space

RETURN
-`P`: an instance of `Probabilityvector`
"""
function update_for_‚àálatent_dynamics!(memory::Memoryforgradient, options::Options, Œ∏native::LatentŒ∏)
	P = Probabilityvector(options.Œît, options.minpa, Œ∏native, options.Œû)
	update_for_‚àátransition_probabilities!(P)
	‚àátransitionmatrix!(memory.‚àáA·µÉsilent, memory.A·µÉsilent, P)
	return P
end

"""
	scale_factor_choiceLL(model)

Scaling factor for the log-likelihood of behavioral choices
"""
function scale_factor_choiceLL(model::Model)
	a = model.options.choiceLL_scaling_exponent
	if a==0
		1.0
	else
		ntimesteps_neurons = sum(collect(trialset.ntimesteps*length(trialset.mpGLMs) for trialset in model.trialsets))
		ntrials = sum(collect(trialset.ntrials for trialset in model.trialsets))
		(ntimesteps_neurons/ntrials)^a
	end
end

"""
	maximum_number_of_clicks(model)

Return the maximum number of clicks across all trials.

The stereoclick is excluded from this analysis as well as all other analyses.
"""
function maximum_number_of_clicks(model::Model)
	maxclicks = 0
	for trialset in model.trialsets
		for trial in trialset.trials
			maxclicks = max(maxclicks, length(trial.clicks.time))
		end
	end
	return maxclicks
end

"""
	maximum_number_of_time_steps(model)

Return the maximum number of time steps across all trials
"""
function maximum_number_of_time_steps(model::Model)
	maxtimesteps = 0
	for trialset in model.trialsets
		for trial in trialset.trials
			maxtimesteps = max(maxtimesteps, trial.ntimesteps)
		end
	end
	return maxtimesteps
end
