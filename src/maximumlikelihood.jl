"""
	functions

-maximizelikelihood!(model, optimizer::Optim.FirstOrderOptimizer)
-loglikelihood(model)
-loglikelihood!(model, memory, concatenatedparameters)
"""

"""
    maximizelikelihood!(model, optimizer)

Optimize the parameters of the factorial hidden Markov drift-diffusion model using a first-order optimizer in Optim

MODIFIED ARGUMENT
- a structure containing information for a factorial hidden Markov drift-diffusion model

UNMODIFIED ARGUMENT
-`optimizer`: an optimizer implemented by Optim.jl. The limited memory quasi-Newton algorithm `LBFGS()` does pretty well, and when using L-BFGS the `HagerZhang()` line search seems to do better than `BackTracking()`

OPTIONAL ARGUMENT
-`extended_trace`: save additional information
-`f_tol`: threshold for determining convergence in the objective value
-`g_tol`: threshold for determining convergence in the gradient
-`iterations`: number of inner iterations that will be run before the optimizer gives up
-`show_every`: trace output is printed every `show_every`th iteration.
-`show_trace`: should a trace of the optimizer's state be shown?
-`x_tol`: threshold for determining convergence in the input vector

RETURN
`losses`: value of the loss function (negative of the model's log-likelihood) across iterations. If `store_trace` were set to false, then these are NaN's
`gradientnorms`: 2-norm of the gradient of  of the loss function across iterations. If `store_trace` were set to false, then these are NaN's
"""
function maximizelikelihood!(model::Model,
							 optimizer::Optim.FirstOrderOptimizer;
			                 extended_trace::Bool=false,
			                 f_tol::AbstractFloat=0.0,
			                 iterations::Integer=500,
			                 show_every::Integer=10,
			                 show_trace::Bool=true,
							 store_trace::Bool=true,
			                 x_tol::AbstractFloat=0.0)
	memory = Memoryforgradient(model)
    f(concatenatedÎ¸) = -loglikelihood!(model, memory, concatenatedÎ¸)
    g!(âˆ‡, concatenatedÎ¸) = âˆ‡negativeloglikelihood!(âˆ‡, memory, model, concatenatedÎ¸)
    Optim_options = Optim.Options(extended_trace=extended_trace,
								  f_tol=f_tol,
                                  g_tol=model.options.g_tol,
                                  iterations=iterations,
                                  show_every=show_every,
                                  show_trace=show_trace,
								  store_trace=store_trace,
                                  x_tol=x_tol)
	Î¸â‚€ = concatenateparameters(model)
	optimizationresults = Optim.optimize(f, g!, Î¸â‚€, optimizer, Optim_options)
    Î¸â‚˜â‚— = Optim.minimizer(optimizationresults)
	sortparameters!(model, Î¸â‚˜â‚—, memory.indexÎ¸)
	real2native!(model.Î¸native, model.options, model.Î¸real)
	println(optimizationresults)
	losses, gradientnorms = fill(NaN, iterations+1), fill(NaN, iterations+1)
	if store_trace
		traces = Optim.trace(optimizationresults)
		for i in eachindex(traces)
			gradientnorms[i] = traces[i].g_norm
			losses[i] = traces[i].value
		end
	end
    return losses, gradientnorms
end

"""
	loglikelihood(model)

Log of the likelihood of the data given the parameters
"""
loglikelihood(model::Model) = loglikelihood!(model, Memoryforgradient(model), concatenateparameters(model))

"""
    loglikelihood!(model, memory, concatenatedÎ¸)

Compute the log-likelihood

MODIFIED ARGUMENT
-`model`: an instance of FHM-DDM
-`memory`: a container of variables used by both the log-likelihood and gradient computation

UNMODIFIED ARGUMENT
-`concatenatedÎ¸`: a vector of concatenated parameter values

RETURN
-log-likelihood
```
"""
function loglikelihood!(model::Model, memory::Memoryforgradient, concatenatedÎ¸::Vector{<:Real})
	if (concatenatedÎ¸ != memory.concatenatedÎ¸) || isnan(memory.â„“[1])
		P = update!(memory, model, concatenatedÎ¸)
		memory.â„“[1] = 0.0
		log_s = log(model.options.sf_y)
		@inbounds for trialset in model.trialsets
			N = length(trialset.mpGLMs)
			for trial in trialset.trials
				T = trial.ntimesteps
				memory.â„“[1] -= N*T*log_s
				forward!(memory, P, model.Î¸native, trial)
			end
		end
	end
	memory.â„“[1]
end

"""
    loglikelihood(concatenatedÎ¸, indexÎ¸, model)

ForwardDiff-compatible computation of the log-likelihood

ARGUMENT
-`concatenatedÎ¸`: a vector of concatenated parameter values
-`indexÎ¸`: struct indexing of each parameter in the vector of concatenated values
-`model`: an instance of FHM-DDM

RETURN
-log-likelihood
"""
function loglikelihood(concatenatedÎ¸::Vector{type}, indexÎ¸::IndexÎ¸, model::Model) where {type<:Real}
	model = Model(concatenatedÎ¸, indexÎ¸, model)
	@unpack options, Î¸native, Î¸real, trialsets = model
	@unpack Î”t, minpa, sf_y, K, Î = options
	pğ˜ğ‘‘=map(model.trialsets) do trialset
			map(trialset.trials) do trial
				map(1:trial.ntimesteps) do t
					ones(type,Î,K)
				end
			end
		end
	pğ‘‘_a=map(model.trialsets) do trialset
			map(trialset.trials) do trial
				ones(type,Î)
			end
		end
    scaledlikelihood!(pğ˜ğ‘‘, pğ‘‘_a, trialsets, Î¸native.Ïˆ[1])
	choiceLLscaling = scale_factor_choiceLL(model)
	Aáµƒinput = ones(type,Î,Î).*minpa
	one_minus_Îminpa = 1.0-Î*minpa
	Aáµƒinput[1,1] += one_minus_Îminpa
	Aáµƒinput[Î,Î] += one_minus_Îminpa
	Aáµƒsilent = copy(Aáµƒinput)
	expÎ»Î”t = exp(Î¸native.Î»[1]*Î”t)
	dÎ¼_dÎ”c = differentiate_Î¼_wrt_Î”c(Î”t, Î¸native.Î»[1])
	dğ›_dB = (2 .*collect(1:Î) .- Î .- 1)./(Î-2)
	ğ› = Î¸native.B[1].*dğ›_dB
	transitionmatrix!(Aáµƒsilent, minpa, expÎ»Î”t.*ğ›, âˆš(Î”t*Î¸native.ÏƒÂ²â‚[1]), ğ›)
	Aá¶œâ‚â‚ = Î¸native.Aá¶œâ‚â‚[1]
	Aá¶œâ‚‚â‚‚ = Î¸native.Aá¶œâ‚‚â‚‚[1]
	Ï€á¶œâ‚ = Î¸native.Ï€á¶œâ‚[1]
	if K == 2
		Aá¶œáµ€ = [Aá¶œâ‚â‚ 1-Aá¶œâ‚â‚; 1-Aá¶œâ‚‚â‚‚ Aá¶œâ‚‚â‚‚]
		Ï€á¶œáµ€ = [Ï€á¶œâ‚ 1-Ï€á¶œâ‚]
	else
		Aá¶œáµ€ = ones(type,1,1)
		Ï€á¶œáµ€ = ones(type,1,1)
	end
	log_s = log(sf_y)
	â„“ = zero(type)
	@inbounds for s in eachindex(trialsets)
		nneurons = length(trialsets[s].mpGLMs)
		for m in eachindex(trialsets[s].trials)
			trial = trialsets[s].trials[m]
			â„“-=nneurons*trial.ntimesteps*log_s
			pğšâ‚œ = probabilityvector(minpa, Î¸native.Î¼â‚€[1]+Î¸native.wâ‚•[1]*trial.previousanswer, âˆšÎ¸native.ÏƒÂ²áµ¢[1], ğ›)
			f = pğ˜ğ‘‘[s][m][1] .* pğšâ‚œ .* Ï€á¶œáµ€
			D = sum(f)
			D = max(D, nextfloat(0.0))
			f./=D
			â„“+=log(D)
			if length(trial.clicks.time) > 0
				adaptedclicks = adapt(trial.clicks, Î¸native.k[1], Î¸native.Ï•[1])
			end
			for t=2:trial.ntimesteps
				if t âˆˆ trial.clicks.inputtimesteps
					cL = sum(adaptedclicks.C[trial.clicks.left[t]])
					cR = sum(adaptedclicks.C[trial.clicks.right[t]])
					ğ› = expÎ»Î”t.*ğ› .+ (cR-cL).*dÎ¼_dÎ”c
					Ïƒ = âˆš((cR+cL)*Î¸native.ÏƒÂ²â‚›[1] + Î”t*Î¸native.ÏƒÂ²â‚[1])
					transitionmatrix!(Aáµƒinput, minpa, ğ›, Ïƒ, ğ›)
					Aáµƒ = Aáµƒinput
				else
					Aáµƒ = Aáµƒsilent
				end
				f = pğ˜ğ‘‘[s][m][t] .* (Aáµƒ * f * Aá¶œáµ€)
				D = sum(f)
				D = max(D, nextfloat(0.0))
				f./=D
				â„“+=log(D)
				if choiceLLscaling > 1
					pğšâ‚œ = Aáµƒ*pğšâ‚œ
				end
			end
			if choiceLLscaling > 1
				â„“ += (choiceLLscaling-1)*log(dot(pğ‘‘_a[s][m], pğšâ‚œ))
			end
		end
	end
	â„“
end

"""
	âˆ‡negativeloglikelihood!(âˆ‡nâ„“, memory, model, concatenatedÎ¸)

Update the gradient of the negative log-likelihood of the model

MODIFIED ARGUMENT
-`âˆ‡nâ„“`: the vector representing the gradient
-`memory`: memory allocated for computing the gradient. The log-likelihood is updated.
-`model`: structure containing the data, parameters, and hyperparameters of the model

ARGUMENT
-`concatenatedÎ¸`: values of the model's parameters concatenated into a vector
"""
function âˆ‡negativeloglikelihood!(âˆ‡nâ„“::Vector{<:Real},
 								 memory::Memoryforgradient,
								 model::Model,
								 concatenatedÎ¸::Vector{<:AbstractFloat})
	if concatenatedÎ¸ != memory.concatenatedÎ¸
		P = update!(memory, model, concatenatedÎ¸)
	else
		P = Probabilityvector(model.options.Î”t, model.options.minpa, model.Î¸native, model.options.Î)
	end
	âˆ‡loglikelihood!(memory,model,P)
	indexall = 0
	indexfit = 0
	for field in fieldnames(LatentÎ¸)
		indexall+=1
		if getfield(memory.indexÎ¸.latentÎ¸, field)[1] > 0
			indexfit +=1
			âˆ‡nâ„“[indexfit] = -memory.âˆ‡â„“latent[indexall]
		end
	end
	native2real!(âˆ‡nâ„“, memory.indexÎ¸.latentÎ¸, model)
	âˆ‡â„“glm = vcat((vcat((concatenateparameters(âˆ‡) for âˆ‡ in âˆ‡s)...) for âˆ‡s in memory.âˆ‡â„“glm)...)
	for i in eachindex(âˆ‡â„“glm)
		âˆ‡nâ„“[indexfit+i] = -âˆ‡â„“glm[i]
	end
	return nothing
end

"""
	âˆ‡loglikelihood!(memory, model, P)

Compute the gradient of the log-likelihood within the fields of an object of composite type `Memoryforgradient`

MODIFIED ARGUMENT
-`memory`: memory allocated for computing the gradient. The log-likelihood is updated.
-`model`: structure containing the data, parameters, and hyperparameters of the model
-`P`: a structure containing allocated memory for computing the accumulator's initial and transition probabilities as well as the partial derivatives of these probabilities
"""
function âˆ‡loglikelihood!(memory::Memoryforgradient, model::Model, P::Probabilityvector)
	memory.â„“ .= 0.0
	memory.âˆ‡â„“latent .= 0.0
	@inbounds for s in eachindex(model.trialsets)
		for m in eachindex(model.trialsets[s].trials)
			âˆ‡loglikelihood!(memory, model, P, s, m)
		end
	end
	@inbounds for s in eachindex(model.trialsets)
		for n = 1:length(model.trialsets[s].mpGLMs)
			expectation_âˆ‡loglikelihood!(memory.âˆ‡â„“glm[s][n], memory.glmderivatives, memory.Î³[s], model.trialsets[s].mpGLMs[n])
		end
	end
	return nothing
end

"""
	âˆ‡loglikelihood!(memory, model, P, s, m)

Update the gradient

MODIFIED ARGUMENT
-`memory`: memory allocated for computing the gradient. The log-likelihood is updated.
-`model`: structure containing the data, parameters, and hyperparameters of the model
-`P`: a structure containing allocated memory for computing the accumulator's initial and transition probabilities as well as the partial derivatives of these probabilities
-`s`: index of the trialset
-`m`: index of the trial
"""
function âˆ‡loglikelihood!(memory::Memoryforgradient,
						 model::Model,
						 P::Probabilityvector,
						 s::Integer,
						 m::Integer)
	trial = model.trialsets[s].trials[m]
	pğ˜ğ‘‘ = memory.pğ˜ğ‘‘[s][m]
	pğ‘‘_a = memory.pğ‘‘_a[s][m]
	@unpack Î¸native = model
	@unpack clicks = trial
	@unpack inputtimesteps, inputindex = clicks
	@unpack Aáµƒinput, âˆ‡Aáµƒinput, Aáµƒsilent, âˆ‡Aáµƒsilent, Aá¶œ, Aá¶œáµ€, âˆ‡Aá¶œ, choiceLLscaling, D, f, fá¶œ, indexÎ¸_paâ‚, indexÎ¸_paâ‚œaâ‚œâ‚‹â‚, indexÎ¸_pcâ‚, indexÎ¸_pcâ‚œcâ‚œâ‚‹â‚, indexÎ¸_Ïˆ, K, â„“, âˆ‡â„“latent, nÎ¸_paâ‚, nÎ¸_paâ‚œaâ‚œâ‚‹â‚, nÎ¸_pcâ‚, nÎ¸_pcâ‚œcâ‚œâ‚‹â‚, âˆ‡paâ‚, Ï€á¶œ, âˆ‡Ï€á¶œ, Î = memory
	if length(clicks.time) > 0
		adaptedclicks = âˆ‡adapt(trial.clicks, Î¸native.k[1], Î¸native.Ï•[1])
	end
	â„“[1] -= length(model.trialsets[s].mpGLMs)*trial.ntimesteps*log(model.options.sf_y)
	t = 1
	âˆ‡priorprobability!(âˆ‡paâ‚, P, trial.previousanswer)
	fá¶œ[1] = copy(P.ğ›‘)
	paâ‚ = fá¶œ[1]
	@inbounds for j=1:Î
		for k = 1:K
			f[t][j,k] = pğ˜ğ‘‘[t][j,k] * paâ‚[j] * Ï€á¶œ[k]
		end
	end
	D[t] = sum(f[t])
	D[t] = max(D[t], nextfloat(0.0))
	f[t] ./= D[t]
	â„“[1] += log(D[t])
	@inbounds for t=2:trial.ntimesteps
		if t âˆˆ clicks.inputtimesteps
			clickindex = clicks.inputindex[t][1]
			Aáµƒ = Aáµƒinput[clickindex]
			âˆ‡Aáµƒ = âˆ‡Aáµƒinput[clickindex]
			update_for_âˆ‡transition_probabilities!(P, adaptedclicks, clicks, t)
			âˆ‡transitionmatrix!(âˆ‡Aáµƒ, Aáµƒ, P)
		else
			Aáµƒ = Aáµƒsilent
		end
		f[t] = pğ˜ğ‘‘[t] .* (Aáµƒ * f[t-1] * Aá¶œáµ€)
		D[t] = sum(f[t])
		D[t] = max(D[t], nextfloat(0.0))
		f[t] ./= D[t]
		â„“[1] += log(D[t])
		if choiceLLscaling > 1
			fá¶œ[t] = Aáµƒ*fá¶œ[t-1]
		end
	end
	b = ones(Î,K)
	fâ¨€b = f # reuse memory
	âˆ‡â„“latent[indexÎ¸_Ïˆ[1]] += expectation_derivative_logpğ‘‘_wrt_Ïˆ(trial.choice, fâ¨€b[trial.ntimesteps], Î¸native.Ïˆ[1])
	if choiceLLscaling > 1
		fá¶œ[trial.ntimesteps] .*= pğ‘‘_a
		Dá¶œ = sum(fá¶œ[trial.ntimesteps])
		Dá¶œ = max(Dá¶œ, nextfloat(0.0))
		â„“[1] += (choiceLLscaling-1)*log(Dá¶œ)
		fá¶œ[trial.ntimesteps] ./= Dá¶œ
		bá¶œ = pğ‘‘_a./Dá¶œ # backward term for the last time step
		Î³ = bá¶œ.*fá¶œ[trial.ntimesteps] # posterior probability for the last time step
		âˆ‡â„“latent[indexÎ¸_Ïˆ[1]] += (choiceLLscaling-1)*expectation_derivative_logpğ‘‘_wrt_Ïˆ(trial.choice, fá¶œ[trial.ntimesteps], Î¸native.Ïˆ[1])
	end
	@inbounds for t = trial.ntimesteps:-1:1
		if t < trial.ntimesteps
			if t+1 âˆˆ clicks.inputtimesteps
				clickindex = clicks.inputindex[t+1][1]
				Aáµƒâ‚œâ‚Šâ‚ = Aáµƒinput[clickindex]
			else
				Aáµƒâ‚œâ‚Šâ‚ = Aáµƒsilent
			end
			b = transpose(Aáµƒâ‚œâ‚Šâ‚) * (b.*pğ˜ğ‘‘[t+1]./D[t+1]) * Aá¶œ
			fâ¨€b[t] .*= b
			if choiceLLscaling > 1
				bá¶œ = transpose(Aáµƒâ‚œâ‚Šâ‚) * bá¶œ
			end
		end
		if t > 1
			if t âˆˆ clicks.inputtimesteps
				clickindex = clicks.inputindex[t][1]
				Aáµƒ = Aáµƒinput[clickindex]
				âˆ‡Aáµƒ = âˆ‡Aáµƒinput[clickindex]
			else
				Aáµƒ = Aáµƒsilent
				âˆ‡Aáµƒ = âˆ‡Aáµƒsilent
			end
			for i = 1:nÎ¸_paâ‚œaâ‚œâ‚‹â‚
				âˆ‡â„“latent[indexÎ¸_paâ‚œaâ‚œâ‚‹â‚[i]] += sum_product_over_states(D[t], f[t-1], b, pğ˜ğ‘‘[t], âˆ‡Aáµƒ[i], Aá¶œ)
			end
			for i = 1:nÎ¸_pcâ‚œcâ‚œâ‚‹â‚
				âˆ‡â„“latent[indexÎ¸_pcâ‚œcâ‚œâ‚‹â‚[i]] += sum_product_over_states(D[t], f[t-1], b, pğ˜ğ‘‘[t], Aáµƒ, âˆ‡Aá¶œ[i])
			end
			if choiceLLscaling > 1
				for i = 1:nÎ¸_paâ‚œaâ‚œâ‚‹â‚
					âˆ‡â„“latent[indexÎ¸_paâ‚œaâ‚œâ‚‹â‚[i]] += (choiceLLscaling-1)*(transpose(bá¶œ)*âˆ‡Aáµƒ[i]*fá¶œ[t-1])[1]
				end
			end
		end
	end
	t = 1
	@inbounds for i = 1:nÎ¸_paâ‚
		âˆ‡â„“latent[indexÎ¸_paâ‚[i]] += sum_product_over_states(D[t], b, pğ˜ğ‘‘[t], âˆ‡paâ‚[i], Ï€á¶œ)
	end
	@inbounds for i = 1:nÎ¸_pcâ‚
		âˆ‡â„“latent[indexÎ¸_pcâ‚[i]] += sum_product_over_states(D[t], b, pğ˜ğ‘‘[t], paâ‚, âˆ‡Ï€á¶œ[i])
	end
	if choiceLLscaling > 1
		@inbounds for i = 1:nÎ¸_paâ‚
			âˆ‡â„“latent[indexÎ¸_paâ‚[i]] += (choiceLLscaling-1)*dot(bá¶œ, âˆ‡paâ‚[i])
		end
	end
	offset = 0
	for i = 1:m-1
		offset += model.trialsets[s].trials[i].ntimesteps
	end
	for t = 1:trial.ntimesteps
		Ï„ = offset+t
		for i = 1:Î
			for k = 1:K
				memory.Î³[s][i,k][Ï„] = fâ¨€b[t][i,k]
			end
		end
	end
	return nothing
end

"""
	Memoryforgradient(model)

Create variables that are memory by the computations of the log-likelihood and its gradient

ARGUMENT
-`model`: structure with information about the factorial hidden Markov drift-diffusion model

OUTPUT
-an instance of the custom type `Memoryforgradient`, which contains the memory quantities
```
"""
function Memoryforgradient(model::Model; choicemodel::Bool=false)
	@unpack options, Î¸native = model
	@unpack Î”t, K, minpa, Î = options
	Aá¶œâ‚â‚ = Î¸native.Aá¶œâ‚â‚[1]
	Aá¶œâ‚‚â‚‚ = Î¸native.Aá¶œâ‚‚â‚‚[1]
	Ï€á¶œâ‚ = Î¸native.Ï€á¶œâ‚[1]
	if K == 2
		Aá¶œ = [Aá¶œâ‚â‚ 1-Aá¶œâ‚‚â‚‚; 1-Aá¶œâ‚â‚ Aá¶œâ‚‚â‚‚]
		âˆ‡Aá¶œ = [[1.0 0.0; -1.0 0.0], [0.0 -1.0; 0.0 1.0]]
		Ï€á¶œ = [Ï€á¶œâ‚, 1-Ï€á¶œâ‚]
		âˆ‡Ï€á¶œ = [[1.0, -1.0]]
	else
		Aá¶œ = ones(1,1)
		âˆ‡Aá¶œ = [zeros(1,1), zeros(1,1)]
		Ï€á¶œ = ones(1)
		âˆ‡Ï€á¶œ = [zeros(1)]
	end
	maxclicks = maximum_number_of_clicks(model)
	maxtimesteps = maximum_number_of_time_steps(model)
	if choicemodel
		concatenatedÎ¸, indexÎ¸ = concatenate_choice_related_parameters(model)
		f = collect(zeros(Î,1) for t=1:maxtimesteps)
	else
		concatenatedÎ¸ = concatenateparameters(model)
		indexÎ¸ = indexparameters(model)
		f = collect(zeros(Î,K) for t=1:maxtimesteps)
	end
	indexÎ¸_paâ‚ = [3,6,11,13]
	indexÎ¸_paâ‚œaâ‚œâ‚‹â‚ = [3,4,5,7,10,12]
	indexÎ¸_pcâ‚ = [8]
	indexÎ¸_pcâ‚œcâ‚œâ‚‹â‚ = [1,2]
	indexÎ¸_Ïˆ = [9]
	nÎ¸_paâ‚ = length(indexÎ¸_paâ‚)
	nÎ¸_paâ‚œaâ‚œâ‚‹â‚ = length(indexÎ¸_paâ‚œaâ‚œâ‚‹â‚)
	âˆ‡â„“glm = map(model.trialsets) do trialset
				map(trialset.mpGLMs) do mpGLM
					GLMÎ¸(eltype(mpGLM.Î¸.ğ®), mpGLM.Î¸)
				end
			end
	one_minus_Îminpa = 1.0 - Î*minpa
	Aáµƒinput=map(1:maxclicks) do t
				A = ones(Î,Î).*minpa
				A[1,1] += one_minus_Îminpa
				A[Î,Î] += one_minus_Îminpa
				return A
			end
	Aáµƒsilent = copy(Aáµƒinput[1])
	âˆ‡Aáµƒinput = collect(collect(zeros(Î,Î) for q=1:nÎ¸_paâ‚œaâ‚œâ‚‹â‚) for t=1:maxclicks)
	âˆ‡Aáµƒsilent = map(i->zeros(Î,Î), 1:nÎ¸_paâ‚œaâ‚œâ‚‹â‚)
	pğ˜ğ‘‘=map(model.trialsets) do trialset
			map(trialset.trials) do trial
				map(1:trial.ntimesteps) do t
					ones(Î,K)
				end
			end
		end
	pğ‘‘_a=map(model.trialsets) do trialset
			map(trialset.trials) do trial
				ones(Î)
			end
		end
	Î³ =	map(model.trialsets) do trialset
			map(CartesianIndices((Î,K))) do index
				zeros(trialset.ntimesteps)
			end
		end
	memory = Memoryforgradient(Aáµƒinput=Aáµƒinput,
								âˆ‡Aáµƒinput=âˆ‡Aáµƒinput,
								Aáµƒsilent=Aáµƒsilent,
								âˆ‡Aáµƒsilent=âˆ‡Aáµƒsilent,
								Aá¶œ=Aá¶œ,
								âˆ‡Aá¶œ=âˆ‡Aá¶œ,
								choiceLLscaling = scale_factor_choiceLL(model),
								concatenatedÎ¸=similar(concatenatedÎ¸),
								Î”t=options.Î”t,
								f=f,
								glmderivatives = GLMDerivatives(model.trialsets[1].mpGLMs[1]),
								indexÎ¸=indexÎ¸,
								indexÎ¸_paâ‚=indexÎ¸_paâ‚,
								indexÎ¸_paâ‚œaâ‚œâ‚‹â‚=indexÎ¸_paâ‚œaâ‚œâ‚‹â‚,
								indexÎ¸_pcâ‚=indexÎ¸_pcâ‚,
								indexÎ¸_pcâ‚œcâ‚œâ‚‹â‚=indexÎ¸_pcâ‚œcâ‚œâ‚‹â‚,
								indexÎ¸_Ïˆ=indexÎ¸_Ïˆ,
								Î³=Î³,
								K=K,
								maxtimesteps=maxtimesteps,
								âˆ‡â„“glm=âˆ‡â„“glm,
								Ï€á¶œ=Ï€á¶œ,
								âˆ‡Ï€á¶œ=âˆ‡Ï€á¶œ,
								pğ‘‘_a=pğ‘‘_a,
								pğ˜ğ‘‘=pğ˜ğ‘‘,
								Î=Î)
	return memory
end

"""
	maximum_number_of_clicks(model)

Return the maximum number of clicks across all trials.

The stereoclick is excluded from this analysis as well as all other analyses.
"""
function maximum_number_of_clicks(model::Model)
	maxclicks = 0
	@inbounds for trialset in model.trialsets
		for trial in trialset.trials
			maxclicks = max(maxclicks, length(trial.clicks.time))
		end
	end
	return maxclicks
end

"""
	maximum_number_of_time_steps(model)

Return the maximum number of time steps across all trials
"""
function maximum_number_of_time_steps(model::Model)
	maxtimesteps = 0
	@inbounds for trialset in model.trialsets
		for trial in trialset.trials
			maxtimesteps = max(maxtimesteps, trial.ntimesteps)
		end
	end
	return maxtimesteps
end

"""
	update!(memory, model)

Update the memory quantities

MODIFIED ARGUMENT
-`memory`: structure containing variables memory between computations of the model's log-likelihood and its gradient

UNMODIFIED ARGUMENT
-`model`: structure with information concerning a factorial hidden Markov drift-diffusion model

RETURN
-`P`: a composite of the type `Probabilityvector` that contains quantifies used for computing the probability vectors of the accumulator variables and its first and second derivatives
"""

update!(memory::Memoryforgradient, model::Model) = update!(memory, model, concatenateparameters(model))

"""
	update!(model, memory, concatenatedÎ¸)

Update the model and the memory quantities according to new parameter values

MODIFIED ARGUMENT
-`memory`: structure containing variables memory between computations of the model's log-likelihood and its gradient
-`model`: structure with information concerning a factorial hidden Markov drift-diffusion model

ARGUMENT
-`concatenatedÎ¸`: newest values of the model's parameters

RETURN
-`P`: an instance of `Probabilityvector`
```
"""
function update!(memory::Memoryforgradient, model::Model, concatenatedÎ¸::Vector{<:Real})
	@unpack options, Î¸native, Î¸real = model
	@unpack Î”t, K, minpa,  Î = options
	memory.concatenatedÎ¸ .= concatenatedÎ¸
	sortparameters!(model, memory.concatenatedÎ¸, memory.indexÎ¸)
	real2native!(Î¸native, options, Î¸real)
	if !isempty(memory.pğ˜ğ‘‘[1][1][1])
	    scaledlikelihood!(memory.pğ˜ğ‘‘, memory.pğ‘‘_a, model.trialsets, Î¸native.Ïˆ[1])
	end
	P = update_for_âˆ‡latent_dynamics!(memory, options, Î¸native)
	return P
end

"""
	update_for_âˆ‡latent_dynamics!(memory, options, Î¸native)

Update quantities for computing the gradient of the prior and transition probabilities of the latent variables

MODIFIED ARGUMENT
-`memory`: structure containing variables memory between computations of the model's log-likelihood and its gradient

UNMODIFIED ARGUMENT
-`options`: settings of the model
-`Î¸native`: values of the parameters that control the latent variables, in the parameters' native space

RETURN
-`P`: an instance of `Probabilityvector`
"""
function update_for_âˆ‡latent_dynamics!(memory::Memoryforgradient, options::Options, Î¸native::LatentÎ¸)
	P = Probabilityvector(options.Î”t, options.minpa, Î¸native, options.Î)
	update_for_âˆ‡transition_probabilities!(P)
	âˆ‡transitionmatrix!(memory.âˆ‡Aáµƒsilent, memory.Aáµƒsilent, P)
	updatecoupling!(memory, Î¸native)
	return P
end

"""
	updatecoupling!(memory, Î¸native)

Update quantities for computing the prior and transition probability of the coupling variables

MODIFIED ARGUMENT
-`memory`: structure containing variables memory between computations of the model's log-likelihood and its gradient

UNMODIFIED ARGUMENT
-`Î¸native`: values of the parameters that control the latent variables, in the parameters' native space
"""
function updatecoupling!(memory::Memoryforgradient, Î¸native::LatentÎ¸)
	if memory.K == 2
		Aá¶œâ‚â‚ = Î¸native.Aá¶œâ‚â‚[1]
		Aá¶œâ‚‚â‚‚ = Î¸native.Aá¶œâ‚‚â‚‚[1]
		Ï€á¶œâ‚ = Î¸native.Ï€á¶œâ‚[1]
		memory.Aá¶œ .= [Aá¶œâ‚â‚ 1-Aá¶œâ‚‚â‚‚; 1-Aá¶œâ‚â‚ Aá¶œâ‚‚â‚‚]
		memory.Ï€á¶œ .= [Ï€á¶œâ‚, 1-Ï€á¶œâ‚]
	end
	return nothing
end

"""
	scale_factor_choiceLL(model)

Scaling factor for the log-likelihood of behavioral choices
"""
function scale_factor_choiceLL(model::Model)
	a = model.options.choiceLL_scaling_exponent
	if a==0
		1.0
	else
		ntimesteps_neurons = sum(collect(trialset.ntimesteps*length(trialset.mpGLMs) for trialset in model.trialsets))
		ntrials = sum(collect(trialset.ntrials for trialset in model.trialsets))
		(ntimesteps_neurons/ntrials)^a
	end
end
