"""
    maximizelikelihood!(model, optimizer)

Optimize the parameters of the factorial hidden Markov drift-diffusion model using a first-order optimizer in Optim

MODIFIED ARGUMENT
- a structure containing information for a factorial hidden Markov drift-diffusion model

UNMODIFIED ARGUMENT
-`optimizer`: an optimizer implemented by Optim.jl. The limited memory quasi-Newton algorithm `LBFGS()` does pretty well, and when using L-BFGS the `HagerZhang()` line search seems to do better than `BackTracking()`

OPTIONAL ARGUMENT
-`extended_trace`: save additional information
-`f_tol`: threshold for determining convergence in the objective value
-`g_tol`: threshold for determining convergence in the gradient
-`iterations`: number of inner iterations that will be run before the optimizer gives up
-`show_every`: trace output is printed every `show_every`th iteration.
-`show_trace`: should a trace of the optimizer's state be shown?
-`x_tol`: threshold for determining convergence in the input vector

RETURN
`losses`: value of the loss function (negative of the model's log-likelihood) across iterations. If `store_trace` were set to false, then these are NaN's
`gradientnorms`: 2-norm of the gradient of  of the loss function across iterations. If `store_trace` were set to false, then these are NaN's

EXAMPLE
```julia-repl
julia> using FHMDDM, Optim
julia> datapath = "/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_02_18_test/data.mat"
julia> model = Model(datapath)
julia> losses, gradientnorms = maximizelikelihood!(model, LBFGS(linesearch = LineSearches.BackTracking()))
```
"""
function maximizelikelihood!(model::Model,
							 optimizer::Optim.FirstOrderOptimizer;
			                 extended_trace::Bool=false,
			                 f_tol::AbstractFloat=0.0,
			                 g_tol::AbstractFloat=1e-8,
			                 iterations::Integer=1000,
			                 show_every::Integer=10,
			                 show_trace::Bool=true,
							 store_trace::Bool=true,
			                 x_tol::AbstractFloat=0.0)
	memory = Memoryforgradient(model)
	@unpack K, Î = model.options
	Î³ =	map(model.trialsets) do trialset
			map(CartesianIndices((Î,K))) do index
				zeros(trialset.ntimesteps)
			end
		end
    f(concatenatedÎ¸) = -loglikelihood!(model, memory, concatenatedÎ¸)
    g!(âˆ‡, concatenatedÎ¸) = âˆ‡negativeloglikelihood!(âˆ‡, Î³, model, memory, concatenatedÎ¸)
    Optim_options = Optim.Options(extended_trace=extended_trace,
								  f_tol=f_tol,
                                  g_tol=g_tol,
                                  iterations=iterations,
                                  show_every=show_every,
                                  show_trace=show_trace,
								  store_trace=store_trace,
                                  x_tol=x_tol)
	Î¸â‚€ = copy(memory.concatenatedÎ¸)
	optimizationresults = Optim.optimize(f, g!, Î¸â‚€, optimizer, Optim_options)
    maximumlikelihoodÎ¸ = Optim.minimizer(optimizationresults)
	sortparameters!(model, maximumlikelihoodÎ¸, memory.indexÎ¸)
	println(optimizationresults)
	losses, gradientnorms = fill(NaN, iterations+1), fill(NaN, iterations+1)
	if store_trace
		traces = Optim.trace(optimizationresults)
		for i in eachindex(traces)
			gradientnorms[i] = traces[i].g_norm
			losses[i] = traces[i].value
		end
	end
    return losses, gradientnorms
end

"""
    maximizelikelihood!(model, optimizer)

Optimize the parameters of the factorial hidden Markov drift-diffusion model using an algorithm implemented by Flux.jl

MODIFIED ARGUMENT
- a structure containing information for a factorial hidden Markov drift-diffusion model

UNMODIFIED ARGUMENT
-`optimizer`: optimization algorithm implemented by Flux.jl

OPTIONAL ARGUMENT
-`iterations`: number of inner iterations that will be run before the optimizer gives up

RETURN
`losses`: a vector of floating-point numbers indicating the value of the loss function (negative of the model's log-likelihood) across iterations. If `store_trace` were set to false, then these are NaN's
`gradientnorms`: a vector of floating-point numbers indicating the 2-norm of the gradient of  of the loss function across iterations. If `store_trace` were set to false, then these are NaN's

EXAMPLE
```julia-repl
julia> using FHMDDM
julia> import Flux
julia> datapath = "/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_02_18_test/data.mat"
julia> model = Model(datapath)
julia> losses, gradientnorms = maximizelikelihood!(model, Flux.ADAM())
```
"""
function maximizelikelihood!(model::Model,
							optimizer::Flux.Optimise.AbstractOptimiser;
							iterations::Integer = 3000)
	memory = Memoryforgradient(model)
	@unpack K, Î = model.options
	Î³ =	map(model.trialsets) do trialset
		map(CartesianIndices((Î,K))) do index
			zeros(trialset.ntimesteps)
		end
	end
	Î¸ = copy(memory.concatenatedÎ¸)
	âˆ‡ = similar(Î¸)
	local x, min_err, min_Î¸
	min_err = typemax(eltype(Î¸)) #dummy variables
	min_Î¸ = copy(Î¸)
	losses, gradientnorms = fill(NaN, iterations+1), fill(NaN, iterations+1)
	optimizationtime = 0.0
	for i = 1:iterations
		iterationtime = @timed begin
			x = -loglikelihood!(model, memory, Î¸)
			âˆ‡negativeloglikelihood!(âˆ‡, Î³, model, memory, Î¸)
			losses[i] = x
			gradientnorms[i] = norm(âˆ‡)
			if x < min_err  # found a better solution
				min_err = x
				min_Î¸ = copy(Î¸)
			end
			Flux.update!(optimizer, Î¸, âˆ‡)
		end
		optimizationtime += iterationtime[2]
		println("iteration=", i, ", loss= ", losses[i], ", gradient norm= ", gradientnorms[i], ", time(s)= ", optimizationtime)
	end
	sortparameters!(model, min_Î¸, memory.indexÎ¸)
    return losses, gradientnorms
end

"""
    loglikelihood!(model, memory, concatenatedÎ¸)

Compute the log-likelihood

ARGUMENT
-`model`: an instance of FHM-DDM
-`memory`: a container of variables used by both the log-likelihood and gradient computation

UNMODIFIED ARGUMENT
-`concatenatedÎ¸`: a vector of concatenated parameter values

RETURN
-log-likelihood

EXAMPLE
```julia-repl
julia> using FHMDDM
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_04_27_test/data.mat"; randomize=true);
julia> concatenatedÎ¸, indexÎ¸ = FHMDDM.concatenateparameters(model)
julia> memory, P = FHMDDM.Memoryforgradient(model)
julia> â„“ = loglikelihood!(model, memory, memory.concatenatedÎ¸)
julia> â„“ = loglikelihood!(model, memory, rand(length(memory.concatenatedÎ¸)))
```
"""
function loglikelihood!(model::Model,
						memory::Memoryforgradient,
					    concatenatedÎ¸::Vector{<:Real})
	if (concatenatedÎ¸ != memory.concatenatedÎ¸) || isnan(memory.â„“[1])
		P = update!(memory, model, concatenatedÎ¸)
		memory.â„“[1] = 0.0
		for s in eachindex(model.trialsets)
			for m in eachindex(model.trialsets[s].trials)
				memory.â„“[1] += loglikelihood(memory.pğ˜ğ‘‘[s][m], memory, P, model.Î¸native, model.trialsets[s].trials[m])
			end
		end
	end
	memory.â„“[1]
end

"""
	loglikelihood(pğ˜ğ‘‘, Î¸native, trial, trialinvariant)

Compute the log-likelihood of the data from one trial

ARGUMENT
-`pğ˜ğ‘‘`: a matrix whose element `pğ˜ğ‘‘[t][i,j]` represents the conditional likelihood `p(ğ˜â‚œ, d âˆ£ ğšâ‚œ=i, ğœâ‚œ=j)`
-`Î¸native`: model parameters in their native space
-`trial`: stimulus and behavioral information of one trial
-`trialinvariant`: a structure containing quantities that are used in each trial

RETURN
-`â„“`: log-likelihood of the data from one trial
"""
function loglikelihood(pğ˜ğ‘‘::Vector{<:Matrix{<:Real}},
   					   memory::Memoryforgradient,
					   P::Probabilityvector,
					   Î¸native::LatentÎ¸,
					   trial::Trial)
	@unpack clicks = trial
	@unpack Aáµƒinput, Aáµƒsilent, Aá¶œáµ€, Ï€á¶œáµ€ = memory
	adaptedclicks = adapt(clicks, Î¸native.k[1], Î¸native.Ï•[1])
	priorprobability!(P, trial.previousanswer)
	paâ‚ = P.ğ›‘
	f = pğ˜ğ‘‘[1] .* paâ‚ .* Ï€á¶œáµ€
	D = sum(f)
	f ./= D
	â„“ = log(D)
	@inbounds for t = 2:trial.ntimesteps
		if isempty(clicks.inputindex[t])
			Aáµƒ = Aáµƒsilent
		else
			Aáµƒ = Aáµƒinput[clicks.inputindex[t][1]]
			update_for_transition_probabilities!(P, adaptedclicks, clicks, t)
			transitionmatrix!(Aáµƒ, P)
		end
		f = pğ˜ğ‘‘[t].*(Aáµƒ * f * Aá¶œáµ€)
		D = sum(f)
		f ./= D
		â„“ += log(D)
	end
	return â„“
end

"""
    loglikelihood(concatenatedÎ¸, indexÎ¸, model)

Compute the log-likelihood in a way that is compatible with ForwardDiff

UNMODIFIED ARGUMENT
-`concatenatedÎ¸`: a vector of concatenated parameter values
-`indexÎ¸`: struct indexing of each parameter in the vector of concatenated values
-`model`: an instance of FHM-DDM

RETURN
-log-likelihood

EXAMPLE
```julia-repl
julia> using FHMDDM
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_04_25_test/data.mat"; randomize=true);
julia> concatenatedÎ¸, indexÎ¸ = FHMDDM.concatenateparameters(model)
julia> â„“ = loglikelihood(concatenatedÎ¸, indexÎ¸, model)
```
"""
function loglikelihood(	concatenatedÎ¸::Vector{<:Real},
					    indexÎ¸::IndexÎ¸,
						model::Model)
	model = Model(concatenatedÎ¸, indexÎ¸, model)
	@unpack options, Î¸native, Î¸real, trialsets = model
	@unpack Î, K = options
	trialinvariant = Trialinvariant(model; purpose="loglikelihood")
	T = eltype(concatenatedÎ¸)
	pğ˜ğ‘‘=map(model.trialsets) do trialset
			map(trialset.trials) do trial
				map(1:trial.ntimesteps) do t
					ones(T,Î,K)
				end
			end
		end
    likelihood!(pğ˜ğ‘‘, trialsets, Î¸native.Ïˆ[1]) # `pğ˜ğ‘‘` is the conditional likelihood p(ğ˜â‚œ, d âˆ£ aâ‚œ, zâ‚œ)
	â„“ = map(trialsets, pğ˜ğ‘‘) do trialset, pğ˜ğ‘‘
			map(trialset.trials, pğ˜ğ‘‘) do trial, pğ˜ğ‘‘
				loglikelihood(pğ˜ğ‘‘, Î¸native, trial, trialinvariant)
			end
		end
	return sum(sum(â„“))
end

"""
    âˆ‡negativeloglikelihood!(âˆ‡, Î³, model, memory, concatenatedÎ¸)

Gradient of the negative log-likelihood of the factorial hidden Markov drift-diffusion model

MODIFIED INPUT
-`âˆ‡`: a vector of partial derivatives
-`Î³`: posterior probability of the latent variables
-`model`: a structure containing information of the factorial hidden Markov drift-diffusion model
-`memory`: structure containing variables memory between computations of the model's log-likelihood and its gradient

UNMODIFIED ARGUMENT
-`concatenatedÎ¸`: parameter values concatenated into a vetor
"""
function âˆ‡negativeloglikelihood!(âˆ‡::Vector{<:AbstractFloat},
								 Î³::Vector{<:Matrix{<:Vector{<:AbstractFloat}}},
								 model::Model,
								 memory::Memoryforgradient,
								 concatenatedÎ¸::Vector{<:AbstractFloat})
	if concatenatedÎ¸ != memory.concatenatedÎ¸
		update!(model, memory, concatenatedÎ¸)
	end
	@unpack indexÎ¸, pğ˜ğ‘‘ = memory
	@unpack options, Î¸native, Î¸real, trialsets = model
	@unpack K = options
	trialinvariant = Trialinvariant(model; purpose="gradient")
	output=	map(trialsets, pğ˜ğ‘‘) do trialset, pğ˜ğ‘‘
				pmap(trialset.trials, pğ˜ğ‘‘) do trial, pğ˜ğ‘‘
					âˆ‡loglikelihood(pğ˜ğ‘‘, trialinvariant, Î¸native, trial)
				end
			end
	latentâˆ‡ = output[1][1][1] # reuse this memory
	for field in fieldnames(LatentÎ¸)
		latentâˆ‚ = getfield(latentâˆ‡, field)
		for i in eachindex(output)
			start = i==1 ? 2 : 1
			for m in start:length(output[i])
				latentâˆ‚[1] += getfield(output[i][m][1], field)[1] #output[i][m][1] are the partial derivatives
			end
		end
	end
	native2real!(latentâˆ‡, options, Î¸native, Î¸real)
	for field in fieldnames(LatentÎ¸)
		index = getfield(indexÎ¸.latentÎ¸,field)[1]
		if index != 0
			âˆ‡[index] = -getfield(latentâˆ‡,field)[1] # note the negative sign
		end
	end
	@inbounds for i in eachindex(output)
        t = 0
        for m in eachindex(output[i])
            for tâ‚˜ in eachindex(output[i][m][2]) # output[i][m][2] is `fb` of trial `m` in trialset `i`
                t += 1
                for jk in eachindex(output[i][m][2][tâ‚˜])
                    Î³[i][jk][t] = output[i][m][2][tâ‚˜][jk]
                end
            end
        end
    end
	âˆ‡glm = zeros(countparameters(trialsets[1].mpGLMs[1].Î¸))
	for i in eachindex(trialsets)
		for n in eachindex(trialsets[i].mpGLMs)
			âˆ‡negativeexpectation!(âˆ‡glm, Î³[i], trialsets[i].mpGLMs[n])
			sortparameters!(âˆ‡, indexÎ¸.glmÎ¸[i][n], âˆ‡glm)
		end
	end
	return nothing
end

"""
    âˆ‡negativeloglikelihood(Î³, model, memory, concatenatedÎ¸)

Gradient of the negative log-likelihood implemented to be compatible with ForwardDiff

MODIFIED INPUT
-`Î³`: posterior probability of the latent variables
-`model`: a structure containing information of the factorial hidden Markov drift-diffusion model
-`memory`: structure containing variables memory between computations of the model's log-likelihood and its gradient

UNMODIFIED ARGUMENT
-`concatenatedÎ¸`: parameter values concatenated into a vetor
"""
function âˆ‡negativeloglikelihood(concatenatedÎ¸::Vector{T},
								indexÎ¸::IndexÎ¸,
								model::Model) where {T<:Real}
	model = sortparameters(concatenatedÎ¸, indexÎ¸, model)
	@unpack options, Î¸native, Î¸real, trialsets = model
	@unpack Î, K = options
	trialinvariant = Trialinvariant(model; purpose="gradient")
	pğ˜ğ‘‘=map(model.trialsets) do trialset
			map(trialset.trials) do trial
				map(1:trial.ntimesteps) do t
					ones(T,Î,K)
				end
			end
		end
    likelihood!(pğ˜ğ‘‘, trialsets, Î¸native.Ïˆ[1]) # `pğ˜ğ‘‘` is the conditional likelihood p(ğ˜â‚œ, d âˆ£ aâ‚œ, zâ‚œ)
	output=	map(trialsets, pğ˜ğ‘‘) do trialset, pğ˜ğ‘‘
				pmap(trialset.trials, pğ˜ğ‘‘) do trial, pğ˜ğ‘‘
					âˆ‡loglikelihood(pğ˜ğ‘‘, trialinvariant, Î¸native, trial)
				end
			end
	latentâˆ‡ = output[1][1][1] # reuse this memory
	for field in fieldnames(LatentÎ¸)
		latentâˆ‚ = getfield(latentâˆ‡, field)
		for i in eachindex(output)
			start = i==1 ? 2 : 1
			for m in start:length(output[i])
				latentâˆ‚[1] += getfield(output[i][m][1], field)[1] #output[i][m][1] are the partial derivatives
			end
		end
	end
	native2real!(latentâˆ‡, options, Î¸native, Î¸real)
	âˆ‡ = similar(concatenatedÎ¸)
	for field in fieldnames(LatentÎ¸)
		index = getfield(indexÎ¸.latentÎ¸,field)[1]
		if index != 0
			âˆ‡[index] = -getfield(latentâˆ‡,field)[1] # note the negative sign
		end
	end
	@unpack K, Î = model.options
	Î³ =	map(model.trialsets) do trialset
			map(CartesianIndices((Î,K))) do index
				zeros(T, trialset.ntimesteps)
			end
		end
	@inbounds for i in eachindex(output)
        t = 0
        for m in eachindex(output[i])
            for tâ‚˜ in eachindex(output[i][m][2]) # output[i][m][2] is `fb` of trial `m` in trialset `i`
                t += 1
                for jk in eachindex(output[i][m][2][tâ‚˜])
                    Î³[i][jk][t] = output[i][m][2][tâ‚˜][jk]
                end
            end
        end
    end
	for i in eachindex(trialsets)
		for n in eachindex(trialsets[i].mpGLMs)
			âˆ‡glm = âˆ‡negativeexpectation(Î³[i], trialsets[i].mpGLMs[n])
			sortparameters!(âˆ‡, indexÎ¸.glmÎ¸[i][n], âˆ‡glm)
		end
	end
	return âˆ‡
end

"""
	âˆ‡loglikelihood(pğ˜ğ‘‘, trialinvariant, Î¸native, trial)

Compute quantities needed for the gradient of the log-likelihood of the data observed in one trial

ARGUMENT
-`pğ˜ğ‘‘`: A vector of matrices of floating-point numbers whose element `pğ˜ğ‘‘[t][i,j]` represents the likelihood of the emissions (spike trains and choice) at time step `t` conditioned on the accumulator variable being in state `i` and the coupling variable in state `j`
-`trialinvariant`: structure containing quantities used across trials
-`Î¸native`: parameters for the latent variables in their native space
-`trial`: information on the stimuli and behavioral choice of one trial

RETURN
-`latentâˆ‡`: gradient of the log-likelihood of the data observed in one trial with respect to the parameters specifying the latent variables
-`fb`: joint posterior probabilities of the accumulator and coupling variables
"""
function âˆ‡loglikelihood(pğ˜ğ‘‘::Vector{<:Matrix{T}},
						trialinvariant::Trialinvariant,
						Î¸native::LatentÎ¸,
						trial::Trial) where {T<:Real}
	@unpack clicks = trial
	@unpack inputtimesteps, inputindex = clicks
	@unpack Aáµƒsilent, dAáµƒsilentdÎ¼, dAáµƒsilentdÏƒÂ², dAáµƒsilentdB, Aá¶œ, Aá¶œáµ€, Î”t, K, ğ›š, Ï€á¶œáµ€, Î, ğ› = trialinvariant
	dâ„“dk, dâ„“dÎ», dâ„“dÏ•, dâ„“dÏƒÂ²â‚, dâ„“dÏƒÂ²â‚›, dâ„“dB = 0., 0., 0., 0., 0., 0.
	âˆ‘Ï‡á¶œ = zeros(T, K,K)
	Î¼ = Î¸native.Î¼â‚€[1] + trial.previousanswer*Î¸native.wâ‚•[1]
	Ïƒ = âˆšÎ¸native.ÏƒÂ²áµ¢[1]
	Ï€áµƒ, dÏ€áµƒdÎ¼, dÏ€áµƒdÏƒÂ², dÏ€áµƒdB = zeros(T, Î), zeros(T, Î), zeros(T, Î), zeros(T, Î)
	probabilityvector!(Ï€áµƒ, dÏ€áµƒdÎ¼, dÏ€áµƒdÏƒÂ², dÏ€áµƒdB, Î¼, ğ›š, Ïƒ, ğ›)
	n_steps_with_input = length(clicks.inputtimesteps)
	Aáµƒ = map(x->zeros(T, Î,Î), clicks.inputtimesteps)
	dAáµƒdÎ¼ = map(x->zeros(T, Î,Î), clicks.inputtimesteps)
	dAáµƒdÏƒÂ² = map(x->zeros(T, Î,Î), clicks.inputtimesteps)
	dAáµƒdB = map(x->zeros(T, Î,Î), clicks.inputtimesteps)
	Î”c = zeros(T, n_steps_with_input)
	âˆ‘c = zeros(T, n_steps_with_input)
	adaptedclicks = âˆ‡adapt(clicks, Î¸native.k[1], Î¸native.Ï•[1])
	@unpack C, dC_dk, dC_dÏ• = adaptedclicks
	for i in 1:n_steps_with_input
		t = clicks.inputtimesteps[i]
		cL = sum(C[clicks.left[t]])
		cR = sum(C[clicks.right[t]])
		transitionmatrix!(Aáµƒ[i], dAáµƒdÎ¼[i], dAáµƒdÏƒÂ²[i], dAáµƒdB[i], cL, cR, trialinvariant, Î¸native)
		Î”c[i] = cR-cL
		âˆ‘c[i] = cL+cR
	end
	D, f = forward(Aáµƒ, inputindex, Ï€áµƒ, pğ˜ğ‘‘, trialinvariant)
	fb = f # reuse memory
	b = ones(T, Î,K)
	Aá¶œreshaped = reshape(Aá¶œ, 1, 1, K, K)
	dÎ¼_dÎ”c = differentiate_Î¼_wrt_Î”c(Î”t, Î¸native.Î»[1])
	dÂ²Î¼_dÎ”cdÎ» = differentiate_Î¼_wrt_Î”c(Î”t, Î¸native.Î»[1])
	ğ›áµ€Î”texpÎ»Î”t = transpose(ğ›).*Î”t.*exp(Î¸native.Î»[1]*Î”t)
	@inbounds for t = trial.ntimesteps:-1:1
		if t < trial.ntimesteps # backward step
			Aáµƒâ‚œâ‚Šâ‚ = isempty(inputindex[t+1]) ? Aáµƒsilent : Aáµƒ[inputindex[t+1][1]]
			b = transpose(Aáµƒâ‚œâ‚Šâ‚) * (b .* pğ˜ğ‘‘[t+1] ./ D[t+1]) * Aá¶œ
			fb[t] .*= b
		end
		if t > 1 # joint posterior over consecutive time bins, computations involving the transition matrix
			if isempty(inputindex[t])
				Aáµƒâ‚œ = Aáµƒsilent
				dAáµƒâ‚œdÎ¼ = dAáµƒsilentdÎ¼
				dAáµƒâ‚œdÏƒÂ² = dAáµƒsilentdÏƒÂ²
				dAáµƒâ‚œdB = dAáµƒsilentdB
			else
				i = inputindex[t][1]
				Aáµƒâ‚œ = Aáµƒ[i]
				dAáµƒâ‚œdÎ¼ = dAáµƒdÎ¼[i]
				dAáµƒâ‚œdÏƒÂ² = dAáµƒdÏƒÂ²[i]
				dAáµƒâ‚œdB = dAáµƒdB[i]
			end
			Ï‡_oslash_Aáµƒ = reshape(pğ˜ğ‘‘[t].*b, Î, 1, K, 1) .* reshape(f[t-1], 1, Î, 1, K) .* Aá¶œreshaped ./ D[t]
	        if K == 2
		        âˆ‘Ï‡á¶œ += dropdims(sum(Ï‡_oslash_Aáµƒ.*Aáµƒâ‚œ, dims=(1,2)); dims=(1,2))
			end
			Ï‡áµƒ_Aáµƒ = dropdims(sum(Ï‡_oslash_Aáµƒ, dims=(3,4)); dims=(3,4))
			Ï‡áµƒ_dlogAáµƒdÎ¼ = Ï‡áµƒ_Aáµƒ .* dAáµƒâ‚œdÎ¼ # Ï‡áµƒâŠ™ d/dÎ¼{log(Aáµƒ)} = Ï‡áµƒâŠ˜ AáµƒâŠ™ d/dÎ¼{Aáµƒ}
			âˆ‘_Ï‡áµƒ_dlogAáµƒdÎ¼ = sum(Ï‡áµƒ_dlogAáµƒdÎ¼)
			âˆ‘_Ï‡áµƒ_dlogAáµƒdÏƒÂ² = sum(Ï‡áµƒ_Aáµƒ .* dAáµƒâ‚œdÏƒÂ²) # similarly, Ï‡áµƒâŠ™ d/dÏƒÂ²{log(Aáµƒ)} = Ï‡áµƒâŠ˜ AáµƒâŠ™ d/dÏƒÂ²{Aáµƒ}
			dâ„“dÏƒÂ²â‚ += âˆ‘_Ï‡áµƒ_dlogAáµƒdÏƒÂ² # the Î”t is multiplied after summing across time steps
			dâ„“dB += sum(Ï‡áµƒ_Aáµƒ .* dAáµƒâ‚œdB)
			if isempty(inputindex[t])
				dÎ¼dÎ» = ğ›áµ€Î”texpÎ»Î”t
			else
				dÎ¼dÎ» = ğ›áµ€Î”texpÎ»Î”t .+ Î”c[i].*dÂ²Î¼_dÎ”cdÎ»
				dâ„“dÏƒÂ²â‚› += âˆ‘_Ï‡áµƒ_dlogAáµƒdÏƒÂ²*âˆ‘c[i]
				dcLdÏ• = sum(dC_dÏ•[clicks.left[t]])
				dcRdÏ• = sum(dC_dÏ•[clicks.right[t]])
				dcLdk = sum(dC_dk[clicks.left[t]])
				dcRdk = sum(dC_dk[clicks.right[t]])
				dÏƒÂ²dÏ• = Î¸native.ÏƒÂ²â‚›[1]*(dcLdÏ• + dcRdÏ•)
				dÏƒÂ²dk = Î¸native.ÏƒÂ²â‚›[1]*(dcLdk + dcRdk)
				dâ„“dÏ• += âˆ‘_Ï‡áµƒ_dlogAáµƒdÎ¼*dÎ¼_dÎ”c*(dcRdÏ• - dcLdÏ•) + âˆ‘_Ï‡áµƒ_dlogAáµƒdÏƒÂ²*dÏƒÂ²dÏ•
				dâ„“dk += âˆ‘_Ï‡áµƒ_dlogAáµƒdÎ¼*dÎ¼_dÎ”c*(dcRdk - dcLdk) + âˆ‘_Ï‡áµƒ_dlogAáµƒdÏƒÂ²*dÏƒÂ²dk
			end
			dâ„“dÎ» += sum(Ï‡áµƒ_dlogAáµƒdÎ¼.*dÎ¼dÎ»)
		end
	end
	dâ„“dÏƒÂ²â‚ *= Î”t
	if K == 2
		dâ„“dAá¶œâ‚â‚ = âˆ‘Ï‡á¶œ[1,1]/Aá¶œ[1,1] - âˆ‘Ï‡á¶œ[2,1]/Aá¶œ[2,1]
		dâ„“dAá¶œâ‚‚â‚‚ = âˆ‘Ï‡á¶œ[2,2]/Aá¶œ[2,2] - âˆ‘Ï‡á¶œ[1,2]/Aá¶œ[1,2]
		âˆ‘Î³á¶œâ‚ = sum(fb[1], dims=1)
		dâ„“dxÏ€á¶œâ‚ = (âˆ‘Î³á¶œâ‚[1] - Î¸native.Ï€á¶œâ‚[1])/Î¸native.Ï€á¶œâ‚[1]/(1.0 - Î¸native.Ï€á¶œâ‚[1])
	else
		dâ„“dAá¶œâ‚â‚ = 0.0
		dâ„“dAá¶œâ‚‚â‚‚ = 0.0
		dâ„“dxÏ€á¶œâ‚ = 0.0
	end
	Î³áµƒâ‚_oslash_Ï€áµƒ = sum(pğ˜ğ‘‘[1] .* Ï€á¶œáµ€ ./ D[1] .* b, dims=2)
	âˆ‘_Î³áµƒâ‚_dlogÏ€áµƒdÎ¼ = Î³áµƒâ‚_oslash_Ï€áµƒ â‹… dÏ€áµƒdÎ¼ # similar to above, Î³áµƒâ‚âŠ™ d/dÎ¼{log(Ï€áµƒ)} = Î³áµƒâ‚âŠ˜ Ï€áµƒâŠ™ d/dÎ¼{Ï€áµƒ}
	dâ„“dÎ¼â‚€ = âˆ‘_Î³áµƒâ‚_dlogÏ€áµƒdÎ¼
	dâ„“dwâ‚• = âˆ‘_Î³áµƒâ‚_dlogÏ€áµƒdÎ¼ * trial.previousanswer
	dâ„“dÏƒÂ²áµ¢ = Î³áµƒâ‚_oslash_Ï€áµƒ â‹… dÏ€áµƒdÏƒÂ²
	dâ„“dB += Î³áµƒâ‚_oslash_Ï€áµƒ â‹… dÏ€áµƒdB
	dâ„“dÏˆ = differentiateâ„“_wrt_Ïˆ(trial.choice, f[end], Î¸native.Ïˆ[1])
	latentâˆ‡ = LatentÎ¸(	Aá¶œâ‚â‚ = [dâ„“dAá¶œâ‚â‚],
						Aá¶œâ‚‚â‚‚ = [dâ„“dAá¶œâ‚‚â‚‚],
						k	 = [dâ„“dk],
						Î»	 = [dâ„“dÎ»],
						Î¼â‚€	 = [dâ„“dÎ¼â‚€],
						Ï•	 = [dâ„“dÏ•],
						Ï€á¶œâ‚	 = [dâ„“dxÏ€á¶œâ‚],
						Ïˆ	 = [dâ„“dÏˆ],
						ÏƒÂ²â‚	 = [dâ„“dÏƒÂ²â‚],
						ÏƒÂ²áµ¢	 = [dâ„“dÏƒÂ²áµ¢],
						ÏƒÂ²â‚›	 = [dâ„“dÏƒÂ²â‚›],
						wâ‚•	 = [dâ„“dwâ‚•],
						B	 = [dâ„“dB])
	return latentâˆ‡, fb
end

"""
	differentiateâ„“_wrt_Ïˆ(choice, Î³_end, Ïˆ)

Partial derivative of the log-likelihood of the data from one trial with respect to the lapse rate Ïˆ

ARGUMENT
-`choice`: a Boolean specifying whether the choice was to the right
-`Î³_end`: a matrix of floating-point numbers representing the posterior likelihood of the latent variables at the end of the trial (i.e., last time step). Element `Î³_end[i,j]` = p(aáµ¢=1, câ±¼=1 âˆ£ ğ˜, d). Rows correspond to states of the accumulator state variable ğš, and columns to states of the coupling variable ğœ.
-`Ïˆ`: a floating-point number specifying the lapse rate

RETURN
-a floating-point number quantifying the partial derivative of the log-likelihood of one trial's data with respect to the lapse rate Ïˆ
"""
function differentiateâ„“_wrt_Ïˆ(choice::Bool, Î³_end::Array{<:Real}, Ïˆ::Real)
	Î³áµƒ_end = sum(Î³_end, dims=2)
	zeroindex = cld(length(Î³áµƒ_end), 2)
	if choice
		choiceconsistent   = sum(Î³áµƒ_end[zeroindex+1:end])
		choiceinconsistent = sum(Î³áµƒ_end[1:zeroindex-1])
	else
		choiceconsistent   = sum(Î³áµƒ_end[1:zeroindex-1])
		choiceinconsistent = sum(Î³áµƒ_end[zeroindex+1:end])
	end
	return choiceconsistent/(Ïˆ-2) + choiceinconsistent/Ïˆ
end

"""
	Trialinvariant(options, Î¸native)

Compute quantities that are used in each trial for computing gradient of the log-likelihood

ARGUMENT
-`model`: custom type containing the settings, data, and parameters of a factorial hidden Markov drift-diffusion model
"""
function Trialinvariant(model::Model; purpose="gradient")
	@unpack options, Î¸native, Î¸real = model
	@unpack Î”t, K, Î = options
	Î» = Î¸native.Î»[1]
	B = Î¸native.B[1]
	Aá¶œâ‚â‚ = Î¸native.Aá¶œâ‚â‚[1]
	Aá¶œâ‚‚â‚‚ = Î¸native.Aá¶œâ‚‚â‚‚[1]
	Ï€á¶œâ‚ = Î¸native.Ï€á¶œâ‚[1]
	if K == 2
		Aá¶œ = [Aá¶œâ‚â‚ 1-Aá¶œâ‚‚â‚‚; 1-Aá¶œâ‚â‚ Aá¶œâ‚‚â‚‚]
		Aá¶œáµ€ = [Aá¶œâ‚â‚ 1-Aá¶œâ‚â‚; 1-Aá¶œâ‚‚â‚‚ Aá¶œâ‚‚â‚‚]
		Ï€á¶œáµ€ = [Ï€á¶œâ‚ 1-Ï€á¶œâ‚]
	else
		Aá¶œ = ones(1,1)
		Aá¶œáµ€ = ones(1,1)
		Ï€á¶œáµ€ = ones(1,1)
	end
	ğ› = B*(2collect(1:Î) .- Î .- 1)/(Î-2)
	ğ› = conditionedmean(0.0, Î”t, Î¸native.Î»[1], ğ›)
	Ïƒ = âˆš(Î¸native.ÏƒÂ²â‚[1]*Î”t)
	T = eltype(ğ›)
	Aáµƒsilent = zeros(T,Î,Î)
	if purpose=="gradient"
		ğ›š = (2collect(1:Î) .- Î .- 1)/2
		Î© = ğ›š .- transpose(ğ›š).*exp.(Î».*Î”t)
		dAáµƒsilentdÎ¼ = zeros(T,Î,Î)
		dAáµƒsilentdÏƒÂ² = zeros(T,Î,Î)
		dAáµƒsilentdB = zeros(T,Î,Î)
		transitionmatrix!(Aáµƒsilent, dAáµƒsilentdÎ¼, dAáµƒsilentdÏƒÂ², dAáµƒsilentdB, ğ›, Ïƒ, Î©, ğ›)
		Trialinvariant( Aáµƒsilent=Aáµƒsilent,
					Aá¶œ=Aá¶œ,
					Aá¶œáµ€=Aá¶œáµ€,
					dAáµƒsilentdÎ¼=dAáµƒsilentdÎ¼,
					dAáµƒsilentdÏƒÂ²=dAáµƒsilentdÏƒÂ²,
					dAáµƒsilentdB=dAáµƒsilentdB,
					Î”t=options.Î”t,
					ğ›š=ğ›š,
					Î©=Î©,
					Ï€á¶œáµ€=Ï€á¶œáµ€,
					Î=Î,
 				    K=K,
					ğ›=ğ›)
	elseif purpose=="loglikelihood"
		transitionmatrix!(Aáµƒsilent, ğ›, Ïƒ, ğ›)
		Trialinvariant(Aáµƒsilent=Aáµƒsilent,
				   Aá¶œáµ€=Aá¶œáµ€,
				   Î”t=options.Î”t,
				   Ï€á¶œáµ€=Ï€á¶œáµ€,
				   ğ›=ğ›,
				   K=K,
				   Î=Î)
	end
end

"""
	Memoryforgradient(model)

Create variables that are memory by the computations of the log-likelihood and its gradient

ARGUMENT
-`model`: structure with information about the factorial hidden Markov drift-diffusion model

OUTPUT
-an instance of the custom type `Memoryforgradient`, which contains the memory quantities
-`P`: an instance of `Probabilityvector`

EXAMPLE
```julia-repl
julia> using FHMDDM
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_04_27_test/data.mat"; randomize=true);
julia> memory, P = FHMDDM.Memoryforgradient(model)
```
"""
function Memoryforgradient(model::Model)
	@unpack options, Î¸native = model
	@unpack Î”t, K, Î = options
	Aá¶œâ‚â‚ = Î¸native.Aá¶œâ‚â‚[1]
	Aá¶œâ‚‚â‚‚ = Î¸native.Aá¶œâ‚‚â‚‚[1]
	Ï€á¶œâ‚ = Î¸native.Ï€á¶œâ‚[1]
	if K == 2
		Aá¶œ = [Aá¶œâ‚â‚ 1-Aá¶œâ‚‚â‚‚; 1-Aá¶œâ‚â‚ Aá¶œâ‚‚â‚‚]
		âˆ‡Aá¶œ = [[1.0 0.0; -1.0 0.0], [0.0 -1.0; 0.0 1.0]]
		Ï€á¶œ = [Ï€á¶œâ‚, 1-Ï€á¶œâ‚]
		âˆ‡Ï€á¶œ = [[1.0, -1.0]]
	else
		Aá¶œ = ones(1,1)
		âˆ‡Aá¶œ = [zeros(1,1), zeros(1,1)]
		Ï€á¶œ = ones(1)
		âˆ‡Ï€á¶œ = [zeros(1)]
	end
	indexÎ¸_paâ‚ = [3,6,11,13]
	indexÎ¸_paâ‚œaâ‚œâ‚‹â‚ = [3,4,5,7,10,12]
	indexÎ¸_pcâ‚ = [8]
	indexÎ¸_pcâ‚œcâ‚œâ‚‹â‚ = [1,2]
	indexÎ¸_Ïˆ = [9]
	nÎ¸_paâ‚ = length(indexÎ¸_paâ‚)
	nÎ¸_paâ‚œaâ‚œâ‚‹â‚ = length(indexÎ¸_paâ‚œaâ‚œâ‚‹â‚)
	maxclicks = 0
	maxtimesteps = 0
	@inbounds for trialset in model.trialsets
		for trial in trialset.trials
			maxclicks = max(maxclicks, length(trial.clicks.time))
			maxtimesteps = max(maxtimesteps, trial.ntimesteps)
		end
	end
	Aáµƒinput=map(1:maxclicks) do t
				A = zeros(Î,Î)
				A[1,1] = A[Î,Î] = 1.0
				return A
			end
	âˆ‡Aáµƒinput = collect(collect(zeros(Î,Î) for q=1:nÎ¸_paâ‚œaâ‚œâ‚‹â‚) for t=1:maxclicks)
	Aáµƒsilent = zeros(Î, Î)
	âˆ‡Aáµƒsilent = map(i->zeros(Î,Î), 1:nÎ¸_paâ‚œaâ‚œâ‚‹â‚)
	Aáµƒsilent[1,1] = Aáµƒsilent[Î, Î] = 1.0
	P = Probabilityvector(Î”t, Î¸native, Î)
	update_for_âˆ‡transition_probabilities!(P)
	âˆ‡transitionmatrix!(âˆ‡Aáµƒsilent, Aáµƒsilent, P)
	pğ˜ğ‘‘ = likelihood(model)
	concatenatedÎ¸, indexÎ¸ = concatenateparameters(model)
	memory = Memoryforgradient(Aáµƒinput=Aáµƒinput,
								âˆ‡Aáµƒinput=âˆ‡Aáµƒinput,
								Aáµƒsilent=Aáµƒsilent,
								âˆ‡Aáµƒsilent=âˆ‡Aáµƒsilent,
								Aá¶œ=Aá¶œ,
								âˆ‡Aá¶œ=âˆ‡Aá¶œ,
								concatenatedÎ¸=concatenatedÎ¸,
								D = zeros(maxtimesteps),
								Î”t=options.Î”t,
								f=collect(zeros(Î,K) for t=1:maxtimesteps),
								indexÎ¸=indexÎ¸,
								indexÎ¸_paâ‚=indexÎ¸_paâ‚,
								indexÎ¸_paâ‚œaâ‚œâ‚‹â‚=indexÎ¸_paâ‚œaâ‚œâ‚‹â‚,
								indexÎ¸_pcâ‚=indexÎ¸_pcâ‚,
								indexÎ¸_pcâ‚œcâ‚œâ‚‹â‚=indexÎ¸_pcâ‚œcâ‚œâ‚‹â‚,
								indexÎ¸_Ïˆ=indexÎ¸_Ïˆ,
								K=K,
								âˆ‡paâ‚ = collect(zeros(Î) for q=1:nÎ¸_paâ‚),
								Ï€á¶œ=Ï€á¶œ,
								âˆ‡Ï€á¶œ=âˆ‡Ï€á¶œ,
								pğ˜ğ‘‘=pğ˜ğ‘‘,
								Î=Î)
	return memory, P
end

"""
	update!(model, memory, concatenatedÎ¸)

Update the model and the memory quantities according to new parameter values

MODIFIED ARGUMENT
-`memory`: structure containing variables memory between computations of the model's log-likelihood and its gradient
-`model`: structure with information concerning a factorial hidden Markov drift-diffusion model

ARGUMENT
-`concatenatedÎ¸`: newest values of the model's parameters

RETURN
-`P`: an instance of `Probabilityvector`

EXAMPLE
```julia-repl
julia> using FHMDDM
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_04_27_test/data.mat"; randomize=true);
julia> memory, P = FHMDDM.Memoryforgradient(model)
julia> P = update!(model, memory, rand(length(memory.concatenatedÎ¸)))
```
"""
function update!(memory::Memoryforgradient,
				 model::Model,
				 concatenatedÎ¸::Vector{<:Real})
	memory.concatenatedÎ¸ .= concatenatedÎ¸
	sortparameters!(model, memory.concatenatedÎ¸, memory.indexÎ¸)
	if !isempty(memory.pğ˜ğ‘‘[1][1][1])
	    likelihood!(memory.pğ˜ğ‘‘, model.trialsets, model.Î¸native.Ïˆ[1])
	end
	@unpack options, Î¸native = model
	@unpack Î”t, K, Î = options
	P = Probabilityvector(Î”t, Î¸native, Î)
	update_for_âˆ‡transition_probabilities!(P)
	âˆ‡transitionmatrix!(memory.âˆ‡Aáµƒsilent, memory.Aáµƒsilent, P)
	if K == 2
		Aá¶œâ‚â‚ = Î¸native.Aá¶œâ‚â‚[1]
		Aá¶œâ‚‚â‚‚ = Î¸native.Aá¶œâ‚‚â‚‚[1]
		Ï€á¶œâ‚ = Î¸native.Ï€á¶œâ‚[1]
		memory.Aá¶œ .= [Aá¶œâ‚â‚ 1-Aá¶œâ‚‚â‚‚; 1-Aá¶œâ‚â‚ Aá¶œâ‚‚â‚‚]
		memory.Ï€á¶œ .= [Ï€á¶œâ‚, 1-Ï€á¶œâ‚]
	end
	return P
end
