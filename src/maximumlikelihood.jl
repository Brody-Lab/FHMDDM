"""
    maximizelikelihood!(model, optimizer)

Optimize the parameters of the factorial hidden Markov drift-diffusion model using a first-order optimizer in Optim

MODIFIED ARGUMENT
- a structure containing information for a factorial hidden Markov drift-diffusion model

UNMODIFIED ARGUMENT
-`optimizer`: an optimizer implemented by Optim.jl. The limited memory quasi-Newton algorithm `LBFGS()` does pretty well, and when using L-BFGS the `HagerZhang()` line search seems to do better than `BackTracking()`

OPTIONAL ARGUMENT
-`extended_trace`: save additional information
-`f_tol`: threshold for determining convergence in the objective value
-`g_tol`: threshold for determining convergence in the gradient
-`iterations`: number of inner iterations that will be run before the optimizer gives up
-`show_every`: trace output is printed every `show_every`th iteration.
-`show_trace`: should a trace of the optimizer's state be shown?
-`x_tol`: threshold for determining convergence in the input vector

RETURN
`losses`: value of the loss function (negative of the model's log-likelihood) across iterations. If `store_trace` were set to false, then these are NaN's
`gradientnorms`: 2-norm of the gradient of  of the loss function across iterations. If `store_trace` were set to false, then these are NaN's

EXAMPLE
```julia-repl
julia> using FHMDDM, Optim
julia> datapath = "/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_02_18_test/data.mat"
julia> model = Model(datapath)
julia> losses, gradientnorms = maximizelikelihood!(model, LBFGS(linesearch = LineSearches.BackTracking()))
```
"""
function maximizelikelihood!(model::Model,
							 optimizer::Optim.FirstOrderOptimizer;
			                 extended_trace::Bool=false,
			                 f_tol::AbstractFloat=0.0,
			                 g_tol::AbstractFloat=1e-8,
			                 iterations::Integer=1000,
			                 show_every::Integer=10,
			                 show_trace::Bool=true,
							 store_trace::Bool=true,
			                 x_tol::AbstractFloat=0.0)
	shared = Shared(model)
	@unpack K, Î = model.options
	Î³ =	map(model.trialsets) do trialset
			map(CartesianIndices((Î,K))) do index
				zeros(trialset.ntimesteps)
			end
		end
    f(concatenatedÎ¸) = -loglikelihood!(model, shared, concatenatedÎ¸)
    g!(âˆ‡, concatenatedÎ¸) = âˆ‡negativeloglikelihood!(âˆ‡, Î³, model, shared, concatenatedÎ¸)
    Optim_options = Optim.Options(extended_trace=extended_trace,
								  f_tol=f_tol,
                                  g_tol=g_tol,
                                  iterations=iterations,
                                  show_every=show_every,
                                  show_trace=show_trace,
								  store_trace=store_trace,
                                  x_tol=x_tol)
	Î¸â‚€ = copy(shared.concatenatedÎ¸)
	optimizationresults = Optim.optimize(f, g!, Î¸â‚€, optimizer, Optim_options)
    maximumlikelihoodÎ¸ = Optim.minimizer(optimizationresults)
	sortparameters!(model, maximumlikelihoodÎ¸, shared.indexÎ¸)
	println(optimizationresults)
	losses, gradientnorms = fill(NaN, iterations+1), fill(NaN, iterations+1)
	if store_trace
		traces = Optim.trace(optimizationresults)
		for i in eachindex(traces)
			gradientnorms[i] = traces[i].g_norm
			losses[i] = traces[i].value
		end
	end
    return losses, gradientnorms
end

"""
    maximizelikelihood!(model, optimizer)

Optimize the parameters of the factorial hidden Markov drift-diffusion model using an algorithm implemented by Flux.jl

MODIFIED ARGUMENT
- a structure containing information for a factorial hidden Markov drift-diffusion model

UNMODIFIED ARGUMENT
-`optimizer`: optimization algorithm implemented by Flux.jl

OPTIONAL ARGUMENT
-`iterations`: number of inner iterations that will be run before the optimizer gives up

RETURN
`losses`: a vector of floating-point numbers indicating the value of the loss function (negative of the model's log-likelihood) across iterations. If `store_trace` were set to false, then these are NaN's
`gradientnorms`: a vector of floating-point numbers indicating the 2-norm of the gradient of  of the loss function across iterations. If `store_trace` were set to false, then these are NaN's

EXAMPLE
```julia-repl
julia> using FHMDDM
julia> import Flux
julia> datapath = "/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_02_18_test/data.mat"
julia> model = Model(datapath)
julia> losses, gradientnorms = maximizelikelihood!(model, Flux.ADAM())
```
"""
function maximizelikelihood!(model::Model,
							optimizer::Flux.Optimise.AbstractOptimiser;
							iterations::Integer = 3000)
	shared = Shared(model)
	@unpack K, Î = model.options
	Î³ =	map(model.trialsets) do trialset
		map(CartesianIndices((Î,K))) do index
			zeros(trialset.ntimesteps)
		end
	end
	Î¸ = copy(shared.concatenatedÎ¸)
	âˆ‡ = similar(Î¸)
	local x, min_err, min_Î¸
	min_err = typemax(eltype(Î¸)) #dummy variables
	min_Î¸ = copy(Î¸)
	losses, gradientnorms = fill(NaN, iterations+1), fill(NaN, iterations+1)
	optimizationtime = 0.0
	for i = 1:iterations
		iterationtime = @timed begin
			x = -loglikelihood!(model, shared, Î¸)
			âˆ‡negativeloglikelihood!(âˆ‡, Î³, model, shared, Î¸)
			losses[i] = x
			gradientnorms[i] = norm(âˆ‡)
			if x < min_err  # found a better solution
				min_err = x
				min_Î¸ = copy(Î¸)
			end
			Flux.update!(optimizer, Î¸, âˆ‡)
		end
		optimizationtime += iterationtime[2]
		println("iteration=", i, ", loss= ", losses[i], ", gradient norm= ", gradientnorms[i], ", time(s)= ", optimizationtime)
	end
	sortparameters!(model, min_Î¸, shared.indexÎ¸)
    return losses, gradientnorms
end

"""
    loglikelihood!(model, shared, concatenatedÎ¸)

Compute the log-likelihood

ARGUMENT
-`model`: an instance of FHM-DDM
-`shared`: a container of variables used by both the log-likelihood and gradient computation

UNMODIFIED ARGUMENT
-`concatenatedÎ¸`: a vector of concatenated parameter values

RETURN
-log-likelihood
"""
function loglikelihood!(model::Model,
						shared::Shared,
					    concatenatedÎ¸::Vector{<:Real}; useparallel=false)
	if concatenatedÎ¸ != shared.concatenatedÎ¸
		update!(model, shared, concatenatedÎ¸)
	end
	trialinvariant = Trialinvariant(model; purpose="loglikelihood")
	â„“ = map(model.trialsets, shared.pğ˜ğ‘‘) do trialset, pğ˜ğ‘‘
			if useparallel
				pmap(trialset.trials, pğ˜ğ‘‘) do trial, pğ˜ğ‘‘
					loglikelihood(pğ˜ğ‘‘, model.Î¸native, trial, trialinvariant)
				end
			else
				map(trialset.trials, pğ˜ğ‘‘) do trial, pğ˜ğ‘‘
					loglikelihood(pğ˜ğ‘‘, model.Î¸native, trial, trialinvariant)
				end
			end
		end
	return sum(sum(â„“))
end

"""
	loglikelihood(pğ˜ğ‘‘, Î¸native, trial, trialinvariant)

Compute the log-likelihood of the data from one trial

ARGUMENT
-`pğ˜ğ‘‘`: a matrix whose element `pğ˜ğ‘‘[t][i,j]` represents the conditional likelihood `p(ğ˜â‚œ, d âˆ£ ğšâ‚œ=i, ğœâ‚œ=j)`
-`Î¸native`: model parameters in their native space
-`trial`: stimulus and behavioral information of one trial
-`trialinvariant`: a structure containing quantities that are used in each trial

RETURN
-`â„“`: log-likelihood of the data from one trial
"""
function loglikelihood(pğ˜ğ‘‘::Vector{<:Matrix{<:Real}},
					   Î¸native::LatentÎ¸,
					   trial::Trial,
					   trialinvariant::Trialinvariant)
	@unpack clicks = trial
	@unpack Aáµƒsilent, Aá¶œáµ€, Î”t, Ï€á¶œáµ€, ğ›, Î = trialinvariant
	C = adapt(clicks, Î¸native.k[1], Î¸native.Ï•[1])
	Î¼ = Î¸native.Î¼â‚€[1] + trial.previousanswer*Î¸native.wâ‚•[1]
	Ïƒ = âˆšÎ¸native.ÏƒÂ²áµ¢[1]
	Ï€áµƒ = probabilityvector(Î¼, Ïƒ, ğ›)
	f = pğ˜ğ‘‘[1] .* Ï€áµƒ .* Ï€á¶œáµ€
	D = sum(f)
	f ./= D
	â„“ = log(D)
	T = eltype(pğ˜ğ‘‘[1])
	Aáµƒ = zeros(T, Î, Î)
	@inbounds for t = 2:trial.ntimesteps
		if isempty(clicks.inputindex[t])
			f .= Aáµƒsilent * f * Aá¶œáµ€
		else
			cL = sum(C[clicks.left[t]])
			cR = sum(C[clicks.right[t]])
			stochasticmatrix!(Aáµƒ, cL, cR, trialinvariant, Î¸native)
			f .= Aáµƒ * f * Aá¶œáµ€
		end
		f .*= pğ˜ğ‘‘[t]
		D = sum(f)
		f ./= D
		if (D < 0) || (isnan(D))
			println("f = ", f)
			error("negative or NaN D")
		end
		â„“ += log(D)
	end
	return â„“
end

"""
    loglikelihood(concatenatedÎ¸, indexÎ¸, model)

Compute the log-likelihood in a way that is compatible with ForwardDiff

UNMODIFIED ARGUMENT
-`concatenatedÎ¸`: a vector of concatenated parameter values
-`indexÎ¸`: struct indexing of each parameter in the vector of concatenated values
-`model`: an instance of FHM-DDM

RETURN
-log-likelihood
"""
function loglikelihood(	concatenatedÎ¸::Vector{<:Real},
					    indexÎ¸::IndexÎ¸,
						model::Model)
	model = sortparameters(concatenatedÎ¸, indexÎ¸, model)
	@unpack options, Î¸native, Î¸real, trialsets = model
	@unpack Î, K = options
	trialinvariant = Trialinvariant(model; purpose="loglikelihood")
	T = eltype(concatenatedÎ¸)
	pğ˜ğ‘‘=map(model.trialsets) do trialset
			map(trialset.trials) do trial
				map(1:trial.ntimesteps) do t
					ones(T,Î,K)
				end
			end
		end
    likelihood!(pğ˜ğ‘‘, trialsets, Î¸native.Ïˆ[1]) # `pğ˜ğ‘‘` is the conditional likelihood p(ğ˜â‚œ, d âˆ£ aâ‚œ, zâ‚œ)
	â„“ = map(trialsets, pğ˜ğ‘‘) do trialset, pğ˜ğ‘‘
			map(trialset.trials, pğ˜ğ‘‘) do trial, pğ˜ğ‘‘
				loglikelihood(pğ˜ğ‘‘, Î¸native, trial, trialinvariant)
			end
		end
	return sum(sum(â„“))
end

"""
    âˆ‡negativeloglikelihood!(âˆ‡, Î³, model, shared, concatenatedÎ¸)

Gradient of the negative log-likelihood of the factorial hidden Markov drift-diffusion model

MODIFIED INPUT
-`âˆ‡`: a vector of partial derivatives
-`Î³`: posterior probability of the latent variables
-`model`: a structure containing information of the factorial hidden Markov drift-diffusion model
-`shared`: structure containing variables shared between computations of the model's log-likelihood and its gradient

UNMODIFIED ARGUMENT
-`concatenatedÎ¸`: parameter values concatenated into a vetor
"""
function âˆ‡negativeloglikelihood!(âˆ‡::Vector{<:AbstractFloat},
								 Î³::Vector{<:Matrix{<:Vector{<:AbstractFloat}}},
								 model::Model,
								 shared::Shared,
								 concatenatedÎ¸::Vector{<:AbstractFloat})
	if concatenatedÎ¸ != shared.concatenatedÎ¸
		update!(model, shared, concatenatedÎ¸)
	end
	@unpack indexÎ¸, pğ˜ğ‘‘ = shared
	@unpack options, Î¸native, Î¸real, trialsets = model
	@unpack K = options
	trialinvariant = Trialinvariant(model; purpose="gradient")
	output=	map(trialsets, pğ˜ğ‘‘) do trialset, pğ˜ğ‘‘
				pmap(trialset.trials, pğ˜ğ‘‘) do trial, pğ˜ğ‘‘
					âˆ‡loglikelihood(pğ˜ğ‘‘, trialinvariant, Î¸native, trial)
				end
			end
	latentâˆ‡ = output[1][1][1] # reuse this memory
	for field in fieldnames(LatentÎ¸)
		latentâˆ‚ = getfield(latentâˆ‡, field)
		for i in eachindex(output)
			start = i==1 ? 2 : 1
			for m in start:length(output[i])
				latentâˆ‚[1] += getfield(output[i][m][1], field)[1] #output[i][m][1] are the partial derivatives
			end
		end
	end
	native2real!(latentâˆ‡, options, Î¸native, Î¸real)
	for field in fieldnames(LatentÎ¸)
		index = getfield(indexÎ¸.latentÎ¸,field)[1]
		if index != 0
			âˆ‡[index] = -getfield(latentâˆ‡,field)[1] # note the negative sign
		end
	end
	@inbounds for i in eachindex(output)
        t = 0
        for m in eachindex(output[i])
            for tâ‚˜ in eachindex(output[i][m][2]) # output[i][m][2] is `fb` of trial `m` in trialset `i`
                t += 1
                for jk in eachindex(output[i][m][2][tâ‚˜])
                    Î³[i][jk][t] = output[i][m][2][tâ‚˜][jk]
                end
            end
        end
    end
	Páµ¤ = length(trialsets[1].mpGLMs[1].Î¸.ğ®)
	Páµ¥ = length(trialsets[1].mpGLMs[1].Î¸.ğ¯)
	for i in eachindex(trialsets)
		âˆ‡glm = pmap(mpGLM->âˆ‡negativeexpectation(Î³[i], mpGLM;fit_a=options.fit_a, fit_b=options.fit_b), trialsets[i].mpGLMs)
		for n in eachindex(trialsets[i].mpGLMs)
			âˆ‡[indexÎ¸.glmÎ¸[i][n].ğ®] .= âˆ‡glm[n][1:Páµ¤]
			âˆ‡[indexÎ¸.glmÎ¸[i][n].ğ¯] .= âˆ‡glm[n][Páµ¤+1:Páµ¤+Páµ¥]
			counter = Páµ¤+Páµ¥
			options.fit_a && (âˆ‡[indexÎ¸.glmÎ¸[i][n].a] .= âˆ‡glm[n][counter+=1])
			options.fit_b && (âˆ‡[indexÎ¸.glmÎ¸[i][n].b] .= âˆ‡glm[n][counter+=1])
		end
	end
	return nothing
end

"""
    âˆ‡negativeloglikelihood(Î³, model, shared, concatenatedÎ¸)

Gradient of the negative log-likelihood implemented to be compatible with ForwardDiff

MODIFIED INPUT
-`Î³`: posterior probability of the latent variables
-`model`: a structure containing information of the factorial hidden Markov drift-diffusion model
-`shared`: structure containing variables shared between computations of the model's log-likelihood and its gradient

UNMODIFIED ARGUMENT
-`concatenatedÎ¸`: parameter values concatenated into a vetor
"""
function âˆ‡negativeloglikelihood(concatenatedÎ¸::Vector{T},
								indexÎ¸::IndexÎ¸,
								model::Model) where {T<:Real}
	model = sortparameters(concatenatedÎ¸, indexÎ¸, model)
	@unpack options, Î¸native, Î¸real, trialsets = model
	@unpack Î, K = options
	trialinvariant = Trialinvariant(model; purpose="gradient")
	pğ˜ğ‘‘=map(model.trialsets) do trialset
			map(trialset.trials) do trial
				map(1:trial.ntimesteps) do t
					ones(T,Î,K)
				end
			end
		end
    likelihood!(pğ˜ğ‘‘, trialsets, Î¸native.Ïˆ[1]) # `pğ˜ğ‘‘` is the conditional likelihood p(ğ˜â‚œ, d âˆ£ aâ‚œ, zâ‚œ)
	@unpack options, Î¸native, Î¸real, trialsets = model
	@unpack K = options
	trialinvariant = Trialinvariant(model; purpose="gradient")
	output=	map(trialsets, pğ˜ğ‘‘) do trialset, pğ˜ğ‘‘
				map(trialset.trials, pğ˜ğ‘‘) do trial, pğ˜ğ‘‘ #pmap
					âˆ‡loglikelihood(pğ˜ğ‘‘, trialinvariant, Î¸native, trial)
				end
			end
	latentâˆ‡ = output[1][1][1] # reuse this memory
	for field in fieldnames(LatentÎ¸)
		latentâˆ‚ = getfield(latentâˆ‡, field)
		for i in eachindex(output)
			start = i==1 ? 2 : 1
			for m in start:length(output[i])
				latentâˆ‚[1] += getfield(output[i][m][1], field)[1] #output[i][m][1] are the partial derivatives
			end
		end
	end
	native2real!(latentâˆ‡, options, Î¸native, Î¸real)
	âˆ‡ = similar(concatenatedÎ¸)
	for field in fieldnames(LatentÎ¸)
		index = getfield(indexÎ¸.latentÎ¸,field)[1]
		if index != 0
			âˆ‡[index] = -getfield(latentâˆ‡,field)[1] # note the negative sign
		end
	end
	@unpack K, Î = model.options
	Î³ =	map(model.trialsets) do trialset
			map(CartesianIndices((Î,K))) do index
				zeros(T, trialset.ntimesteps)
			end
		end
	@inbounds for i in eachindex(output)
        t = 0
        for m in eachindex(output[i])
            for tâ‚˜ in eachindex(output[i][m][2]) # output[i][m][2] is `fb` of trial `m` in trialset `i`
                t += 1
                for jk in eachindex(output[i][m][2][tâ‚˜])
                    Î³[i][jk][t] = output[i][m][2][tâ‚˜][jk]
                end
            end
        end
    end
	Páµ¤ = length(trialsets[1].mpGLMs[1].Î¸.ğ®)
	Páµ¥ = length(trialsets[1].mpGLMs[1].Î¸.ğ¯)
	for i in eachindex(trialsets)
		âˆ‡glm = map(mpGLM->âˆ‡negativeexpectation(Î³[i], mpGLM;fit_a=options.fit_a, fit_b=options.fit_b), trialsets[i].mpGLMs) #pmap
		for n in eachindex(trialsets[i].mpGLMs)
			âˆ‡[indexÎ¸.glmÎ¸[i][n].ğ®] .= âˆ‡glm[n][1:Páµ¤]
			âˆ‡[indexÎ¸.glmÎ¸[i][n].ğ¯] .= âˆ‡glm[n][Páµ¤+1:Páµ¤+Páµ¥]
			counter = Páµ¤+Páµ¥
			options.fit_a && (âˆ‡[indexÎ¸.glmÎ¸[i][n].a] .= âˆ‡glm[n][counter+=1])
			options.fit_b && (âˆ‡[indexÎ¸.glmÎ¸[i][n].b] .= âˆ‡glm[n][counter+=1])
		end
	end
	return âˆ‡
end

"""
	âˆ‡loglikelihood(pğ˜ğ‘‘, trialinvariant, Î¸native, trial)

Compute quantities needed for the gradient of the log-likelihood of the data observed in one trial

ARGUMENT
-`pğ˜ğ‘‘`: A vector of matrices of floating-point numbers whose element `pğ˜ğ‘‘[t][i,j]` represents the likelihood of the emissions (spike trains and choice) at time step `t` conditioned on the accumulator variable being in state `i` and the coupling variable in state `j`
-`trialinvariant`: structure containing quantities used across trials
-`Î¸native`: parameters for the latent variables in their native space
-`trial`: information on the stimuli and behavioral choice of one trial

RETURN
-`latentâˆ‡`: gradient of the log-likelihood of the data observed in one trial with respect to the parameters specifying the latent variables
-`fb`: joint posterior probabilities of the accumulator and coupling variables
"""
function âˆ‡loglikelihood(pğ˜ğ‘‘::Vector{<:Matrix{T}},
						trialinvariant::Trialinvariant,
						Î¸native::LatentÎ¸,
						trial::Trial) where {T<:Real}
	@unpack clicks = trial
	@unpack inputtimesteps, inputindex = clicks
	@unpack Aáµƒsilent, dAáµƒsilentdÎ¼, dAáµƒsilentdÏƒÂ², dAáµƒsilentdB, Aá¶œ, Aá¶œáµ€, Î”t, K, ğ›š, Ï€á¶œáµ€, Î, ğ› = trialinvariant
	dâ„“dk, dâ„“dÎ», dâ„“dÏ•, dâ„“dÏƒÂ²â‚, dâ„“dÏƒÂ²â‚›, dâ„“dB = 0., 0., 0., 0., 0., 0.
	âˆ‘Ï‡á¶œ = zeros(T, K,K)
	Î¼ = Î¸native.Î¼â‚€[1] + trial.previousanswer*Î¸native.wâ‚•[1]
	Ïƒ = âˆšÎ¸native.ÏƒÂ²áµ¢[1]
	Ï€áµƒ, dÏ€áµƒdÎ¼, dÏ€áµƒdÏƒÂ², dÏ€áµƒdB = zeros(T, Î), zeros(T, Î), zeros(T, Î), zeros(T, Î)
	probabilityvector!(Ï€áµƒ, dÏ€áµƒdÎ¼, dÏ€áµƒdÏƒÂ², dÏ€áµƒdB, Î¼, ğ›š, Ïƒ, ğ›)
	n_steps_with_input = length(clicks.inputtimesteps)
	Aáµƒ = map(x->zeros(T, Î,Î), clicks.inputtimesteps)
	dAáµƒdÎ¼ = map(x->zeros(T, Î,Î), clicks.inputtimesteps)
	dAáµƒdÏƒÂ² = map(x->zeros(T, Î,Î), clicks.inputtimesteps)
	dAáµƒdB = map(x->zeros(T, Î,Î), clicks.inputtimesteps)
	Î”c = zeros(T, n_steps_with_input)
	âˆ‘c = zeros(T, n_steps_with_input)
	C, dCdk, dCdÏ• = âˆ‡adapt(clicks, Î¸native.k[1], Î¸native.Ï•[1])
	for i in 1:n_steps_with_input
		t = clicks.inputtimesteps[i]
		cL = sum(C[clicks.left[t]])
		cR = sum(C[clicks.right[t]])
		stochasticmatrix!(Aáµƒ[i], dAáµƒdÎ¼[i], dAáµƒdÏƒÂ²[i], dAáµƒdB[i], cL, cR, trialinvariant, Î¸native)
		Î”c[i] = cR-cL
		âˆ‘c[i] = cL+cR
	end
	D, f = forward(Aáµƒ, inputindex, Ï€áµƒ, pğ˜ğ‘‘, trialinvariant)
	fb = f # reuse memory
	b = ones(T, Î,K)
	Aá¶œreshaped = reshape(Aá¶œ, 1, 1, K, K)
	if Î¸native.Î»[1] == 0.0
		dÎ¼dÎ”c = 1.0
		Î· = 0.0
		ğ›áµ€Î”texpÎ»Î”t = zeros(T, 1, length(ğ›))
	else
		Î»Î”t = Î¸native.Î»[1]*Î”t
		expÎ»Î”t = exp(Î»Î”t)
		dÎ¼dÎ”c = (expÎ»Î”t - 1.0)/Î»Î”t
		Î· = (expÎ»Î”t - dÎ¼dÎ”c)/Î¸native.Î»[1]
		ğ›áµ€Î”texpÎ»Î”t = transpose(ğ›).*Î”t.*expÎ»Î”t
	end
	@inbounds for t = trial.ntimesteps:-1:1
		if t < trial.ntimesteps # backward step
			Aáµƒâ‚œâ‚Šâ‚ = isempty(inputindex[t+1]) ? Aáµƒsilent : Aáµƒ[inputindex[t+1][1]]
			b .*= pğ˜ğ‘‘[t+1]
			b = transpose(Aáµƒâ‚œâ‚Šâ‚) * b * Aá¶œ / D[t+1]
			fb[t] .*= b
		end
		if t > 1 # joint posterior over consecutive time bins, computations involving the transition matrix
			if isempty(inputindex[t])
				Aáµƒâ‚œ = Aáµƒsilent
				dAáµƒâ‚œdÎ¼ = dAáµƒsilentdÎ¼
				dAáµƒâ‚œdÏƒÂ² = dAáµƒsilentdÏƒÂ²
				dAáµƒâ‚œdB = dAáµƒsilentdB
			else
				i = inputindex[t][1]
				Aáµƒâ‚œ = Aáµƒ[i]
				dAáµƒâ‚œdÎ¼ = dAáµƒdÎ¼[i]
				dAáµƒâ‚œdÏƒÂ² = dAáµƒdÏƒÂ²[i]
				dAáµƒâ‚œdB = dAáµƒdB[i]
			end
			Ï‡_oslash_Aáµƒ = reshape(pğ˜ğ‘‘[t].*b, Î, 1, K, 1) .* reshape(f[t-1], 1, Î, 1, K) .* Aá¶œreshaped ./ D[t]
	        âˆ‘Ï‡á¶œ += dropdims(sum(Ï‡_oslash_Aáµƒ.*Aáµƒâ‚œ, dims=(1,2)); dims=(1,2))
			Ï‡áµƒ_Aáµƒ = dropdims(sum(Ï‡_oslash_Aáµƒ, dims=(3,4)); dims=(3,4))
			Ï‡áµƒ_dlogAáµƒdÎ¼ = Ï‡áµƒ_Aáµƒ .* dAáµƒâ‚œdÎ¼ # Ï‡áµƒâŠ™ d/dÎ¼{log(Aáµƒ)} = Ï‡áµƒâŠ˜ AáµƒâŠ™ d/dÎ¼{Aáµƒ}
			âˆ‘_Ï‡áµƒ_dlogAáµƒdÎ¼ = sum(Ï‡áµƒ_dlogAáµƒdÎ¼)
			âˆ‘_Ï‡áµƒ_dlogAáµƒdÏƒÂ² = sum(Ï‡áµƒ_Aáµƒ .* dAáµƒâ‚œdÏƒÂ²) # similarly, Ï‡áµƒâŠ™ d/dÏƒÂ²{log(Aáµƒ)} = Ï‡áµƒâŠ˜ AáµƒâŠ™ d/dÏƒÂ²{Aáµƒ}
			dâ„“dÏƒÂ²â‚ += âˆ‘_Ï‡áµƒ_dlogAáµƒdÏƒÂ² # the Î”t is multiplied after summing across time steps
			dâ„“dB += sum(Ï‡áµƒ_Aáµƒ .* dAáµƒâ‚œdB)
			if isempty(inputindex[t])
				dÎ¼dÎ» = ğ›áµ€Î”texpÎ»Î”t
			else
				dÎ¼dÎ» = ğ›áµ€Î”texpÎ»Î”t .+ Î”c[i].*Î·
				dâ„“dÏƒÂ²â‚› += âˆ‘_Ï‡áµƒ_dlogAáµƒdÏƒÂ²*âˆ‘c[i]
				dcLdÏ• = sum(dCdÏ•[clicks.left[t]])
				dcRdÏ• = sum(dCdÏ•[clicks.right[t]])
				dcLdk = sum(dCdk[clicks.left[t]])
				dcRdk = sum(dCdk[clicks.right[t]])
				dÏƒÂ²dÏ• = Î¸native.ÏƒÂ²â‚›[1]*(dcLdÏ• + dcRdÏ•)
				dÏƒÂ²dk = Î¸native.ÏƒÂ²â‚›[1]*(dcLdk + dcRdk)
				dâ„“dÏ• += âˆ‘_Ï‡áµƒ_dlogAáµƒdÎ¼*dÎ¼dÎ”c*(dcRdÏ• - dcLdÏ•) + âˆ‘_Ï‡áµƒ_dlogAáµƒdÏƒÂ²*dÏƒÂ²dÏ•
				dâ„“dk += âˆ‘_Ï‡áµƒ_dlogAáµƒdÎ¼*dÎ¼dÎ”c*(dcRdk - dcLdk) + âˆ‘_Ï‡áµƒ_dlogAáµƒdÏƒÂ²*dÏƒÂ²dk
			end
			dâ„“dÎ» += sum(Ï‡áµƒ_dlogAáµƒdÎ¼.*dÎ¼dÎ»)
		end
	end
	dâ„“dÏƒÂ²â‚ *= Î”t
	dâ„“dAá¶œâ‚â‚ = âˆ‘Ï‡á¶œ[1,1]/Aá¶œ[1,1] - âˆ‘Ï‡á¶œ[2,1]/Aá¶œ[2,1]
	dâ„“dAá¶œâ‚‚â‚‚ = âˆ‘Ï‡á¶œ[2,2]/Aá¶œ[2,2] - âˆ‘Ï‡á¶œ[1,2]/Aá¶œ[1,2]
	âˆ‘Î³á¶œâ‚ = sum(fb[1], dims=1)
	dâ„“dxÏ€á¶œâ‚ = (âˆ‘Î³á¶œâ‚[1] - Î¸native.Ï€á¶œâ‚[1])/Î¸native.Ï€á¶œâ‚[1]/(1.0 - Î¸native.Ï€á¶œâ‚[1])
	Î³áµƒâ‚_oslash_Ï€áµƒ = sum(pğ˜ğ‘‘[1] .* Ï€á¶œáµ€ ./ D[1] .* b, dims=2)
	âˆ‘_Î³áµƒâ‚_dlogÏ€áµƒdÎ¼ = Î³áµƒâ‚_oslash_Ï€áµƒ â‹… dÏ€áµƒdÎ¼ # similar to above, Î³áµƒâ‚âŠ™ d/dÎ¼{log(Ï€áµƒ)} = Î³áµƒâ‚âŠ˜ Ï€áµƒâŠ™ d/dÎ¼{Ï€áµƒ}
	dâ„“dÎ¼â‚€ = âˆ‘_Î³áµƒâ‚_dlogÏ€áµƒdÎ¼
	dâ„“dwâ‚• = âˆ‘_Î³áµƒâ‚_dlogÏ€áµƒdÎ¼ * trial.previousanswer
	dâ„“dÏƒÂ²áµ¢ = Î³áµƒâ‚_oslash_Ï€áµƒ â‹… dÏ€áµƒdÏƒÂ²
	dâ„“dB += Î³áµƒâ‚_oslash_Ï€áµƒ â‹… dÏ€áµƒdB
	dâ„“dÏˆ = differentiateâ„“_wrt_Ïˆ(trial.choice, f[end], Î¸native.Ïˆ[1])
	latentâˆ‡ = LatentÎ¸(	Aá¶œâ‚â‚ = [dâ„“dAá¶œâ‚â‚],
						Aá¶œâ‚‚â‚‚ = [dâ„“dAá¶œâ‚‚â‚‚],
						k	 = [dâ„“dk],
						Î»	 = [dâ„“dÎ»],
						Î¼â‚€	 = [dâ„“dÎ¼â‚€],
						Ï•	 = [dâ„“dÏ•],
						Ï€á¶œâ‚	 = [dâ„“dxÏ€á¶œâ‚],
						Ïˆ	 = [dâ„“dÏˆ],
						ÏƒÂ²â‚	 = [dâ„“dÏƒÂ²â‚],
						ÏƒÂ²áµ¢	 = [dâ„“dÏƒÂ²áµ¢],
						ÏƒÂ²â‚›	 = [dâ„“dÏƒÂ²â‚›],
						wâ‚•	 = [dâ„“dwâ‚•],
						B	 = [dâ„“dB])
	return latentâˆ‡, fb
end

"""
	differentiateâ„“_wrt_Ïˆ(choice, Î³_end, Ïˆ)

Partial derivative of the log-likelihood of the data from one trial with respect to the lapse rate Ïˆ

ARGUMENT
-`choice`: a Boolean specifying whether the choice was to the right
-`Î³_end`: a matrix of floating-point numbers representing the posterior likelihood of the latent variables at the end of the trial (i.e., last time step). Element `Î³_end[i,j]` = p(aáµ¢=1, câ±¼=1 âˆ£ ğ˜, d). Rows correspond to states of the accumulator state variable ğš, and columns to states of the coupling variable ğœ.
-`Ïˆ`: a floating-point number specifying the lapse rate

RETURN
-a floating-point number quantifying the partial derivative of the log-likelihood of one trial's data with respect to the lapse rate Ïˆ
"""
function differentiateâ„“_wrt_Ïˆ(choice::Bool, Î³_end::Array{<:Real}, Ïˆ::Real)
	Î³áµƒ_end = sum(Î³_end, dims=2)
	zeroindex = cld(length(Î³áµƒ_end), 2)
	if choice
		choiceconsistent   = sum(Î³áµƒ_end[zeroindex+1:end])
		choiceinconsistent = sum(Î³áµƒ_end[1:zeroindex-1])
	else
		choiceconsistent   = sum(Î³áµƒ_end[1:zeroindex-1])
		choiceinconsistent = sum(Î³áµƒ_end[zeroindex+1:end])
	end
	return choiceconsistent/(Ïˆ-2) + choiceinconsistent/Ïˆ
end

"""
	Trialinvariant(options, Î¸native)

Compute quantities that are used in each trial for computing gradient of the log-likelihood

ARGUMENT
-`model`: custom type containing the settings, data, and parameters of a factorial hidden Markov drift-diffusion model
"""
function Trialinvariant(model::Model; purpose="gradient")
	@unpack options, Î¸native, Î¸real = model
	@unpack Î”t, K, Î = options
	Î» = Î¸native.Î»[1]
	B = Î¸native.B[1]
	Aá¶œâ‚â‚ = Î¸native.Aá¶œâ‚â‚[1]
	Aá¶œâ‚‚â‚‚ = Î¸native.Aá¶œâ‚‚â‚‚[1]
	Ï€á¶œâ‚ = Î¸native.Ï€á¶œâ‚[1]
	Aá¶œ = [Aá¶œâ‚â‚ 1-Aá¶œâ‚‚â‚‚; 1-Aá¶œâ‚â‚ Aá¶œâ‚‚â‚‚]
	Aá¶œáµ€ = [Aá¶œâ‚â‚ 1-Aá¶œâ‚â‚; 1-Aá¶œâ‚‚â‚‚ Aá¶œâ‚‚â‚‚]
	Ï€á¶œáµ€ = [Ï€á¶œâ‚ 1-Ï€á¶œâ‚]
	ğ› = B*(2collect(1:Î) .- Î .- 1)/(Î-2)
	ğ› = conditionedmean(0.0, Î”t, Î¸native.Î»[1], ğ›)
	Ïƒ = âˆš(Î¸native.ÏƒÂ²â‚[1]*Î”t)
	T = eltype(ğ›)
	Aáµƒsilent = zeros(T,Î,Î)
	if purpose=="gradient"
		ğ›š = (2collect(1:Î) .- Î .- 1)/2
		Î© = ğ›š .- transpose(ğ›š).*exp.(Î».*Î”t)
		dAáµƒsilentdÎ¼ = zeros(T,Î,Î)
		dAáµƒsilentdÏƒÂ² = zeros(T,Î,Î)
		dAáµƒsilentdB = zeros(T,Î,Î)
		stochasticmatrix!(Aáµƒsilent, dAáµƒsilentdÎ¼, dAáµƒsilentdÏƒÂ², dAáµƒsilentdB, ğ›, Ïƒ, Î©, ğ›)
		Trialinvariant( Aáµƒsilent=Aáµƒsilent,
					Aá¶œ=Aá¶œ,
					Aá¶œáµ€=Aá¶œáµ€,
					dAáµƒsilentdÎ¼=dAáµƒsilentdÎ¼,
					dAáµƒsilentdÏƒÂ²=dAáµƒsilentdÏƒÂ²,
					dAáµƒsilentdB=dAáµƒsilentdB,
					Î”t=options.Î”t,
					ğ›š=ğ›š,
					Î©=Î©,
					Ï€á¶œáµ€=Ï€á¶œáµ€,
					Î=Î,
 				    K=K,
					ğ›=ğ›)
	elseif purpose=="loglikelihood"
		stochasticmatrix!(Aáµƒsilent, ğ›, Ïƒ, ğ›)
		Trialinvariant(Aáµƒsilent=Aáµƒsilent,
				   Aá¶œáµ€=Aá¶œáµ€,
				   Î”t=options.Î”t,
				   Ï€á¶œáµ€=Ï€á¶œáµ€,
				   ğ›=ğ›,
				   K=K,
				   Î=Î)
	end
end

"""
	Shared(model)

Create variables that are shared by the computations of the log-likelihood and its gradient

ARGUMENT
-`model`: structure with information about the factorial hidden Markov drift-diffusion model

OUTPUT
-an instance of the custom type `Shared`, which contains the shared quantities
"""
function Shared(model::Model)
	@unpack K, Î = model.options
	pğ˜ğ‘‘ = likelihood(model)
	concatenatedÎ¸, indexÎ¸ = concatenateparameters(model)
	Shared(	concatenatedÎ¸=concatenatedÎ¸,
			indexÎ¸=indexÎ¸,
			pğ˜ğ‘‘=pğ˜ğ‘‘)
end

"""
	update!(model, shared, concatenatedÎ¸)

Update the model and the shared quantities according to new parameter values

ARGUMENT
-`model`: structure with information concerning a factorial hidden Markov drift-diffusion model
-`shared`: structure containing variables shared between computations of the model's log-likelihood and its gradient
-`concatenatedÎ¸`: newest values of the model's parameters
"""
function update!(model::Model,
				 shared::Shared,
				 concatenatedÎ¸::Vector{<:Real})
	shared.concatenatedÎ¸ .= concatenatedÎ¸
	sortparameters!(model, shared.concatenatedÎ¸, shared.indexÎ¸)
	if !isempty(shared.pğ˜ğ‘‘[1][1][1])
	    likelihood!(shared.pğ˜ğ‘‘, model.trialsets, model.Î¸native.Ïˆ[1])
	end
	return nothing
end
