"""
    maximizelikelihood!(model, optimizer)

Optimize the parameters of the factorial hidden Markov drift-diffusion model using a first-order optimizer in Optim

MODIFIED ARGUMENT
- a structure containing information for a factorial hidden Markov drift-diffusion model

UNMODIFIED ARGUMENT
-`optimizer`: an optimizer implemented by Optim.jl. The limited memory quasi-Newton algorithm `LBFGS()` does pretty well, and when using L-BFGS the `HagerZhang()` line search seems to do better than `BackTracking()`

OPTIONAL ARGUMENT
-`extended_trace`: save additional information
-`f_tol`: threshold for determining convergence in the objective value
-`g_tol`: threshold for determining convergence in the gradient
-`iterations`: number of inner iterations that will be run before the optimizer gives up
-`show_every`: trace output is printed every `show_every`th iteration.
-`show_trace`: should a trace of the optimizer's state be shown?
-`x_tol`: threshold for determining convergence in the input vector

RETURN
`losses`: value of the loss function (negative of the model's log-likelihood) across iterations. If `store_trace` were set to false, then these are NaN's
`gradientnorms`: 2-norm of the gradient of  of the loss function across iterations. If `store_trace` were set to false, then these are NaN's
"""
function maximizelikelihood!(model::Model,
							 optimizer::Optim.FirstOrderOptimizer;
			                 extended_trace::Bool=false,
			                 f_tol::AbstractFloat=0.0,
			                 iterations::Integer=500,
			                 show_every::Integer=10,
			                 show_trace::Bool=true,
							 store_trace::Bool=true,
			                 x_tol::AbstractFloat=0.0)
	memory = Memoryforgradient(model)
    f(concatenatedŒ∏) = -loglikelihood!(model, memory, concatenatedŒ∏)
    g!(‚àá, concatenatedŒ∏) = ‚àánegativeloglikelihood!(‚àá, memory, model, concatenatedŒ∏)
    Optim_options = Optim.Options(extended_trace=extended_trace,
								  f_tol=f_tol,
                                  g_tol=model.options.g_tol,
                                  iterations=iterations,
                                  show_every=show_every,
                                  show_trace=show_trace,
								  store_trace=store_trace,
                                  x_tol=x_tol)
	Œ∏‚ÇÄ = concatenateparameters(model)[1]
	optimizationresults = Optim.optimize(f, g!, Œ∏‚ÇÄ, optimizer, Optim_options)
    Œ∏‚Çò‚Çó = Optim.minimizer(optimizationresults)
	sortparameters!(model, Œ∏‚Çò‚Çó, memory.indexŒ∏)
	real2native!(model.Œ∏native, model.options, model.Œ∏real)
	println(optimizationresults)
	losses, gradientnorms = fill(NaN, iterations+1), fill(NaN, iterations+1)
	if store_trace
		traces = Optim.trace(optimizationresults)
		for i in eachindex(traces)
			gradientnorms[i] = traces[i].g_norm
			losses[i] = traces[i].value
		end
	end
    return losses, gradientnorms
end

"""
    maximizelikelihood!(model, optimizer)

Optimize the parameters of the factorial hidden Markov drift-diffusion model using an algorithm implemented by Flux.jl

MODIFIED ARGUMENT
- a structure containing information for a factorial hidden Markov drift-diffusion model

UNMODIFIED ARGUMENT
-`optimizer`: optimization algorithm implemented by Flux.jl

OPTIONAL ARGUMENT
-`iterations`: maximum number of iterations

RETURN
-see documentation for `maximizelikelihood!(model, optimizer)`
```
"""
function maximizelikelihood!(model::Model, optimizer::Flux.Optimise.AbstractOptimiser; iterations::Integer = 3000)
	memory = Memoryforgradient(model)
	Œ∏ = concatenateparameters(model)[1]
	‚àá = similar(Œ∏)
	local x, min_err, min_Œ∏
	min_err = typemax(eltype(Œ∏)) #dummy variables
	min_Œ∏ = copy(Œ∏)
	losses, gradientnorms = fill(NaN, iterations+1), fill(NaN, iterations+1)
	optimizationtime = 0.0
	for i = 1:iterations
		iterationtime = @timed begin
			x = -loglikelihood!(model, memory, Œ∏)
			‚àánegativeloglikelihood!(‚àá, memory, model, Œ∏)
			losses[i] = x
			gradientnorms[i] = norm(‚àá)
			if x < min_err  # found a better solution
				min_err = x
				min_Œ∏ = copy(Œ∏)
			end
			Flux.update!(optimizer, Œ∏, ‚àá)
		end
		optimizationtime += iterationtime[2]
		println("iteration=", i, ", loss= ", losses[i], ", gradient norm= ", gradientnorms[i], ", time(s)= ", optimizationtime)
	end
	sortparameters!(model, min_Œ∏, memory.indexŒ∏)
	real2native!(model.Œ∏native, model.options, model.Œ∏real)
    return losses, gradientnorms
end

"""
	loglikelihood(model)

Log of the likelihood of the data given the parameters
"""
loglikelihood(model::Model) = loglikelihood!(model, Memoryforgradient(model), concatenateparameters(model)[1])

"""
    loglikelihood!(model, memory, concatenatedŒ∏)

Compute the log-likelihood

MODIFIED ARGUMENT
-`model`: an instance of FHM-DDM
-`memory`: a container of variables used by both the log-likelihood and gradient computation

UNMODIFIED ARGUMENT
-`concatenatedŒ∏`: a vector of concatenated parameter values

RETURN
-log-likelihood

EXAMPLE
```julia-repl
julia> using FHMDDM
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_04_27_test/data.mat"; randomize=true);
julia> concatenatedŒ∏, indexŒ∏ = FHMDDM.concatenateparameters(model)
julia> memory = FHMDDM.Memoryforgradient(model)
julia> ‚Ñì = loglikelihood!(model, memory, memory.concatenatedŒ∏)
julia> ‚Ñì = loglikelihood!(model, memory, rand(length(memory.concatenatedŒ∏)))
```
"""
function loglikelihood!(model::Model,
						memory::Memoryforgradient,
					    concatenatedŒ∏::Vector{<:Real})
	if concatenatedŒ∏ != memory.concatenatedŒ∏
		P = update!(memory, model, concatenatedŒ∏)
		memory.‚Ñì[1] = 0.0
		@inbounds for s in eachindex(model.trialsets)
			for m in eachindex(model.trialsets[s].trials)
				memory.‚Ñì[1] += loglikelihood(memory.pùêòùëë[s][m], memory.pùëë_a[s][m], memory, P, model.Œ∏native, model.trialsets[s].trials[m])
			end
		end
	end
	memory.‚Ñì[1]
end

"""
	loglikelihood(pùêòùëë, Œ∏native, trial)

Compute the log-likelihood of the data from one trial

ARGUMENT
-`pùêòùëë`: a matrix whose element `pùêòùëë[t][i,j]` represents the conditional likelihood `p(ùêò‚Çú, d ‚à£ ùêö‚Çú=i, ùêú‚Çú=j)`
-`Œ∏native`: model parameters in their native space
-`trial`: stimulus and behavioral information of one trial

RETURN
-`‚Ñì`: log-likelihood of the data from one trial

EXAMPLE
```julia-repl
julia> using FHMDDM, ForwardDiff
julia> datapath = "/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_07_18a_test/T176_2018_05_03_scaled/data.mat"
julia> model = Model(datapath)
julia> concatenatedŒ∏, indexŒ∏ = FHMDDM.concatenateparameters(model)
julia> ‚Ñì = FHMDDM.loglikelihood(concatenatedŒ∏, indexŒ∏, model)
julia> f(x) = FHMDDM.loglikelihood(x, indexŒ∏, model)
julia> memory = FHMDDM.Memoryforgradient(model)
julia> ‚Ñì2 = FHMDDM.loglikelihood!(model, memory, concatenatedŒ∏) #ForwardDiff-incompatible
julia> abs(‚Ñì2-‚Ñì)
julia>
```
"""
function loglikelihood(pùêòùëë::Vector{<:Matrix{<:Real}},
					   pùëë_a::Vector{<:Real},
   					   memory::Memoryforgradient,
					   P::Probabilityvector,
					   Œ∏native::LatentŒ∏,
					   trial::Trial)
	@unpack clicks = trial
	@unpack A·µÉinput, A·µÉsilent, A·∂ú·µÄ, choiceLLscaling, œÄ·∂ú·µÄ = memory
    if length(clicks.time) > 0
		adaptedclicks = adapt(clicks, Œ∏native.k[1], Œ∏native.œï[1])
	end
	priorprobability!(P, trial.previousanswer)
	pùêö‚Çú = P.ùõë
	f = pùêòùëë[1] .* pùêö‚Çú .* œÄ·∂ú·µÄ
	D = sum(f)
	f ./= D
	‚Ñì = log(D)
	@inbounds for t = 2:trial.ntimesteps
		if isempty(clicks.inputindex[t])
			A·µÉ = A·µÉsilent
		else
			A·µÉ = A·µÉinput[clicks.inputindex[t][1]]
			update_for_transition_probabilities!(P, adaptedclicks, clicks, t)
			transitionmatrix!(A·µÉ, P)
		end
		f = pùêòùëë[t].*(A·µÉ * f * A·∂ú·µÄ)
		D = sum(f)
		f ./= D
		‚Ñì += log(D)
		if choiceLLscaling > 1
			pùêö‚Çú = A·µÉ*pùêö‚Çú
		end
	end
	if choiceLLscaling > 1
		‚Ñì += (choiceLLscaling-1)*log(dot(pùëë_a, pùêö‚Çú))
	end
	return ‚Ñì
end

"""
    loglikelihood(concatenatedŒ∏, indexŒ∏, model)

ForwardDiff-compatible computation of the log-likelihood

ARGUMENT
-`concatenatedŒ∏`: a vector of concatenated parameter values
-`indexŒ∏`: struct indexing of each parameter in the vector of concatenated values
-`model`: an instance of FHM-DDM

RETURN
-log-likelihood
"""
function loglikelihood(concatenatedŒ∏::Vector{T}, indexŒ∏::IndexŒ∏, model::Model) where {T<:Real}
	model = Model(concatenatedŒ∏, indexŒ∏, model)
	@unpack options, Œ∏native, Œ∏real, trialsets = model
	@unpack Œît, minpa, K, Œû = options
	pùêòùëë=map(model.trialsets) do trialset
			map(trialset.trials) do trial
				map(1:trial.ntimesteps) do t
					ones(T,Œû,K)
				end
			end
		end
	pùëë_a=map(model.trialsets) do trialset
			map(trialset.trials) do trial
				ones(T,Œû)
			end
		end
    scaledlikelihood!(pùêòùëë, pùëë_a, trialsets, Œ∏native.œà[1])
	choiceLLscaling = scaling_factor_choiceLL(model)
	A·µÉinput = ones(T,Œû,Œû).*minpa
	one_minus_Œûminpa = 1.0-Œû*minpa
	A·µÉinput[1,1] += one_minus_Œûminpa
	A·µÉinput[Œû,Œû] += one_minus_Œûminpa
	A·µÉsilent = copy(A·µÉinput)
	expŒªŒît = exp(Œ∏native.Œª[1]*Œît)
	dŒº_dŒîc = differentiate_Œº_wrt_Œîc(Œît, Œ∏native.Œª[1])
	dùõè_dB = (2 .*collect(1:Œû) .- Œû .- 1)./(Œû-2)
	ùõè = Œ∏native.B[1].*dùõè_dB
	transitionmatrix!(A·µÉsilent, minpa, expŒªŒît.*ùõè, ‚àö(Œît*Œ∏native.œÉ¬≤‚Çê[1]), ùõè)
	A·∂ú‚ÇÅ‚ÇÅ = Œ∏native.A·∂ú‚ÇÅ‚ÇÅ[1]
	A·∂ú‚ÇÇ‚ÇÇ = Œ∏native.A·∂ú‚ÇÇ‚ÇÇ[1]
	œÄ·∂ú‚ÇÅ = Œ∏native.œÄ·∂ú‚ÇÅ[1]
	if K == 2
		A·∂ú·µÄ = [A·∂ú‚ÇÅ‚ÇÅ 1-A·∂ú‚ÇÅ‚ÇÅ; 1-A·∂ú‚ÇÇ‚ÇÇ A·∂ú‚ÇÇ‚ÇÇ]
		œÄ·∂ú·µÄ = [œÄ·∂ú‚ÇÅ 1-œÄ·∂ú‚ÇÅ]
	else
		A·∂ú·µÄ = ones(T,1,1)
		œÄ·∂ú·µÄ = ones(T,1,1)
	end
	‚Ñì = zero(T)
	@inbounds for s in eachindex(trialsets)
		for m in eachindex(trialsets[s].trials)
			trial = trialsets[s].trials[m]
			pùêö‚Çú = probabilityvector(minpa, Œ∏native.Œº‚ÇÄ[1]+Œ∏native.w‚Çï[1]*trial.previousanswer, ‚àöŒ∏native.œÉ¬≤·µ¢[1], ùõè)
			f = pùêòùëë[s][m][1] .* pùêö‚Çú .* œÄ·∂ú·µÄ
			D = sum(f)
			f./=D
			‚Ñì+=log(D)
			if length(trial.clicks.time) > 0
				adaptedclicks = adapt(trial.clicks, Œ∏native.k[1], Œ∏native.œï[1])
			end
			for t=2:trial.ntimesteps
				if t ‚àà trial.clicks.inputtimesteps
					cL = sum(adaptedclicks.C[trial.clicks.left[t]])
					cR = sum(adaptedclicks.C[trial.clicks.right[t]])
					ùõç = expŒªŒît.*ùõè .+ (cR-cL).*dŒº_dŒîc
					œÉ = ‚àö((cR+cL)*Œ∏native.œÉ¬≤‚Çõ[1] + Œît*Œ∏native.œÉ¬≤‚Çê[1])
					transitionmatrix!(A·µÉinput, minpa, ùõç, œÉ, ùõè)
					A·µÉ = A·µÉinput
				else
					A·µÉ = A·µÉsilent
				end
				f = pùêòùëë[s][m][t] .* (A·µÉ * f * A·∂ú·µÄ)
				D = sum(f)
				f./=D
				‚Ñì+=log(D)
				if choiceLLscaling > 1
					pùêö‚Çú = A·µÉ*pùêö‚Çú
				end
			end
			if choiceLLscaling > 1
				‚Ñì += (choiceLLscaling-1)*log(dot(pùëë_a[s][m], pùêö‚Çú))
			end
		end
	end
	‚Ñì
end

"""
	‚àánegativeloglikelihood!(‚àán‚Ñì, memory, model, concatenatedŒ∏)

Update the gradient of the negative log-likelihood of the model

MODIFIED ARGUMENT
-`‚àán‚Ñì`: the vector representing the gradient
-`memory`: memory allocated for computing the gradient. The log-likelihood is updated.
-`model`: structure containing the data, parameters, and hyperparameters of the model

ARGUMENT
-`concatenatedŒ∏`: values of the model's parameters concatenated into a vector

EXAMPLE
```julia-repl
julia> using FHMDDM, ForwardDiff
julia> datapath = "/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_07_18a_test/T176_2018_05_03_scaled/data.mat"
julia> model = Model(datapath)
julia> concatenatedŒ∏, indexŒ∏ = FHMDDM.concatenateparameters(model)
julia> ‚àán‚Ñì = similar(concatenatedŒ∏)
julia> memory = FHMDDM.Memoryforgradient(model)
julia> FHMDDM.‚àánegativeloglikelihood!(‚àán‚Ñì, memory, model, concatenatedŒ∏)
julia> f(x) = -FHMDDM.loglikelihood(x, indexŒ∏, model)
julia> ‚Ñì_auto = f(concatenatedŒ∏)
julia> ‚àán‚Ñì_auto = ForwardDiff.gradient(f, concatenatedŒ∏)
julia> println("")
julia> println("   max(|Œîloss|): ", abs(‚Ñì_auto + memory.‚Ñì[1]))
julia> println("   max(|Œîgradient|): ", maximum(abs.(‚àán‚Ñì_auto .- ‚àán‚Ñì)))
julia>
```
"""
function ‚àánegativeloglikelihood!(‚àán‚Ñì::Vector{<:Real},
 								 memory::Memoryforgradient,
								 model::Model,
								 concatenatedŒ∏::Vector{<:AbstractFloat})
	if concatenatedŒ∏ != memory.concatenatedŒ∏
		P = update!(memory, model, concatenatedŒ∏)
	else
		P = Probabilityvector(model.options.Œît, model.options.minpa, model.Œ∏native, model.options.Œû)
	end
	‚àáloglikelihood!(memory,model,P)
	indexall = 0
	indexfit = 0
	for field in fieldnames(LatentŒ∏)
		indexall+=1
		if getfield(memory.indexŒ∏.latentŒ∏, field)[1] > 0
			indexfit +=1
			‚àán‚Ñì[indexfit] = -memory.‚àá‚Ñìlatent[indexall]
		end
	end
	native2real!(‚àán‚Ñì, memory.indexŒ∏.latentŒ∏, model)
	‚àánegativeloglikelihood!(‚àán‚Ñì, memory.‚àá‚Ñìglm, indexfit)
	return nothing
end

"""
	‚àáloglikelihood!(memory, model, P)

MODIFIED ARGUMENT
-`memory`: memory allocated for computing the gradient. The log-likelihood is updated.
-`model`: structure containing the data, parameters, and hyperparameters of the model
-`P`: a structure containing allocated memory for computing the accumulator's initial and transition probabilities as well as the partial derivatives of these probabilities
"""
function ‚àáloglikelihood!(memory::Memoryforgradient,
						 model::Model,
						 P::Probabilityvector)
	memory.‚Ñì .= 0.0
	memory.‚àá‚Ñìlatent .= 0.0
	@inbounds for s in eachindex(model.trialsets)
		for m in eachindex(model.trialsets[s].trials)
			‚àáloglikelihood!(memory, model, P, s, m)
		end
	end
	@inbounds for s in eachindex(model.trialsets)
		N = length(model.trialsets[s].mpGLMs)
		sf = 1/N
		for n = 1:N
			expectation_‚àáloglikelihood!(memory.‚àá‚Ñìglm[s][n], memory.Œ≥[s], model.trialsets[s].mpGLMs[n])
			scale_expectation_‚àáloglikelihood!(memory.‚àá‚Ñìglm[s][n], sf)
		end
	end
	return nothing
end

"""
	‚àáloglikelihood!(memory, model, P, s, m)

Update the gradient

MODIFIED ARGUMENT
-`memory`: memory allocated for computing the gradient. The log-likelihood is updated.
-`model`: structure containing the data, parameters, and hyperparameters of the model
-`P`: a structure containing allocated memory for computing the accumulator's initial and transition probabilities as well as the partial derivatives of these probabilities
-`s`: index of the trialset
-`m`: index of the trial
"""
function ‚àáloglikelihood!(memory::Memoryforgradient,
						 model::Model,
						 P::Probabilityvector,
						 s::Integer,
						 m::Integer)
	trial = model.trialsets[s].trials[m]
	pùêòùëë = memory.pùêòùëë[s][m]
	pùëë_a = memory.pùëë_a[s][m]
	@unpack Œ∏native = model
	@unpack clicks = trial
	@unpack inputtimesteps, inputindex = clicks
	@unpack A·µÉinput, ‚àáA·µÉinput, A·µÉsilent, ‚àáA·µÉsilent, A·∂ú, A·∂ú·µÄ, ‚àáA·∂ú, choiceLLscaling, D, f, f·∂ú, indexŒ∏_pa‚ÇÅ, indexŒ∏_pa‚Çúa‚Çú‚Çã‚ÇÅ, indexŒ∏_pc‚ÇÅ, indexŒ∏_pc‚Çúc‚Çú‚Çã‚ÇÅ, indexŒ∏_œà, K, ‚Ñì, ‚àá‚Ñìlatent, nŒ∏_pa‚ÇÅ, nŒ∏_pa‚Çúa‚Çú‚Çã‚ÇÅ, nŒ∏_pc‚ÇÅ, nŒ∏_pc‚Çúc‚Çú‚Çã‚ÇÅ, ‚àápa‚ÇÅ, œÄ·∂ú, ‚àáœÄ·∂ú, Œû = memory
	if length(clicks.time) > 0
		adaptedclicks = ‚àáadapt(trial.clicks, Œ∏native.k[1], Œ∏native.œï[1])
	end
	t = 1
	‚àápriorprobability!(‚àápa‚ÇÅ, P, trial.previousanswer)
	f·∂ú[1] = copy(P.ùõë)
	pa‚ÇÅ = f·∂ú[1]
	@inbounds for j=1:Œû
		for k = 1:K
			f[t][j,k] = pùêòùëë[t][j,k] * pa‚ÇÅ[j] * œÄ·∂ú[k]
		end
	end
	D[t] = sum(f[t])
	f[t] ./= D[t]
	‚Ñì[1] += log(D[t])
	@inbounds for t=2:trial.ntimesteps
		if t ‚àà clicks.inputtimesteps
			clickindex = clicks.inputindex[t][1]
			A·µÉ = A·µÉinput[clickindex]
			‚àáA·µÉ = ‚àáA·µÉinput[clickindex]
			update_for_‚àátransition_probabilities!(P, adaptedclicks, clicks, t)
			‚àátransitionmatrix!(‚àáA·µÉ, A·µÉ, P)
		else
			A·µÉ = A·µÉsilent
		end
		f[t] = pùêòùëë[t] .* (A·µÉ * f[t-1] * A·∂ú·µÄ)
		D[t] = sum(f[t])
		f[t] ./= D[t]
		‚Ñì[1] += log(D[t])
		if choiceLLscaling > 1
			f·∂ú[t] = A·µÉ*f·∂ú[t-1]
		end
	end
	b = ones(Œû,K)
	f‚®Äb = f # reuse memory
	‚àá‚Ñìlatent[indexŒ∏_œà[1]] += expectation_derivative_logpùëë_wrt_œà(trial.choice, f‚®Äb[trial.ntimesteps], Œ∏native.œà[1])
	if choiceLLscaling > 1
		f·∂ú[trial.ntimesteps] .*= pùëë_a
		D·∂ú = sum(f·∂ú[trial.ntimesteps])
		‚Ñì[1] += (choiceLLscaling-1)*log(D·∂ú)
		f·∂ú[trial.ntimesteps] ./= D·∂ú
		b·∂ú = pùëë_a./D·∂ú # backward term for the last time step
		Œ≥ = b·∂ú.*f·∂ú[trial.ntimesteps] # posterior probability for the last time step
		‚àá‚Ñìlatent[indexŒ∏_œà[1]] += (choiceLLscaling-1)*expectation_derivative_logpùëë_wrt_œà(trial.choice, f·∂ú[trial.ntimesteps], Œ∏native.œà[1])
	end
	@inbounds for t = trial.ntimesteps:-1:1
		if t < trial.ntimesteps
			if t+1 ‚àà clicks.inputtimesteps
				clickindex = clicks.inputindex[t+1][1]
				A·µÉ‚Çú‚Çä‚ÇÅ = A·µÉinput[clickindex]
			else
				A·µÉ‚Çú‚Çä‚ÇÅ = A·µÉsilent
			end
			b = transpose(A·µÉ‚Çú‚Çä‚ÇÅ) * (b.*pùêòùëë[t+1]./D[t+1]) * A·∂ú
			f‚®Äb[t] .*= b
			if choiceLLscaling > 1
				b·∂ú = transpose(A·µÉ‚Çú‚Çä‚ÇÅ) * b·∂ú
			end
		end
		if t > 1
			if t ‚àà clicks.inputtimesteps
				clickindex = clicks.inputindex[t][1]
				A·µÉ = A·µÉinput[clickindex]
				‚àáA·µÉ = ‚àáA·µÉinput[clickindex]
			else
				A·µÉ = A·µÉsilent
				‚àáA·µÉ = ‚àáA·µÉsilent
			end
			for i = 1:nŒ∏_pa‚Çúa‚Çú‚Çã‚ÇÅ
				‚àá‚Ñìlatent[indexŒ∏_pa‚Çúa‚Çú‚Çã‚ÇÅ[i]] += sum_product_over_states(D[t], f[t-1], b, pùêòùëë[t], ‚àáA·µÉ[i], A·∂ú)
			end
			for i = 1:nŒ∏_pc‚Çúc‚Çú‚Çã‚ÇÅ
				‚àá‚Ñìlatent[indexŒ∏_pc‚Çúc‚Çú‚Çã‚ÇÅ[i]] += sum_product_over_states(D[t], f[t-1], b, pùêòùëë[t], A·µÉ, ‚àáA·∂ú[i])
			end
			if choiceLLscaling > 1
				for i = 1:nŒ∏_pa‚Çúa‚Çú‚Çã‚ÇÅ
					‚àá‚Ñìlatent[indexŒ∏_pa‚Çúa‚Çú‚Çã‚ÇÅ[i]] += (choiceLLscaling-1)*(transpose(b·∂ú)*‚àáA·µÉ[i]*f·∂ú[t-1])[1]
				end
			end
		end
	end
	t = 1
	@inbounds for i = 1:nŒ∏_pa‚ÇÅ
		‚àá‚Ñìlatent[indexŒ∏_pa‚ÇÅ[i]] += sum_product_over_states(D[t], b, pùêòùëë[t], ‚àápa‚ÇÅ[i], œÄ·∂ú)
	end
	@inbounds for i = 1:nŒ∏_pc‚ÇÅ
		‚àá‚Ñìlatent[indexŒ∏_pc‚ÇÅ[i]] += sum_product_over_states(D[t], b, pùêòùëë[t], pa‚ÇÅ, ‚àáœÄ·∂ú[i])
	end
	if choiceLLscaling > 1
		@inbounds for i = 1:nŒ∏_pa‚ÇÅ
			‚àá‚Ñìlatent[indexŒ∏_pa‚ÇÅ[i]] += (choiceLLscaling-1)*dot(b·∂ú, ‚àápa‚ÇÅ[i])
		end
	end
	offset = 0
	for i = 1:m-1
		offset += model.trialsets[s].trials[i].ntimesteps
	end
	for t = 1:trial.ntimesteps
		œÑ = offset+t
		for i = 1:Œû
			for k = 1:K
				memory.Œ≥[s][i,k][œÑ] = f‚®Äb[t][i,k]
			end
		end
	end
	return nothing
end

"""
	Memoryforgradient(model)

Create variables that are memory by the computations of the log-likelihood and its gradient

ARGUMENT
-`model`: structure with information about the factorial hidden Markov drift-diffusion model

OUTPUT
-an instance of the custom type `Memoryforgradient`, which contains the memory quantities
-`P`: an instance of `Probabilityvector`

EXAMPLE
```julia-repl
julia> using FHMDDM
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_04_27_test/data.mat"; randomize=true);
julia> memory = FHMDDM.Memoryforgradient(model)
```
"""
function Memoryforgradient(model::Model; choicemodel::Bool=false)
	@unpack options, Œ∏native = model
	@unpack Œît, K, minpa, Œû = options
	A·∂ú‚ÇÅ‚ÇÅ = Œ∏native.A·∂ú‚ÇÅ‚ÇÅ[1]
	A·∂ú‚ÇÇ‚ÇÇ = Œ∏native.A·∂ú‚ÇÇ‚ÇÇ[1]
	œÄ·∂ú‚ÇÅ = Œ∏native.œÄ·∂ú‚ÇÅ[1]
	if K == 2
		A·∂ú = [A·∂ú‚ÇÅ‚ÇÅ 1-A·∂ú‚ÇÇ‚ÇÇ; 1-A·∂ú‚ÇÅ‚ÇÅ A·∂ú‚ÇÇ‚ÇÇ]
		‚àáA·∂ú = [[1.0 0.0; -1.0 0.0], [0.0 -1.0; 0.0 1.0]]
		œÄ·∂ú = [œÄ·∂ú‚ÇÅ, 1-œÄ·∂ú‚ÇÅ]
		‚àáœÄ·∂ú = [[1.0, -1.0]]
	else
		A·∂ú = ones(1,1)
		‚àáA·∂ú = [zeros(1,1), zeros(1,1)]
		œÄ·∂ú = ones(1)
		‚àáœÄ·∂ú = [zeros(1)]
	end
	maxclicks = maximum_number_of_clicks(model)
	maxtimesteps = maximum_number_of_time_steps(model)
	if choicemodel
		concatenatedŒ∏, indexŒ∏ = concatenate_choice_related_parameters(model)
		f = collect(zeros(Œû,1) for t=1:maxtimesteps)
	else
		concatenatedŒ∏, indexŒ∏ = concatenateparameters(model)
		f = collect(zeros(Œû,K) for t=1:maxtimesteps)
	end
	indexŒ∏_pa‚ÇÅ = [3,6,11,13]
	indexŒ∏_pa‚Çúa‚Çú‚Çã‚ÇÅ = [3,4,5,7,10,12]
	indexŒ∏_pc‚ÇÅ = [8]
	indexŒ∏_pc‚Çúc‚Çú‚Çã‚ÇÅ = [1,2]
	indexŒ∏_œà = [9]
	nŒ∏_pa‚ÇÅ = length(indexŒ∏_pa‚ÇÅ)
	nŒ∏_pa‚Çúa‚Çú‚Çã‚ÇÅ = length(indexŒ∏_pa‚Çúa‚Çú‚Çã‚ÇÅ)
	‚àá‚Ñìglm = map(model.trialsets) do trialset
				map(trialset.mpGLMs) do mpGLM
					initialize(mpGLM.Œ∏)
				end
			end
	one_minus_Œûminpa = 1.0 - Œû*minpa
	A·µÉinput=map(1:maxclicks) do t
				A = ones(Œû,Œû).*minpa
				A[1,1] += one_minus_Œûminpa
				A[Œû,Œû] += one_minus_Œûminpa
				return A
			end
	A·µÉsilent = copy(A·µÉinput[1])
	‚àáA·µÉinput = collect(collect(zeros(Œû,Œû) for q=1:nŒ∏_pa‚Çúa‚Çú‚Çã‚ÇÅ) for t=1:maxclicks)
	‚àáA·µÉsilent = map(i->zeros(Œû,Œû), 1:nŒ∏_pa‚Çúa‚Çú‚Çã‚ÇÅ)
	pùêòùëë=map(model.trialsets) do trialset
			map(trialset.trials) do trial
				map(1:trial.ntimesteps) do t
					ones(Œû,K)
				end
			end
		end
	pùëë_a=map(model.trialsets) do trialset
			map(trialset.trials) do trial
				ones(Œû)
			end
		end
	Œ≥ =	map(model.trialsets) do trialset
			map(CartesianIndices((Œû,K))) do index
				zeros(trialset.ntimesteps)
			end
		end
	memory = Memoryforgradient(A·µÉinput=A·µÉinput,
								‚àáA·µÉinput=‚àáA·µÉinput,
								A·µÉsilent=A·µÉsilent,
								‚àáA·µÉsilent=‚àáA·µÉsilent,
								A·∂ú=A·∂ú,
								‚àáA·∂ú=‚àáA·∂ú,
								choiceLLscaling = scaling_factor_choiceLL(model),
								concatenatedŒ∏=similar(concatenatedŒ∏),
								D = zeros(maxtimesteps),
								Œît=options.Œît,
								f=f,
								f·∂ú=collect(zeros(Œû) for t=1:maxtimesteps),
								indexŒ∏=indexŒ∏,
								indexŒ∏_pa‚ÇÅ=indexŒ∏_pa‚ÇÅ,
								indexŒ∏_pa‚Çúa‚Çú‚Çã‚ÇÅ=indexŒ∏_pa‚Çúa‚Çú‚Çã‚ÇÅ,
								indexŒ∏_pc‚ÇÅ=indexŒ∏_pc‚ÇÅ,
								indexŒ∏_pc‚Çúc‚Çú‚Çã‚ÇÅ=indexŒ∏_pc‚Çúc‚Çú‚Çã‚ÇÅ,
								indexŒ∏_œà=indexŒ∏_œà,
								Œ≥=Œ≥,
								K=K,
								‚àá‚Ñìglm=‚àá‚Ñìglm,
								‚àá‚Ñìlatent=zeros(13),
								‚àápa‚ÇÅ = collect(zeros(Œû) for q=1:nŒ∏_pa‚ÇÅ),
								œÄ·∂ú=œÄ·∂ú,
								‚àáœÄ·∂ú=‚àáœÄ·∂ú,
								pùëë_a=pùëë_a,
								pùêòùëë=pùêòùëë,
								Œû=Œû)
	return memory
end

"""
	maximum_number_of_clicks(model)

Return the maximum number of clicks across all trials.

The stereoclick is excluded from this analysis as well as all other analyses.
"""
function maximum_number_of_clicks(model::Model)
	maxclicks = 0
	@inbounds for trialset in model.trialsets
		for trial in trialset.trials
			maxclicks = max(maxclicks, length(trial.clicks.time))
		end
	end
	return maxclicks
end

"""
	maximum_number_of_time_steps(model)

Return the maximum number of time steps across all trials
"""
function maximum_number_of_time_steps(model::Model)
	maxtimesteps = 0
	@inbounds for trialset in model.trialsets
		for trial in trialset.trials
			maxtimesteps = max(maxtimesteps, trial.ntimesteps)
		end
	end
	return maxtimesteps
end

"""
	update!(model, memory, concatenatedŒ∏)

Update the model and the memory quantities according to new parameter values

MODIFIED ARGUMENT
-`memory`: structure containing variables memory between computations of the model's log-likelihood and its gradient
-`model`: structure with information concerning a factorial hidden Markov drift-diffusion model

ARGUMENT
-`concatenatedŒ∏`: newest values of the model's parameters

RETURN
-`P`: an instance of `Probabilityvector`

EXAMPLE
```julia-repl
julia> using FHMDDM
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_04_27_test/data.mat"; randomize=true);
julia> memory, P = FHMDDM.Memoryforgradient(model)
julia> P = update!(model, memory, rand(length(memory.concatenatedŒ∏)))
```
"""
function update!(memory::Memoryforgradient,
				 model::Model,
				 concatenatedŒ∏::Vector{<:Real})
	memory.concatenatedŒ∏ .= concatenatedŒ∏
	sortparameters!(model, memory.concatenatedŒ∏, memory.indexŒ∏)
	real2native!(model.Œ∏native, model.options, model.Œ∏real)
	if !isempty(memory.pùêòùëë[1][1][1])
	    scaledlikelihood!(memory.pùêòùëë, memory.pùëë_a, model.trialsets, model.Œ∏native.œà[1])
	end
	@unpack options, Œ∏native = model
	@unpack Œît, K, minpa, Œû = options
	P = Probabilityvector(Œît, minpa, Œ∏native, Œû)
	update_for_‚àátransition_probabilities!(P)
	‚àátransitionmatrix!(memory.‚àáA·µÉsilent, memory.A·µÉsilent, P)
	if K == 2
		A·∂ú‚ÇÅ‚ÇÅ = Œ∏native.A·∂ú‚ÇÅ‚ÇÅ[1]
		A·∂ú‚ÇÇ‚ÇÇ = Œ∏native.A·∂ú‚ÇÇ‚ÇÇ[1]
		œÄ·∂ú‚ÇÅ = Œ∏native.œÄ·∂ú‚ÇÅ[1]
		memory.A·∂ú .= [A·∂ú‚ÇÅ‚ÇÅ 1-A·∂ú‚ÇÇ‚ÇÇ; 1-A·∂ú‚ÇÅ‚ÇÅ A·∂ú‚ÇÇ‚ÇÇ]
		memory.œÄ·∂ú .= [œÄ·∂ú‚ÇÅ, 1-œÄ·∂ú‚ÇÅ]
	end
	return P
end

"""
	scaling_factor_choiceLL(model)

Scaling factor for the log-likelihood of behavioral choices
"""
function scaling_factor_choiceLL(model::Model)
	if model.options.scalechoiceLL
		ntimesteps= sum(collect(trialset.ntimesteps for trialset in model.trialsets))
		ntrials = sum(collect(trialset.ntrials for trialset in model.trialsets))
		ntimesteps/ntrials
	else
		1.0
	end
end
