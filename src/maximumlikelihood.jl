"""
    maximizelikelihood!(model, optimizer)

Optimize the parameters of the factorial hidden Markov drift-diffusion model using a first-order optimizer in Optim

MODIFIED ARGUMENT
- a structure containing information for a factorial hidden Markov drift-diffusion model

UNMODIFIED ARGUMENT
-`optimizer`: an optimizer implemented by Optim.jl. The limited memory quasi-Newton algorithm `LBFGS()` does pretty well, and when using L-BFGS the `HagerZhang()` line search seems to do better than `BackTracking()`

OPTIONAL ARGUMENT
-`extended_trace`: save additional information
-`f_tol`: threshold for determining convergence in the objective value
-`g_tol`: threshold for determining convergence in the gradient
-`iterations`: number of inner iterations that will be run before the optimizer gives up
-`show_every`: trace output is printed every `show_every`th iteration.
-`show_trace`: should a trace of the optimizer's state be shown?
-`x_tol`: threshold for determining convergence in the input vector

RETURN
`losses`: value of the loss function (negative of the model's log-likelihood) across iterations. If `store_trace` were set to false, then these are NaN's
`gradientnorms`: 2-norm of the gradient of  of the loss function across iterations. If `store_trace` were set to false, then these are NaN's

EXAMPLE
```julia-repl
julia> using FHMDDM, LineSearches, Optim
julia> datapath = "/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_05_05_test/data.mat"
julia> model = Model(datapath)
julia> losses, gradientnorms = maximizelikelihood!(model, LBFGS(linesearch = LineSearches.BackTracking()))
```
"""
function maximizelikelihood!(model::Model,
							 optimizer::Optim.FirstOrderOptimizer;
			                 extended_trace::Bool=false,
			                 f_tol::AbstractFloat=0.0,
			                 g_tol::AbstractFloat=1e-8,
			                 iterations::Integer=1000,
			                 show_every::Integer=10,
			                 show_trace::Bool=true,
							 store_trace::Bool=true,
			                 x_tol::AbstractFloat=0.0)
	memory = Memoryforgradient(model)
    f(concatenatedÎ¸) = -loglikelihood!(model, memory, concatenatedÎ¸)
    g!(âˆ‡, concatenatedÎ¸) = âˆ‡negativeloglikelihood!(âˆ‡, memory, model, concatenatedÎ¸)
    Optim_options = Optim.Options(extended_trace=extended_trace,
								  f_tol=f_tol,
                                  g_tol=g_tol,
                                  iterations=iterations,
                                  show_every=show_every,
                                  show_trace=show_trace,
								  store_trace=store_trace,
                                  x_tol=x_tol)
	Î¸â‚€ = concatenateparameters(model)[1]
	optimizationresults = [] # so that the variable is not confined to the scope of the while loop
	ongoing = true
	while ongoing
		optimizationresults = Optim.optimize(f, g!, Î¸â‚€, optimizer, Optim_options)
		ongoing = isnan(Optim.minimum(optimizationresults))
	end
    Î¸â‚˜â‚— = Optim.minimizer(optimizationresults)
	sortparameters!(model, Î¸â‚˜â‚—, memory.indexÎ¸)
	real2native!(model.Î¸native, model.options, model.Î¸real)
	println(optimizationresults)
	losses, gradientnorms = fill(NaN, iterations+1), fill(NaN, iterations+1)
	if store_trace
		traces = Optim.trace(optimizationresults)
		for i in eachindex(traces)
			gradientnorms[i] = traces[i].g_norm
			losses[i] = traces[i].value
		end
	end
    return losses, gradientnorms
end

"""
    maximizelikelihood!(model, optimizer)

Optimize the parameters of the factorial hidden Markov drift-diffusion model using an algorithm implemented by Flux.jl

MODIFIED ARGUMENT
- a structure containing information for a factorial hidden Markov drift-diffusion model

UNMODIFIED ARGUMENT
-`optimizer`: optimization algorithm implemented by Flux.jl

OPTIONAL ARGUMENT
-`iterations`: number of inner iterations that will be run before the optimizer gives up

RETURN
`losses`: a vector of floating-point numbers indicating the value of the loss function (negative of the model's log-likelihood) across iterations. If `store_trace` were set to false, then these are NaN's
`gradientnorms`: a vector of floating-point numbers indicating the 2-norm of the gradient of  of the loss function across iterations. If `store_trace` were set to false, then these are NaN's

EXAMPLE
```julia-repl
julia> using FHMDDM
julia> import Flux
julia> datapath = "/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_04_27_test/data.mat"
julia> model = Model(datapath)
julia> losses, gradientnorms = FHMDDM.maximizelikelihood!(model, Flux.ADAM())
```
"""
function maximizelikelihood!(model::Model,
							optimizer::Flux.Optimise.AbstractOptimiser;
							iterations::Integer = 3000)
	memory = Memoryforgradient(model)
	Î¸ = concatenateparameters(model)[1]
	âˆ‡ = similar(Î¸)
	local x, min_err, min_Î¸
	min_err = typemax(eltype(Î¸)) #dummy variables
	min_Î¸ = copy(Î¸)
	losses, gradientnorms = fill(NaN, iterations+1), fill(NaN, iterations+1)
	optimizationtime = 0.0
	for i = 1:iterations
		iterationtime = @timed begin
			x = -loglikelihood!(model, memory, Î¸)
			âˆ‡negativeloglikelihood!(âˆ‡, memory, model, Î¸)
			losses[i] = x
			gradientnorms[i] = norm(âˆ‡)
			if x < min_err  # found a better solution
				min_err = x
				min_Î¸ = copy(Î¸)
			end
			Flux.update!(optimizer, Î¸, âˆ‡)
		end
		optimizationtime += iterationtime[2]
		println("iteration=", i, ", loss= ", losses[i], ", gradient norm= ", gradientnorms[i], ", time(s)= ", optimizationtime)
	end
	sortparameters!(model, min_Î¸, memory.indexÎ¸)
	real2native!(model.Î¸native, model.options, model.Î¸real)
    return losses, gradientnorms
end

"""
    loglikelihood!(model, memory, concatenatedÎ¸)

Compute the log-likelihood

MODIFIED ARGUMENT
-`model`: an instance of FHM-DDM
-`memory`: a container of variables used by both the log-likelihood and gradient computation

UNMODIFIED ARGUMENT
-`concatenatedÎ¸`: a vector of concatenated parameter values

RETURN
-log-likelihood

EXAMPLE
```julia-repl
julia> using FHMDDM
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_04_27_test/data.mat"; randomize=true);
julia> concatenatedÎ¸, indexÎ¸ = FHMDDM.concatenateparameters(model)
julia> memory = FHMDDM.Memoryforgradient(model)
julia> â„“ = loglikelihood!(model, memory, memory.concatenatedÎ¸)
julia> â„“ = loglikelihood!(model, memory, rand(length(memory.concatenatedÎ¸)))
```
"""
function loglikelihood!(model::Model,
						memory::Memoryforgradient,
					    concatenatedÎ¸::Vector{<:Real})
	if concatenatedÎ¸ != memory.concatenatedÎ¸
		P = update!(memory, model, concatenatedÎ¸)
		memory.â„“[1] = 0.0
		@inbounds for s in eachindex(model.trialsets)
			for m in eachindex(model.trialsets[s].trials)
				memory.â„“[1] += loglikelihood(memory.pğ˜ğ‘‘[s][m], memory, P, model.Î¸native, model.trialsets[s].trials[m])
			end
		end
	end
	memory.â„“[1]
end

"""
	loglikelihood(pğ˜ğ‘‘, Î¸native, trial)

Compute the log-likelihood of the data from one trial

ARGUMENT
-`pğ˜ğ‘‘`: a matrix whose element `pğ˜ğ‘‘[t][i,j]` represents the conditional likelihood `p(ğ˜â‚œ, d âˆ£ ğšâ‚œ=i, ğœâ‚œ=j)`
-`Î¸native`: model parameters in their native space
-`trial`: stimulus and behavioral information of one trial

RETURN
-`â„“`: log-likelihood of the data from one trial
"""
function loglikelihood(pğ˜ğ‘‘::Vector{<:Matrix{<:Real}},
   					   memory::Memoryforgradient,
					   P::Probabilityvector,
					   Î¸native::LatentÎ¸,
					   trial::Trial)
	@unpack clicks = trial
	@unpack Aáµƒinput, Aáµƒsilent, Aá¶œáµ€, Ï€á¶œáµ€ = memory
    if length(clicks.time) > 0
		adaptedclicks = adapt(clicks, Î¸native.k[1], Î¸native.Ï•[1])
	end
	priorprobability!(P, trial.previousanswer)
	paâ‚ = P.ğ›‘
	f = pğ˜ğ‘‘[1] .* paâ‚ .* Ï€á¶œáµ€
	D = sum(f)
	f ./= D
	â„“ = log(D)
	@inbounds for t = 2:trial.ntimesteps
		if isempty(clicks.inputindex[t])
			Aáµƒ = Aáµƒsilent
		else
			Aáµƒ = Aáµƒinput[clicks.inputindex[t][1]]
			update_for_transition_probabilities!(P, adaptedclicks, clicks, t)
			transitionmatrix!(Aáµƒ, P)
		end
		f = pğ˜ğ‘‘[t].*(Aáµƒ * f * Aá¶œáµ€)
		D = sum(f)
		f ./= D
		â„“ += log(D)
	end
	return â„“
end

"""
    loglikelihood(concatenatedÎ¸, indexÎ¸, model)

ForwardDiff-compatible computation of the log-likelihood

UNMODIFIED ARGUMENT
-`concatenatedÎ¸`: a vector of concatenated parameter values
-`indexÎ¸`: struct indexing of each parameter in the vector of concatenated values
-`model`: an instance of FHM-DDM

RETURN
-log-likelihood

EXAMPLE
```julia-repl
julia> using FHMDDM
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_05_05_test/data.mat"; randomize=true);
julia> concatenatedÎ¸, indexÎ¸ = FHMDDM.concatenateparameters(model)
julia> â„“ = loglikelihood(concatenatedÎ¸, indexÎ¸, model)
julia> using ForwardDiff
julia> f(x) = loglikelihood(x, indexÎ¸, model)
julia> g = ForwardDiff.gradient(f, concatenatedÎ¸)
julia> memory = FHMDDM.Memoryforgradient(model)
julia> â„“2 = loglikelihood!(model, memory, concatenatedÎ¸) #ForwardDiff-incompatible
julia> abs(â„“2-â„“)
```
"""
function loglikelihood(	concatenatedÎ¸::Vector{T},
					    indexÎ¸::IndexÎ¸,
						model::Model) where {T<:Real}
	model = Model(concatenatedÎ¸, indexÎ¸, model)
	@unpack options, Î¸native, Î¸real, trialsets = model
	@unpack Î”t, K, Î = options
	pğ˜ğ‘‘=map(model.trialsets) do trialset
			map(trialset.trials) do trial
				map(1:trial.ntimesteps) do t
					ones(T,Î,K)
				end
			end
		end
    likelihood!(pğ˜ğ‘‘, trialsets, Î¸native.Ïˆ[1])
	Aáµƒinput, Aáµƒsilent = zeros(T,Î,Î), zeros(T,Î,Î)
	expÎ»Î”t = exp(Î¸native.Î»[1]*Î”t)
	dÎ¼_dÎ”c = differentiate_Î¼_wrt_Î”c(Î”t, Î¸native.Î»[1])
	dğ›_dB = (2 .*collect(1:Î) .- Î .- 1)./(Î-2)
	ğ› = Î¸native.B[1].*dğ›_dB
	transitionmatrix!(Aáµƒsilent, expÎ»Î”t.*ğ›, âˆš(Î”t*Î¸native.ÏƒÂ²â‚[1]), ğ›)
	Aá¶œâ‚â‚ = Î¸native.Aá¶œâ‚â‚[1]
	Aá¶œâ‚‚â‚‚ = Î¸native.Aá¶œâ‚‚â‚‚[1]
	Ï€á¶œâ‚ = Î¸native.Ï€á¶œâ‚[1]
	if K == 2
		Aá¶œáµ€ = [Aá¶œâ‚â‚ 1-Aá¶œâ‚â‚; 1-Aá¶œâ‚‚â‚‚ Aá¶œâ‚‚â‚‚]
		Ï€á¶œáµ€ = [Ï€á¶œâ‚ 1-Ï€á¶œâ‚]
	else
		Aá¶œáµ€ = ones(T,1,1)
		Ï€á¶œáµ€ = ones(T,1,1)
	end
	â„“ = zero(T)
	@inbounds for s in eachindex(trialsets)
		for m in eachindex(trialsets[s].trials)
			trial = trialsets[s].trials[m]
			paâ‚ = probabilityvector(Î¸native.Î¼â‚€[1]+Î¸native.wâ‚•[1]*trial.previousanswer, âˆšÎ¸native.ÏƒÂ²áµ¢[1], ğ›)
			f = pğ˜ğ‘‘[s][m][1] .* paâ‚ .* Ï€á¶œáµ€
			D = sum(f)
			f./=D
			â„“+=log(D)
			if length(trial.clicks.time) > 0
				adaptedclicks = adapt(trial.clicks, Î¸native.k[1], Î¸native.Ï•[1])
			end
			for t=2:trial.ntimesteps
				if t âˆˆ trial.clicks.inputtimesteps
					cL = sum(adaptedclicks.C[trial.clicks.left[t]])
					cR = sum(adaptedclicks.C[trial.clicks.right[t]])
					ğ› = expÎ»Î”t.*ğ› .+ (cR-cL).*dÎ¼_dÎ”c
					Ïƒ = âˆš((cR+cL)*Î¸native.ÏƒÂ²â‚›[1] + Î”t*Î¸native.ÏƒÂ²â‚[1])
					transitionmatrix!(Aáµƒinput, ğ›, Ïƒ, ğ›)
					Aáµƒ = Aáµƒinput
				else
					Aáµƒ = Aáµƒsilent
				end
				f = pğ˜ğ‘‘[s][m][t] .* (Aáµƒ * f * Aá¶œáµ€)
				D = sum(f)
				f./=D
				â„“+=log(D)
			end
		end
	end
	â„“
end

"""
	âˆ‡negativeloglikelihood!(âˆ‡nâ„“, memory, model, concatenatedÎ¸)

Update the gradient of the negative log-likelihood of the model

MODIFIED ARGUMENT
-`âˆ‡nâ„“`: the vector representing the gradient
-`memory`: memory allocated for computing the gradient. The log-likelihood is updated.
-`model`: structure containing the data, parameters, and hyperparameters of the model

ARGUMENT
-`concatenatedÎ¸`: values of the model's parameters concatenated into a vector

EXAMPLE
```julia-repl
julia> using FHMDDM
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_05_19_test/T176_2018_05_03/data.mat");
julia> concatenatedÎ¸, indexÎ¸ = FHMDDM.concatenateparameters(model)
julia> âˆ‡nâ„“ = similar(concatenatedÎ¸)
julia> memory = FHMDDM.Memoryforgradient(model)
julia> FHMDDM.âˆ‡negativeloglikelihood!(âˆ‡nâ„“, memory, model, concatenatedÎ¸)
julia> using ForwardDiff
julia> f(x) = -FHMDDM.loglikelihood(x, indexÎ¸, model)
julia> â„“_auto = f(concatenatedÎ¸)
julia> âˆ‡nâ„“_auto = ForwardDiff.gradient(f, concatenatedÎ¸)
julia> abs(â„“_auto + memory.â„“[1])
julia> maximum(abs.(âˆ‡nâ„“_auto .- âˆ‡nâ„“))
```
"""
function âˆ‡negativeloglikelihood!(âˆ‡nâ„“::Vector{<:Real},
 								 memory::Memoryforgradient,
								 model::Model,
								 concatenatedÎ¸::Vector{<:AbstractFloat})
	if concatenatedÎ¸ != memory.concatenatedÎ¸
		P = update!(memory, model, concatenatedÎ¸)
	else
		P = Probabilityvector(model.options.Î”t, model.Î¸native, model.options.Î)
	end
	âˆ‡loglikelihood!(memory,model,P)
	indexall = 0
	indexfit = 0
	for field in fieldnames(LatentÎ¸)
		indexall+=1
		if getfield(memory.indexÎ¸.latentÎ¸, field)[1] > 0
			indexfit +=1
			âˆ‡nâ„“[indexfit] = -memory.âˆ‡â„“latent[indexall]
		end
	end
	native2real!(âˆ‡nâ„“, memory.indexÎ¸.latentÎ¸, model)
	for âˆ‡â„“glms in memory.âˆ‡â„“glm
		for âˆ‡â„“glm in âˆ‡â„“glms
			for u in âˆ‡â„“glm.ğ®
				indexfit+=1
				âˆ‡nâ„“[indexfit] = -u
			end
			for ğ¯â‚– in âˆ‡â„“glm.ğ¯
				for v in ğ¯â‚–
					indexfit+=1
					âˆ‡nâ„“[indexfit] = -v
				end
			end
		end
	end
	return nothing
end

"""
	âˆ‡loglikelihood!(memory, model, P)

MODIFIED ARGUMENT
-`memory`: memory allocated for computing the gradient. The log-likelihood is updated.
-`model`: structure containing the data, parameters, and hyperparameters of the model
-`P`: a structure containing allocated memory for computing the accumulator's initial and transition probabilities as well as the partial derivatives of these probabilities
"""
function âˆ‡loglikelihood!(memory::Memoryforgradient,
						 model::Model,
						 P::Probabilityvector)
	memory.â„“ .= 0.0
	memory.âˆ‡â„“latent .= 0.0
	@inbounds for s in eachindex(model.trialsets)
		for m in eachindex(model.trialsets[s].trials)
			âˆ‡loglikelihood!(memory, model, P, s, m)
		end
	end
	@inbounds for s in eachindex(model.trialsets)
		for n in eachindex(model.trialsets[s].mpGLMs)
			expectation_âˆ‡loglikelihood!(memory.âˆ‡â„“glm[s][n], memory.Î³[s], model.trialsets[s].mpGLMs[n])
		end
	end
	return nothing
end

"""
	âˆ‡loglikelihood!(memory, model, P, s, m)

Update the gradient

MODIFIED ARGUMENT
-`memory`: memory allocated for computing the gradient. The log-likelihood is updated.
-`model`: structure containing the data, parameters, and hyperparameters of the model
-`P`: a structure containing allocated memory for computing the accumulator's initial and transition probabilities as well as the partial derivatives of these probabilities
-`s`: index of the trialset
-`m`: index of the trial
"""
function âˆ‡loglikelihood!(memory::Memoryforgradient,
						 model::Model,
						 P::Probabilityvector,
						 s::Integer,
						 m::Integer)
	trial = model.trialsets[s].trials[m]
	pğ˜ğ‘‘ = memory.pğ˜ğ‘‘[s][m]
	@unpack Î¸native = model
	@unpack clicks = trial
	@unpack inputtimesteps, inputindex = clicks
	@unpack Aáµƒinput, âˆ‡Aáµƒinput, Aáµƒsilent, âˆ‡Aáµƒsilent, Aá¶œ, Aá¶œáµ€, âˆ‡Aá¶œ, D, f, indexÎ¸_paâ‚, indexÎ¸_paâ‚œaâ‚œâ‚‹â‚, indexÎ¸_pcâ‚, indexÎ¸_pcâ‚œcâ‚œâ‚‹â‚, indexÎ¸_Ïˆ, K, â„“, âˆ‡â„“latent, nÎ¸_paâ‚, nÎ¸_paâ‚œaâ‚œâ‚‹â‚, nÎ¸_pcâ‚, nÎ¸_pcâ‚œcâ‚œâ‚‹â‚, âˆ‡paâ‚, Ï€á¶œ, âˆ‡Ï€á¶œ, Î = memory
	t = 1
	âˆ‡priorprobability!(âˆ‡paâ‚, P, trial.previousanswer)
	paâ‚ = copy(P.ğ›‘) # save for later
	@inbounds for j=1:Î
		for k = 1:K
			f[t][j,k] = pğ˜ğ‘‘[t][j,k] * paâ‚[j] * Ï€á¶œ[k]
		end
	end
	D[t] = sum(f[t])
	f[t] ./= D[t]
	â„“[1] += log(D[t])
	if length(clicks.time) > 0
		adaptedclicks = âˆ‡adapt(trial.clicks, Î¸native.k[1], Î¸native.Ï•[1])
	end
	@inbounds for t=2:trial.ntimesteps
		if t âˆˆ clicks.inputtimesteps
			clickindex = clicks.inputindex[t][1]
			Aáµƒ = Aáµƒinput[clickindex]
			âˆ‡Aáµƒ = âˆ‡Aáµƒinput[clickindex]
			update_for_âˆ‡transition_probabilities!(P, adaptedclicks, clicks, t)
			âˆ‡transitionmatrix!(âˆ‡Aáµƒ, Aáµƒ, P)
		else
			Aáµƒ = Aáµƒsilent
		end
		f[t] = pğ˜ğ‘‘[t] .* (Aáµƒ * f[t-1] * Aá¶œáµ€)
		D[t] = sum(f[t])
		f[t] ./= D[t]
		â„“[1] += log(D[t])
	end
	b = ones(Î,K)
	fâ¨€b = f # reuse memory
	âˆ‡â„“latent[indexÎ¸_Ïˆ[1]] += expectation_derivative_logpğ‘‘_wrt_Ïˆ(trial.choice, fâ¨€b[trial.ntimesteps], Î¸native.Ïˆ[1])
	@inbounds for t = trial.ntimesteps:-1:1
		if t < trial.ntimesteps
			if t+1 âˆˆ clicks.inputtimesteps
				clickindex = clicks.inputindex[t+1][1]
				Aáµƒâ‚œâ‚Šâ‚ = Aáµƒinput[clickindex]
			else
				Aáµƒâ‚œâ‚Šâ‚ = Aáµƒsilent
			end
			b = transpose(Aáµƒâ‚œâ‚Šâ‚) * (b.*pğ˜ğ‘‘[t+1]./D[t+1]) * Aá¶œ
			fâ¨€b[t] .*= b
		end
		if t > 1
			if t âˆˆ clicks.inputtimesteps
				clickindex = clicks.inputindex[t][1]
				Aáµƒ = Aáµƒinput[clickindex]
				âˆ‡Aáµƒ = âˆ‡Aáµƒinput[clickindex]
			else
				Aáµƒ = Aáµƒsilent
				âˆ‡Aáµƒ = âˆ‡Aáµƒsilent
			end
			for i = 1:nÎ¸_paâ‚œaâ‚œâ‚‹â‚
				âˆ‡â„“latent[indexÎ¸_paâ‚œaâ‚œâ‚‹â‚[i]] += sum_product_over_states(D[t], f[t-1], b, pğ˜ğ‘‘[t], âˆ‡Aáµƒ[i], Aá¶œ)
			end
			for i = 1:nÎ¸_pcâ‚œcâ‚œâ‚‹â‚
				âˆ‡â„“latent[indexÎ¸_pcâ‚œcâ‚œâ‚‹â‚[i]] += sum_product_over_states(D[t], f[t-1], b, pğ˜ğ‘‘[t], Aáµƒ, âˆ‡Aá¶œ[i])
			end
		end
	end
	t = 1
	@inbounds for i = 1:nÎ¸_paâ‚
		âˆ‡â„“latent[indexÎ¸_paâ‚[i]] += sum_product_over_states(D[t], b, pğ˜ğ‘‘[t], âˆ‡paâ‚[i], Ï€á¶œ)
	end
	@inbounds for i = 1:nÎ¸_pcâ‚
		âˆ‡â„“latent[indexÎ¸_pcâ‚[i]] += sum_product_over_states(D[t], b, pğ˜ğ‘‘[t], paâ‚, âˆ‡Ï€á¶œ[i])
	end
	offset = 0
	for i = 1:m-1
		offset += model.trialsets[s].trials[i].ntimesteps
	end
	for t = 1:trial.ntimesteps
		Ï„ = offset+t
		for i = 1:Î
			for k = 1:K
				memory.Î³[s][i,k][Ï„] = fâ¨€b[t][i,k]
			end
		end
	end
	return nothing
end

"""
	Memoryforgradient(model)

Create variables that are memory by the computations of the log-likelihood and its gradient

ARGUMENT
-`model`: structure with information about the factorial hidden Markov drift-diffusion model

OUTPUT
-an instance of the custom type `Memoryforgradient`, which contains the memory quantities
-`P`: an instance of `Probabilityvector`

EXAMPLE
```julia-repl
julia> using FHMDDM
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_04_27_test/data.mat"; randomize=true);
julia> memory = FHMDDM.Memoryforgradient(model)
```
"""
function Memoryforgradient(model::Model; choicemodel::Bool=false)
	@unpack options, Î¸native = model
	@unpack Î”t, K, Î = options
	Aá¶œâ‚â‚ = Î¸native.Aá¶œâ‚â‚[1]
	Aá¶œâ‚‚â‚‚ = Î¸native.Aá¶œâ‚‚â‚‚[1]
	Ï€á¶œâ‚ = Î¸native.Ï€á¶œâ‚[1]
	if K == 2
		Aá¶œ = [Aá¶œâ‚â‚ 1-Aá¶œâ‚‚â‚‚; 1-Aá¶œâ‚â‚ Aá¶œâ‚‚â‚‚]
		âˆ‡Aá¶œ = [[1.0 0.0; -1.0 0.0], [0.0 -1.0; 0.0 1.0]]
		Ï€á¶œ = [Ï€á¶œâ‚, 1-Ï€á¶œâ‚]
		âˆ‡Ï€á¶œ = [[1.0, -1.0]]
	else
		Aá¶œ = ones(1,1)
		âˆ‡Aá¶œ = [zeros(1,1), zeros(1,1)]
		Ï€á¶œ = ones(1)
		âˆ‡Ï€á¶œ = [zeros(1)]
	end
	maxclicks = maximum_number_of_clicks(model)
	maxtimesteps = maximum_number_of_time_steps(model)
	if choicemodel
		concatenatedÎ¸, indexÎ¸ = concatenate_choice_related_parameters(model)
		f = collect(zeros(Î,1) for t=1:maxtimesteps)
	else
		concatenatedÎ¸, indexÎ¸ = concatenateparameters(model)
		f = collect(zeros(Î,K) for t=1:maxtimesteps)
	end
	indexÎ¸_paâ‚ = [3,6,11,13]
	indexÎ¸_paâ‚œaâ‚œâ‚‹â‚ = [3,4,5,7,10,12]
	indexÎ¸_pcâ‚ = [8]
	indexÎ¸_pcâ‚œcâ‚œâ‚‹â‚ = [1,2]
	indexÎ¸_Ïˆ = [9]
	nÎ¸_paâ‚ = length(indexÎ¸_paâ‚)
	nÎ¸_paâ‚œaâ‚œâ‚‹â‚ = length(indexÎ¸_paâ‚œaâ‚œâ‚‹â‚)
	âˆ‡â„“glm = map(model.trialsets) do trialset
				map(trialset.mpGLMs) do mpGLM
					GLMÎ¸(mpGLM.Î¸, eltype(mpGLM.Î¸.ğ®))
				end
			end
	Aáµƒinput=map(1:maxclicks) do t
				A = zeros(Î,Î)
				A[1,1] = A[Î,Î] = 1.0
				return A
			end
	âˆ‡Aáµƒinput = collect(collect(zeros(Î,Î) for q=1:nÎ¸_paâ‚œaâ‚œâ‚‹â‚) for t=1:maxclicks)
	Aáµƒsilent = zeros(Î, Î)
	âˆ‡Aáµƒsilent = map(i->zeros(Î,Î), 1:nÎ¸_paâ‚œaâ‚œâ‚‹â‚)
	Aáµƒsilent[1,1] = Aáµƒsilent[Î, Î] = 1.0
	pğ˜ğ‘‘=map(model.trialsets) do trialset
			map(trialset.trials) do trial
				map(1:trial.ntimesteps) do t
					ones(Î,K)
				end
			end
		end
	Î³ =	map(model.trialsets) do trialset
			map(CartesianIndices((Î,K))) do index
				zeros(trialset.ntimesteps)
			end
		end
	memory = Memoryforgradient(Aáµƒinput=Aáµƒinput,
								âˆ‡Aáµƒinput=âˆ‡Aáµƒinput,
								Aáµƒsilent=Aáµƒsilent,
								âˆ‡Aáµƒsilent=âˆ‡Aáµƒsilent,
								Aá¶œ=Aá¶œ,
								âˆ‡Aá¶œ=âˆ‡Aá¶œ,
								concatenatedÎ¸=similar(concatenatedÎ¸),
								D = zeros(maxtimesteps),
								Î”t=options.Î”t,
								f=f,
								indexÎ¸=indexÎ¸,
								indexÎ¸_paâ‚=indexÎ¸_paâ‚,
								indexÎ¸_paâ‚œaâ‚œâ‚‹â‚=indexÎ¸_paâ‚œaâ‚œâ‚‹â‚,
								indexÎ¸_pcâ‚=indexÎ¸_pcâ‚,
								indexÎ¸_pcâ‚œcâ‚œâ‚‹â‚=indexÎ¸_pcâ‚œcâ‚œâ‚‹â‚,
								indexÎ¸_Ïˆ=indexÎ¸_Ïˆ,
								Î³=Î³,
								K=K,
								âˆ‡â„“glm=âˆ‡â„“glm,
								âˆ‡â„“latent=zeros(13),
								âˆ‡paâ‚ = collect(zeros(Î) for q=1:nÎ¸_paâ‚),
								Ï€á¶œ=Ï€á¶œ,
								âˆ‡Ï€á¶œ=âˆ‡Ï€á¶œ,
								pğ˜ğ‘‘=pğ˜ğ‘‘,
								Î=Î)
	return memory
end

"""
	maximum_number_of_clicks(model)

Return the maximum number of clicks across all trials.

The stereoclick is excluded from this analysis as well as all other analyses.
"""
function maximum_number_of_clicks(model::Model)
	maxclicks = 0
	@inbounds for trialset in model.trialsets
		for trial in trialset.trials
			maxclicks = max(maxclicks, length(trial.clicks.time))
		end
	end
	return maxclicks
end

"""
	maximum_number_of_time_steps(model)

Return the maximum number of time steps across all trials
"""
function maximum_number_of_time_steps(model::Model)
	maxtimesteps = 0
	@inbounds for trialset in model.trialsets
		for trial in trialset.trials
			maxtimesteps = max(maxtimesteps, trial.ntimesteps)
		end
	end
	return maxtimesteps
end

"""
	update!(model, memory, concatenatedÎ¸)

Update the model and the memory quantities according to new parameter values

MODIFIED ARGUMENT
-`memory`: structure containing variables memory between computations of the model's log-likelihood and its gradient
-`model`: structure with information concerning a factorial hidden Markov drift-diffusion model

ARGUMENT
-`concatenatedÎ¸`: newest values of the model's parameters

RETURN
-`P`: an instance of `Probabilityvector`

EXAMPLE
```julia-repl
julia> using FHMDDM
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_04_27_test/data.mat"; randomize=true);
julia> memory, P = FHMDDM.Memoryforgradient(model)
julia> P = update!(model, memory, rand(length(memory.concatenatedÎ¸)))
```
"""
function update!(memory::Memoryforgradient,
				 model::Model,
				 concatenatedÎ¸::Vector{<:Real})
	memory.concatenatedÎ¸ .= concatenatedÎ¸
	sortparameters!(model, memory.concatenatedÎ¸, memory.indexÎ¸)
	real2native!(model.Î¸native, model.options, model.Î¸real)
	if !isempty(memory.pğ˜ğ‘‘[1][1][1])
	    likelihood!(memory.pğ˜ğ‘‘, model.trialsets, model.Î¸native.Ïˆ[1])
	end
	@unpack options, Î¸native = model
	@unpack Î”t, K, Î = options
	P = Probabilityvector(Î”t, Î¸native, Î)
	update_for_âˆ‡transition_probabilities!(P)
	âˆ‡transitionmatrix!(memory.âˆ‡Aáµƒsilent, memory.Aáµƒsilent, P)
	if K == 2
		Aá¶œâ‚â‚ = Î¸native.Aá¶œâ‚â‚[1]
		Aá¶œâ‚‚â‚‚ = Î¸native.Aá¶œâ‚‚â‚‚[1]
		Ï€á¶œâ‚ = Î¸native.Ï€á¶œâ‚[1]
		memory.Aá¶œ .= [Aá¶œâ‚â‚ 1-Aá¶œâ‚‚â‚‚; 1-Aá¶œâ‚â‚ Aá¶œâ‚‚â‚‚]
		memory.Ï€á¶œ .= [Ï€á¶œâ‚, 1-Ï€á¶œâ‚]
	end
	return P
end
