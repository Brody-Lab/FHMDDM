
"""
	learn_state_independent_filters!(model)

Learn the filters of the state-independent inputs of each neuron's GLM

EXAMPLE
```julia-repl
julia> using FHMDDM, Random
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_05_05_test/data.mat"; randomize=true);
julia> FHMDDM.learn_state_independent_filters!(model)
```
"""
function learn_state_independent_filters!(model::Model)
	q = length(model.trialsets[1].mpGLMs[1].Î¸.ğ®)
	Opt = PoissonGLMOptimization(ğ® = fill(NaN, q))
	for trialset in model.trialsets
		for mpGLM in trialset.mpGLMs
			learn_state_independent_filters!(mpGLM, Opt)
		end
	end
	return nothing
end

"""
    learn_state_independent_filters!(mpGLM, Opt)

Learn the filters of the state-independent inputs

ARGUMENT
-`mpGLM`: the Poisson mixture GLM of one neuron

OPTIONAL ARGUMENT
-`show_trace`: whether to show information about each step of the optimization

RETURN
-weights concatenated into a single vector

EXAMPLE
```julia-repl
julia> using FHMDDM, Random
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_05_05_test/data.mat"; randomize=true);
julia> mpGLM = model.trialsets[1].mpGLMs[1]
julia> Opt = FHMDDM.PoissonGLMOptimization(ğ® = fill(NaN, length(mpGLM.Î¸.ğ®)))
julia> FHMDDM.estimatefilters!(mpGLM, Opt)
```
"""
function learn_state_independent_filters!(mpGLM::MixturePoissonGLM,
										Opt::PoissonGLMOptimization,
										iterations::Integer=20,
										show_trace::Bool=false)
    f(ğ®) = -loglikelihood!(mpGLM,Opt,ğ®)
	g!(âˆ‡, ğ®) = âˆ‡negloglikelihood!(âˆ‡,mpGLM,Opt,ğ®)
	h!(âˆ‡âˆ‡, ğ®) = âˆ‡âˆ‡negloglikelihood!(âˆ‡âˆ‡,mpGLM,Opt,ğ®)
    results = Optim.optimize(f, g!, h!, copy(mpGLM.Î¸.ğ®), NewtonTrustRegion(), Optim.Options(show_trace=show_trace, iterations=iterations))
	mpGLM.Î¸.ğ® .= Optim.minimizer(results)
	return nothing
end

"""
	loglikelihood!(mpGLM, Opt, concatenatedÎ¸)

Compute the log-likelihood of Poisson GLM

ARGUMENT
-`mpGLM`: a structure containing the parameters, input, and observations a mixture of Poisson GLM
-`Opt`: a structure for maximizing the expectation of the log-likelihood of a mixture of Poisson GLM
-`concatenatedÎ¸`: parameters of a GLM concatenated into a vector

RETURN
-log-likelihood of a Poisson GLM
"""
function loglikelihood!(mpGLM::MixturePoissonGLM,
						Opt::PoissonGLMOptimization,
						ğ®::Vector{<:AbstractFloat})
	update!(mpGLM, Opt, ğ®)
	return Opt.â„“[1]
end

"""
	âˆ‡negloglikelihood!(g, mpGLM, Opt, Î³, concatenatedÎ¸)

Compute the gradient of the negative of the log-likelihood of a Poisson GLM

MODIFIED ARGUMENT
-`g`: gradient

ARGUMENT
-`mpGLM`: a structure containing the parameters, input, and observations a mixture of Poisson GLM
-`Opt`: a structure for maximizing the expectation of the log-likelihood of a mixture of Poisson GLM
-`concatenatedÎ¸`: parameters of a GLM concatenated into a vector
"""
function âˆ‡negloglikelihood!(g::Vector{<:AbstractFloat},
							mpGLM::MixturePoissonGLM,
							Opt::PoissonGLMOptimization,
							ğ®::Vector{<:AbstractFloat})
	update!(mpGLM, Opt, ğ®)
	for i in eachindex(g)
		g[i] = -Opt.âˆ‡â„“[i]
	end
	return nothing
end

"""
	âˆ‡âˆ‡negloglikelihood!(h, mpGLM, Opt, Î³, concatenatedÎ¸)

Compute the hessian of the negative of the log-likelihood of a Poisson GLM

MODIFIED ARGUMENT
-`h`: hessian

ARGUMENT
-`mpGLM`: a structure containing the parameters, input, and observations a mixture of Poisson GLM
-`Opt`: a structure for maximizing the expectation of the log-likelihood of a mixture of Poisson GLM
-`concatenatedÎ¸`: parameters of a GLM concatenated into a vector
"""
function âˆ‡âˆ‡negloglikelihood!(h::Matrix{<:AbstractFloat},
							mpGLM::MixturePoissonGLM,
							Opt::PoissonGLMOptimization,
							ğ®::Vector{<:AbstractFloat})
	update!(mpGLM, Opt, ğ®)
	for i in eachindex(h)
		h[i] = -Opt.âˆ‡âˆ‡â„“[i]
	end
	return nothing
end

"""
	update!(mpGLM, Opt, concatenatedÎ¸)

Update quantities for computing the log-likelihood of a Poisson GLM and its gradient and hessian

MODIFIED ARGUMENT
-`mpGLM`: a structure containing the parameters, input, and observations a mixture of Poisson GLM
-`Opt`: a structure for maximizing the expectation of the log-likelihood of a mixture of Poisson GLM

ARGUMENT
-`concatenatedÎ¸`: parameters of a GLM concatenated into a vector

EXAMPLE
```julia-rep
julia> using FHMDDM, Random
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_05_05_test/data.mat"; randomize=true);
julia> mpGLM = model.trialsets[1].mpGLMs[1]
julia> Opt = FHMDDM.PoissonGLMOptimization(ğ® = fill(NaN, length(mpGLM.Î¸.ğ®)))
julia> randğ® = copy(mpGLM.Î¸.ğ®)
julia> FHMDDM.update!(mpGLM, Opt, randğ®)
julia> using ForwardDiff
julia> f(ğ®) = FHMDDM.loglikelihood(mpGLM, ğ®)
julia> fauto = f(randğ®)
julia> gauto = ForwardDiff.gradient(f, randğ®)
julia> hauto = ForwardDiff.hessian(f, randğ®)
julia> abs(fauto - Opt.â„“[1])
julia> maximum(abs.(gauto .- Opt.âˆ‡â„“))
julia> maximum(abs.(hauto .- Opt.âˆ‡âˆ‡â„“))
```
"""
function update!(mpGLM::MixturePoissonGLM,
				Opt::PoissonGLMOptimization,
				ğ®::Vector{<:AbstractFloat})
	if ğ® != Opt.ğ®
		Opt.ğ® .= ğ®
		mpGLM.Î¸.ğ® .= ğ®
		âˆ‡âˆ‡loglikelihood!(Opt, mpGLM)
	end
end

"""
    âˆ‡âˆ‡loglikelihood!(Opt, mpGLM)

Compute the log-likelihood of a Poisson mixture GLM and its first and second derivatives

MODIFIED ARGUMENT
-`Opt`: a structure for maximizing the expectation of the log-likelihood of a mixture of Poisson GLM

UNMODIFIED ARGUMENT
-`mpGLM`: the Poisson mixture GLM of one neuron
```
"""
function âˆ‡âˆ‡loglikelihood!(Opt::PoissonGLMOptimization, mpGLM::MixturePoissonGLM)
	@unpack â„“, âˆ‡â„“, âˆ‡âˆ‡â„“ = Opt
    @unpack Î”t, ğ—, ğ² = mpGLM
	@unpack ğ® = mpGLM.Î¸
	ğ” = @view ğ—[:,1:length(ğ®)]
	ğ”áµ€ = transpose(ğ”)
	ğ‹ = ğ”*ğ®
	T = length(ğ²)
	dÂ²â„“_dLÂ², dâ„“_dL = zeros(T), zeros(T)
	â„“[1] = 0.0
	for t = 1:T
		dÂ²â„“_dLÂ²[t], dâ„“_dL[t], â„“â‚œ = differentiate_loglikelihood_twice_wrt_linearpredictor(Î”t, ğ‹[t], ğ²[t])
		â„“[1] += â„“â‚œ
	end
	âˆ‡â„“ .= ğ”áµ€*dâ„“_dL
	âˆ‡âˆ‡â„“ .= ğ”áµ€*(dÂ²â„“_dLÂ².*ğ”)
	return nothing
end

"""
	loglikelihood(mpGLM, concatenatedÎ¸)

Compute the log-likelihood of Poisson GLM

ARGUMENT
-`mpGLM`: a structure containing the parameters, input, and observations a mixture of Poisson GLM
-`ğ®`: filters of the state-independent inputs of the GLM

RETURN
-log-likelihood of a Poisson GLM
"""
function loglikelihood(mpGLM::MixturePoissonGLM, ğ®::Vector{type}) where {type<:Real}
	@unpack Î”t, ğ—, ğ² = mpGLM
	ğ” = @view ğ—[:,1:length(ğ®)]
	ğ”áµ€ = transpose(ğ”)
	ğ‹ = ğ”*ğ®
	T = length(ğ²)
	dÂ²â„“_dLÂ², dâ„“_dL = zeros(type, T), zeros(type, T)
	â„“ = 0.0
	for t = 1:T
		â„“ += poissonloglikelihood(Î”t, ğ‹[t], ğ²[t])
	end
	â„“
end


"""
    expectation_loglikelihood(Î³, mpGLM, x)

ForwardDiff-compatible computation of the expectation of the log-likelihood of the mixture of Poisson generalized model of one neuron

Ignores the log(y!) term, which does not depend on the parameters

ARGUMENT
-`Î³`: posterior probability of the latent variable
-`mpGLM`: the GLM of one neuron
-`x`: weights of the linear filters of the GLM concatenated as a vector of floating-point numbers

RETURN
-expectation of the log-likelihood of the spike train of one neuron
"""
function expectation_loglikelihood(concatenatedÎ¸::Vector{<:Real},
								   Î³::Matrix{<:Vector{<:AbstractFloat}},
								   mpGLM::MixturePoissonGLM)
	mpGLM = MixturePoissonGLM(concatenatedÎ¸, mpGLM)
    @unpack Î”t, ğ² = mpGLM
    T = length(ğ²)
    Î,K = size(Î³)
    Q = 0.0
    @inbounds for i = 1:Î
	    for k = 1:K
			ğ‹ = linearpredictor(mpGLM,i,k)
            for t = 1:T
				Q += Î³[i,k][t]*poissonloglikelihood(Î”t, ğ‹[t], ğ²[t])
            end
        end
    end
    return Q
end

"""
	expectation_âˆ‡loglikelihood!(âˆ‡Q, indexÎ¸, Î³, mpGLM)

Expectation under the posterior probability of the gradient of the log-likelihood

MODIFIED ARGUMENT
-`âˆ‡`: The gradient

UNMODIFIED ARGUMENT
-`Î³`: Joint posterior probability of the accumulator and coupling variable. Î³[i,k][t] corresponds to the i-th accumulator state and the k-th coupling state in the t-th time bin in the trialset.
-`mpGLM`: structure containing information for the mixture of Poisson GLM for one neuron

EXAMPLE
```julia-rep
julia> using FHMDDM, Random
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_05_05_test/data.mat"; randomize=true);
julia> mpGLM = model.trialsets[1].mpGLMs[1]
julia> Î³ = FHMDDM.randomposterior(mpGLM; rng=MersenneTwister(1234))
julia> âˆ‡Q = FHMDDM.GLMÎ¸(mpGLM.Î¸, eltype(mpGLM.Î¸.ğ®))
julia> FHMDDM.expectation_âˆ‡loglikelihood!(âˆ‡Q, Î³, mpGLM)
julia> ghand = FHMDDM.concatenateparameters(âˆ‡Q)[1]
julia> using ForwardDiff
julia> concatenatedÎ¸ = FHMDDM.concatenateparameters(mpGLM.Î¸)[1]
julia> f(x) = FHMDDM.expectation_loglikelihood(x, Î³, mpGLM)
julia> gauto = ForwardDiff.gradient(f, concatenatedÎ¸)
julia> maximum(abs.(gauto .- ghand))
```
"""
function expectation_âˆ‡loglikelihood!(âˆ‡Q::GLMÎ¸,
	                                Î³::Matrix{<:Vector{<:Real}},
	                                mpGLM::MixturePoissonGLM)
	@unpack Î”t, dğ›_dB, ğ•, ğ—, ğ² = mpGLM
	Î, K = size(Î³)
	T = length(ğ²)
	âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚– = collect(zeros(T) for k=1:K)
	âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–â¨€dÎ¾áµ¢_dB = collect(zeros(T) for k=1:K)
	@inbounds for i = 1:Î
		for k = 1:K
			ğ‹ = linearpredictor(mpGLM,i,k)
			for t=1:T
				dQáµ¢â‚–_dLáµ¢â‚– = Î³[i,k][t] * differentiate_loglikelihood_wrt_linearpredictor(Î”t, ğ‹[t], ğ²[t])
				âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–[k][t] += dQáµ¢â‚–_dLáµ¢â‚–
				âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–â¨€dÎ¾áµ¢_dB[k][t] += dQáµ¢â‚–_dLáµ¢â‚–*dğ›_dB[i]
			end
		end
	end
	q = size(ğ—,2)-size(ğ•,2)
	ğ”áµ€ = transpose(@view ğ—[:,2:q])
	ğ•áµ€ = transpose(ğ•)
	âˆ‘áµ¢â‚–_dQáµ¢â‚–_dLáµ¢â‚– = sum(âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–)
	âˆ‡Q.ğ® .= ğ”áµ€*âˆ‘áµ¢â‚–_dQáµ¢â‚–_dLáµ¢â‚–
	if K > 1
		if pointer(ğ [2]) == pointer(ğ [1]) # gain is not state-dependent
			âˆ‡Q.ğ [1] .= sum(âˆ‘áµ¢â‚–_dQáµ¢â‚–_dLáµ¢â‚–) # equivalent to transpose(@view ğ—[:,1])*âˆ‘áµ¢â‚–_dQáµ¢â‚–_dLáµ¢â‚–
		else
			@inbounds for k = 1:K
				âˆ‡Q.ğ [k] .= sum(âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–â¨€dÎ¾áµ¢_dB[k]) # equivalent to transpose(@view ğ—[:,1])*âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–â¨€dÎ¾áµ¢_dB[k]
			end
		end
		if pointer(ğ¯[2]) == pointer(ğ¯[1]) # tuning is not state-dependent
			âˆ‡Q.ğ¯[1] .= ğ•áµ€*sum(âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–â¨€dÎ¾áµ¢_dB)
		else
			@inbounds for k = 1:K
				âˆ‡Q.ğ¯[k] .= ğ•áµ€*âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–â¨€dÎ¾áµ¢_dB[k]
			end
		end
	end
	return nothing
end
