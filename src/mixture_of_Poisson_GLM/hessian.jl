"""
	functions

-linearpredictor(mpGLMs)
-spikcountderivatives!(memoryforhessian, mpGLMs, sameacrosstrials, trial)
-conditionallikelihood!(pY, ğ›‚, ğ‹, mpGLMs, ntimesteps, Ï„â‚€)
-âˆ‡âˆ‡conditional_log_likelihood!(âˆ‡logpy, âˆ‡âˆ‡logpy, glmderivatives, indexÎ¸, ğ‹, mpGLM, ğ›š, dğ›š_db, dÂ²ğ›š_dbÂ², Ï„)
"""

"""
    linearpredictor(mpGLMs)

Linear combination of the weights in the j-th accumulator state and k-th coupling state

ARGUMENT
-`trialsets`: a vector of trialsets

RETURN
-`ğ‹`: a nested array whose element ğ‹[i][n][j,k][t] corresponds to i-th trialset, n-th neuron, j-th accumualtor state, k-th coupling state, and the t-th time bin in the trialset
"""
function linearpredictor(trialsets::Vector{<:Trialset})
    map(trialsets) do trialset
        map(trialset.mpGLMs) do mpGLM
            Î = mpGLM.Î
            K = length(mpGLM.Î¸.ğ¯)
            map(CartesianIndices((Î,K))) do index
                j = index[1]
                k = index[2]
                linearpredictor(mpGLM, j, k)
            end
        end
    end
end

"""
	spikcountderivatives!(memoryforhessian, mpGLMs, sameacrosstrials, trial)

Derivatives related to the spike count response at each time step in a trial

The quantities that are computed include the first- and second-order partial derivatives of the log-likelihood of each neuron's spike count response (`âˆ‡logpy` and `âˆ‡âˆ‡logpy`) and the gradient of the likelihood of the population spike count response (`âˆ‡pY`) . 

MODIFIED ARGUMENT
-`memoryforhessian`: a structure containing quantities used in each trial

UNMODIFIED ARGUMENT
-`mpGLMs`: vector whose each element is a structure containing the data and parameters of a mixture of Poisson GLM
-`sameacrosstrials`: structure containing intermediate quantities that are fixed across trials
-`trial`: structure containing the data of the trial being used for computation
"""
function spikcountderivatives!(memoryforhessian::Memoryforhessian, mpGLMs::Vector{<:MixturePoissonGLM}, sameacrosstrials::Sameacrosstrials, trial::Trial)
    @unpack ntimesteps, Ï„â‚€, trialsetindex = trial
    ğ‹ = memoryforhessian.ğ‹[trialsetindex]
	ğ›‚ = memoryforhessian.ğ›‚[trialsetindex]
	indexÎ¸glms = memoryforhessian.indexÎ¸glms[trialsetindex]
    ğ›š = memoryforhessian.ğ›š[trialsetindex]
	dğ›š_db = memoryforhessian.dğ›š_db[trialsetindex]
	dÂ²ğ›š_dbÂ² = memoryforhessian.dÂ²ğ›š_dbÂ²[trialsetindex]
    @unpack glmderivatives, âˆ‡logpy, âˆ‡âˆ‡logpy, pY, âˆ‡pY = memoryforhessian
	scaledlikelihood!(pY, ğ›‚, ğ‹, mpGLMs, ntimesteps, Ï„â‚€)
	@inbounds for n in eachindex(mpGLMs)
		differentiate_twice_overdispersion!(glmderivatives,mpGLMs[n].Î¸.a[1])
		for t = 1:ntimesteps
			Ï„ = t+Ï„â‚€
			âˆ‡âˆ‡loglikelihood!(âˆ‡logpy[t][n], âˆ‡âˆ‡logpy[t][n], glmderivatives, indexÎ¸glms[n], ğ‹[n], mpGLMs[n], ğ›š[n], dğ›š_db[n], dÂ²ğ›š_dbÂ²[n], Ï„)
		end
	end
	Î = length(ğ›š[1])
	K = length(mpGLMs[1].Î¸.ğ¯)
	nÎ¸_py = sameacrosstrials.nÎ¸_py[trialsetindex]
	@inbounds for t = 1:ntimesteps
		r = 0
		for n=eachindex(mpGLMs)
			for q = 1:nÎ¸_py[n]
				r+=1
				for i=1:Î
					for j=1:K
						âˆ‡pY[t][r][i,j] = âˆ‡logpy[t][n][q][i,j]*pY[t][i,j]
					end
				end
			end
		end
	end
	return nothing
end

"""
	conditionallikelihood!(pY, ğ›‚, ğ‹, mpGLMs, ntimesteps, Ï„â‚€)

Conditional likelihood of the population spike count response at each time step

The spike count response model is negative binomial

MODIFIED ARGUMENT
-`pY`:nested array used for in-place computation of the conditional likelihood of the population response. Element `pY[t][i,j]` corresponds to the t-th time step in a trial, i-th accumulator state, and j-th coupling state

UNMODIFIED ARGUMENT
-`Î»`: conditional mean of the Poisson or negative-binomial spike count response model at each time step. Element `Î»[n][t][i,j]` corresponds to the n-th neuron, t-th time step in a trial, i-th accumulator state, and j-th coupling state
-`ğ›‚`: overdispersion parameters of each neuron
-`mpGLMs`: a vector of mixture of Poisson GLM's. Each element corresponds to a neuron
-`ntimesteps`: number of time steps in the trial
-`Ï„â‚€`: total number of time steps across trials preceding this trial
"""
function scaledlikelihood!(pY::Vector{<:Matrix{<:AbstractFloat}}, ğ›‚::Vector{<:AbstractFloat}, ğ‹::Vector{<:Matrix{<:Vector{<:AbstractFloat}}}, mpGLMs::Vector{<:MixturePoissonGLM}, ntimesteps::Integer, Ï„â‚€::Integer)
	Î”t = mpGLMs[1].Î”t
	fit_overdispersion = mpGLMs[1].Î¸.fit_overdispersion
	likelihoodscalefactor = mpGLMs[1].likelihoodscalefactor
	@inbounds for t = 1:ntimesteps
		Ï„ = t+Ï„â‚€
		for ij in eachindex(pY[t])
			pY[t][ij] = 1.0
			for n = eachindex(mpGLMs)
				L = ğ‹[n][ij][Ï„]
				y = mpGLMs[n].ğ²[Ï„]
				Î¼ = inverselink(L)
				p = fit_overdispersion ? negbinlikelihood(ğ›‚[n], Î”t, Î¼, y) : poissonlikelihood(Î¼*Î”t, y)
				pY[t][ij] *= p*likelihoodscalefactor
			end
		end
	end
	return nothing
end

"""
	âˆ‡âˆ‡loglikelihood!(âˆ‡logpy, âˆ‡âˆ‡logpy, glmderivatives, indexÎ¸, ğ‹, mpGLM, ğ›š, dğ›š_db, dÂ²ğ›š_dbÂ², Ï„)

Gradient and Hessian of the conditional log-likelihood of one particular neuron at single timestep

MODIFIED ARGUMENT
-`âˆ‡logpy`: Gradient of the conditional log-likelihood of a neuron's response at a single time. Element âˆ‡logpy[i][j,k] correponds to the partial derivative of log p{y(n,t) âˆ£ a(t)=Î¾(j), c(t)=k} with respect to the i-th parameter of the neuron's GLM
-`âˆ‡âˆ‡logpy`: Hessian of the conditional log-likelihood. Element âˆ‡âˆ‡logpy[q,r][i,j] correponds to the partial derivative of log p{y(n,t) âˆ£ a(t)=Î¾(i), c(t)=j} with respect to the q-th and r-th parameters of the neuron's GLM

UNMODIFIED ARGUMENT
-`D`: an object for computing derivatives
-`indexÎ¸`: an object containing indices of parameters
-`ğ‹`: linear predictors. Element `ğ‹[i,j][Ï„]` corresponds to the i-th accumulator state and j-th coupling state for the Ï„-timestep in the trialset
-`mpGLM`: a composite containing the data and parameters of a Poisson mixture generalized linear model
-`ğ›š`: transformed values of the accumulator
-`dğ›š_db`: first derivative of the transformed values of the accumulator with respect to the transformation parameter
-`dÂ²ğ›š_dbÂ²`: second derivative of the transformed values of the accumulator with respect to the transformation parameter
-`Ï„` time step in the trialset
"""
function âˆ‡âˆ‡loglikelihood!(âˆ‡logpy::Vector{<:Matrix{<:Real}},
						âˆ‡âˆ‡logpy::Matrix{<:Matrix{<:Real}},
						glmderivatives::GLMDerivatives,
						indexÎ¸::GLMÎ¸,
						ğ‹::Matrix{<:Vector{<:Real}},
						mpGLM::MixturePoissonGLM,
						ğ›š::Vector{<:Real},
						dğ›š_db::Vector{<:Real},
						dÂ²ğ›š_dbÂ²::Vector{<:Real},
						Ï„::Integer)
	@unpack Î”t, ğ—, Î, ğ•, ğ² = mpGLM
	@unpack b, ğ®, ğ¯, ğ›ƒ, fit_ğ›ƒ, fit_overdispersion = mpGLM.Î¸
	y = mpGLM.ğ²[Ï„]
	K = length(ğ¯)
	nğ® = length(ğ®)
	nğ¯ = length(ğ¯[1])
	indexa = indexÎ¸.a[1]
	indexb = indexÎ¸.b[1]
	Vâ‚œáµ€ğ¯ = zeros(K)
	dL_dğ¯ = zeros(nğ¯)
	for j = 1:K
		for q=1:nğ¯
			Vâ‚œáµ€ğ¯[j] += ğ•[Ï„,q]*ğ¯[j][q]
		end
	end
	for i = 1:Î
		for m=1:nğ¯
			dL_dğ¯[m] = ğ•[Ï„,m]*ğ›š[i]
		end
		for j = 1:K
			indicesğ¯ğ›ƒ = (fit_ğ›ƒ && ((i==1) || (i==Î))) ? indexÎ¸.ğ›ƒ[j] : indexÎ¸.ğ¯[j]
			L = ğ‹[i,j][Ï„]
			differentiate_twice_loglikelihood!(glmderivatives, L, y)
			dâ„“_da = glmderivatives.dâ„“_da[1]
			dâ„“_dL = glmderivatives.dâ„“_dL[1]
			dÂ²â„“_daÂ² = glmderivatives.dÂ²â„“_daÂ²[1]
			dÂ²â„“_dadL = glmderivatives.dÂ²â„“_dadL[1]
			dÂ²â„“_dLÂ² = glmderivatives.dÂ²â„“_dLÂ²[1]
			dL_db = Vâ‚œáµ€ğ¯[j]*dğ›š_db[i]
			dÂ²L_dbÂ² = Vâ‚œáµ€ğ¯[j]*dÂ²ğ›š_dbÂ²[i]
			for m=1:nğ®
				âˆ‡logpy[m][i,j] = dâ„“_dL*ğ—[Ï„,m]
			end
			for m=1:nğ¯
				âˆ‡logpy[indicesğ¯ğ›ƒ[m]][i,j] = dâ„“_dL*dL_dğ¯[m]
			end
			âˆ‡logpy[indexa][i,j] = dâ„“_da
			âˆ‡logpy[indexb][i,j] = dâ„“_dL*dL_db
			for m=1:nğ®
				for n=m:nğ®
					âˆ‡âˆ‡logpy[m,n][i,j] = dÂ²â„“_dLÂ²*ğ—[Ï„,m]*ğ—[Ï„,n]
				end
				for n=1:nğ¯
					âˆ‡âˆ‡logpy[m,indicesğ¯ğ›ƒ[n]][i,j] = dÂ²â„“_dLÂ²*ğ—[Ï„,m]*dL_dğ¯[n]
				end
				âˆ‡âˆ‡logpy[m,indexa][i,j] = dÂ²â„“_dadL*ğ—[Ï„,m]
				âˆ‡âˆ‡logpy[m,indexb][i,j] = dÂ²â„“_dLÂ²*ğ—[Ï„,m]*dL_db
			end
			for m=1:nğ¯
				for n=m:nğ¯
					âˆ‡âˆ‡logpy[indicesğ¯ğ›ƒ[m], indicesğ¯ğ›ƒ[n]][i,j] = dÂ²â„“_dLÂ² * dL_dğ¯[m] * dL_dğ¯[n]
				end
				âˆ‡âˆ‡logpy[indicesğ¯ğ›ƒ[m],indexa][i,j] = dÂ²â„“_dadL*dL_dğ¯[m]
				dÂ²L_dvdb = ğ•[Ï„,m]*dğ›š_db[i]
				âˆ‡âˆ‡logpy[indicesğ¯ğ›ƒ[m],indexb][i,j] = dÂ²â„“_dLÂ²*dL_dğ¯[m]*dL_db + dâ„“_dL*dÂ²L_dvdb
			end
			âˆ‡âˆ‡logpy[indexa,indexa][i,j] = dÂ²â„“_daÂ²
			âˆ‡âˆ‡logpy[indexa,indexb][i,j] = dÂ²â„“_dadL*dL_db
			âˆ‡âˆ‡logpy[indexb,indexb][i,j] = dÂ²â„“_dLÂ²*dL_db^2 + dâ„“_dL*dÂ²L_dbÂ²
		end
	end
	return nothing
end