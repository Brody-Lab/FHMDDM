"""
<<<<<<< Updated upstream
=======
	GLMÎ¸(indicesğ®, options, nğ¯)

Randomly initiate the parameters for a mixture of Poisson generalized linear model

ARGUMENT
-`indicesğ®`: indices of the encoding weights of the temporal basis vectors of each filter that is independent of the accumulator
-`nğ¯`: number of temporal basis vectors specifying the time-varying weight of the accumulator
-`options`: settings of the model

OUTPUT
-an instance of `GLMÎ¸`
"""
function GLMÎ¸(indicesğ®::Indicesğ®, nğ¯::Integer, options::Options)
	nğ® = maximum(vcat((getfield(indicesğ®, field) for field in fieldnames(Indicesğ®))...))
	Î¸ = GLMÎ¸(b_scalefactor = options.b_scalefactor,
			fit_b = options.fit_b,
			fit_ğ›ƒ = options.fit_ğ›ƒ,
			fit_overdispersion = options.fit_overdispersion,
			ğ® = fill(NaN, nğ®),
			indicesğ®=indicesğ®,
			ğ¯ = collect(fill(NaN,nğ¯) for k=1:options.K))
	randomizeparameters!(Î¸, options)
	return Î¸
end

"""
	randomizeparameters!(Î¸, options)

Randomly initialize parameters of a mixture of Poisson GLM

MODIFIED ARGUMENT
-`Î¸`: structure containing parameters of a mixture of Poisson GLM

UNMODIFIED ARGUMENT
-`options`: hyperparameters of the model
"""
function randomizeparameters!(Î¸::GLMÎ¸, options::Options)
	Î¸.a[1] = Î¸.fit_overdispersion ? rand() : -Inf
	Î¸.b[1] = 0.0
	for i in eachindex(Î¸.ğ®)
		Î¸.ğ®[i] = 1.0 .- 2rand()
	end
	Î¸.ğ®[Î¸.indicesğ®.gain] ./= options.tbf_gain_scalefactor
	Î¸.ğ®[Î¸.indicesğ®.postspike] ./= options.tbf_hist_scalefactor
	Î¸.ğ®[Î¸.indicesğ®.poststereoclick] ./= options.tbf_time_scalefactor
	Î¸.ğ®[Î¸.indicesğ®.premovement] ./= options.tbf_move_scalefactor
	Î¸.ğ®[Î¸.indicesğ®.postphotostimulus] ./= options.tbf_phot_scalefactor
	K = length(Î¸.ğ¯)
	if K > 1
		ğ¯â‚€ = (-1.0:2.0/(K-1):1.0)./options.tbf_accu_scalefactor
		for k = 1:K
			Î¸.ğ¯[k] .= ğ¯â‚€[k]
		end
	else
		Î¸.ğ¯[1] .= (1.0 .- 2rand(length(Î¸.ğ¯[1])))./options.tbf_accu_scalefactor
	end
	for k = 1:K
		Î¸.ğ›ƒ[k] .= Î¸.fit_ğ›ƒ ? -Î¸.ğ¯[k] : 0.0
	end
end

"""
>>>>>>> Stashed changes
	initialize_GLM_parameters!(model)

Initialize the GLM parameters using expectation-maximization.

In the E-step, the posterior probability of the accumulator is computed by conditioning on only the behavioral choices. In the M-step, only the GLM parameters are updated. The E- and M-steps assume the coupling variable have only one state. After performing these two steps, if there are multiple coupling states, and the gain is state-dependent, it is randomly initialized. If there are multiple coupling states, and the encoding of the accumulated evidence is state-dependent, then the weight in the first state is set to be three times of the initialized value, and the weight in the second state is set to be the negative of the initialized value.

MODIFIED ARGUMENT
-`model`: structure containing the data, parameters, and hyperparameters

OPTIONAL ARGUMENT
-`show_trace`: whether the details of the M-step should be shown
"""
function initialize_GLM_parameters!(model::Model; iterations::Integer=5, show_trace::Bool=false)
	memory = FHMDDM.Memoryforgradient(model)
	P = choiceposteriors!(memory, model)
	for (trialset, Î³áµ¢) in zip(model.trialsets, memory.Î³)
	    for mpGLM in trialset.mpGLMs
	        maximize_expectation_of_loglikelihood!(mpGLM, Î³áµ¢; show_trace=show_trace)
	    end
	end
	if model.options.K > 1
		for i in eachindex(model.trialsets)
			for mpGLM in model.trialsets[i].mpGLMs
				vmean = mean(mpGLM.Î¸.ğ¯)
				mpGLM.Î¸.ğ¯[1] .= mpGLM.Î¸.ğ›ƒ[1] .= 3.0.*vmean
				mpGLM.Î¸.ğ¯[2] .= mpGLM.Î¸.ğ›ƒ[2] .= -vmean
			end
		end
	end
	maximize_expectation_of_loglikelihood!(model;iterations=iterations, show_trace=show_trace)
end

"""
	maximize_expectation_of_loglikelihood!(model)

Learn the parameters of each neuron's Poisson mixture GLM in the model
"""
function maximize_expectation_of_loglikelihood!(model::Model; iterations::Integer=5, show_trace::Bool=false)
	memory = FHMDDM.Memoryforgradient(model)
	P = update!(memory, model)
	for j = 1:iterations
		posteriors!(memory, P, model)
		for i in eachindex(model.trialsets)
			for mpGLM in model.trialsets[i].mpGLMs
				maximize_expectation_of_loglikelihood!(mpGLM, memory.Î³[i]; show_trace=show_trace)
			end
		end
	end
end

"""
	maximize_expectation_of_loglikelihood!(mpGLM, Î³)

Learn the filters of a Poisson mixture GLM by maximizing the expectation of the log-likelihood

MODIFIED ARGUMENT
-`mpGLM`: a structure containing the data and parameters of the mixture of Poisson GLM of one neuron

UNMODIFIED ARGUMENT
-`Î³`: posterior probability of the latent variables. Element `Î³[j][Ï„]` corresponds to the posterior probability of the j-th accumulator state  in the Ï„-th time step
"""
function maximize_expectation_of_loglikelihood!(mpGLM::MixturePoissonGLM, Î³::Matrix{<:Vector{<:Real}}; show_trace::Bool=false, iterations::Integer=20)
	xâ‚€ = concatenateparameters(mpGLM.Î¸; initialization=true)
	nparameters = length(xâ‚€)
	D = GLMDerivatives(mpGLM)
	Q = fill(NaN,1)
	âˆ‡Q = fill(NaN, nparameters)
	âˆ‡âˆ‡Q = fill(NaN, nparameters, nparameters)
	f(x) = negexpectation_of_loglikelihood!(mpGLM,D,Q,âˆ‡Q,âˆ‡âˆ‡Q,Î³,x)
	âˆ‡f!(âˆ‡, x) = negexpectation_of_âˆ‡loglikelihood!(âˆ‡,mpGLM,D,Q,âˆ‡Q,âˆ‡âˆ‡Q,Î³,x)
	âˆ‡âˆ‡f!(âˆ‡âˆ‡, x) = negexpectation_of_âˆ‡âˆ‡loglikelihood!(âˆ‡âˆ‡,mpGLM,D,Q,âˆ‡Q,âˆ‡âˆ‡Q,Î³,x)
    results = Optim.optimize(f, âˆ‡f!, âˆ‡âˆ‡f!, xâ‚€, NewtonTrustRegion(), Optim.Options(show_trace=show_trace, iterations=iterations))
	sortparameters!(mpGLM.Î¸, Optim.minimizer(results); initialization=true)
	return nothing
end

"""
	negexpectation_of_âˆ‡âˆ‡loglikelihood!(âˆ‡âˆ‡,mpGLM,D,Q,âˆ‡Q,âˆ‡âˆ‡Q,Î³,x)

Hessian of the negative of the expectation of the log-likelihood under the posterior probability of the latent variables

MODIFIED ARGUMENT
-`âˆ‡âˆ‡`: Hessian of the negative of the expectation
-`mpGLM`: a structure containing the data and parameters of the mixture of Poisson GLM of one neuron
-`Q`: an one-element vector that quantifies the expectation
-`D`: an object for in-place computation of first and second derivatives of the log-likelihood
-`âˆ‡Q`: gradient of the expectation with respect to the filters in the k-th state
-`âˆ‡âˆ‡Q`: Hessian of the expectation with respect to the filters in the k-th state

UNMODIFIED ARGUMENT
-`Î³`: posterior probability of the latent variables. Element `Î³[j][Ï„]` corresponds to the posterior probability of the j-th accumulator state  in the Ï„-th time step
-`x`: filters
"""
function negexpectation_of_âˆ‡âˆ‡loglikelihood!(âˆ‡âˆ‡::Matrix{<:Real}, mpGLM::MixturePoissonGLM, D::GLMDerivatives, Q::Vector{<:Real}, âˆ‡Q::Vector{<:Real}, âˆ‡âˆ‡Q::Matrix{<:Real}, Î³::Matrix{<:Vector{<:Real}}, x::Vector{<:Real})
	xâ‚€ = concatenateparameters(mpGLM.Î¸; initialization=true)
	if (x != xâ‚€) || isnan(Q[1])
		sortparameters!(mpGLM.Î¸, x; initialization=true)
		expectation_of_âˆ‡âˆ‡loglikelihood!(D,Q,âˆ‡Q,âˆ‡âˆ‡Q,Î³,mpGLM)
	end
	nparameters = length(x)
	for i =1:nparameters
		for j=i:nparameters
			âˆ‡âˆ‡[i,j] = âˆ‡âˆ‡[j,i] = -âˆ‡âˆ‡Q[i,j]
		end
	end
	return nothing
end

"""
	negexpectation_of_loglikelihood!(mpGLM,D,Q,âˆ‡Q,âˆ‡âˆ‡Q,Î³,x)

Negative expectation of the log-likelihood under the posterior probability of the latent variables

MODIFIED ARGUMENT
-`âˆ‡`: gradient of the negative of the expectation

For other modified and unmodified arguments see documentation for `negexpectation_of_âˆ‡âˆ‡loglikelihood!(âˆ‡âˆ‡,mpGLM,D,Q,âˆ‡Q,âˆ‡âˆ‡Q,Î³,x)`

RETURN
-a scalar that is the negative of the expectation of the log-likelihood under the posterior probability distribution
"""
function negexpectation_of_loglikelihood!(mpGLM::MixturePoissonGLM, D::GLMDerivatives, Q::Vector{<:Real}, âˆ‡Q::Vector{<:Real}, âˆ‡âˆ‡Q::Matrix{<:Real}, Î³::Matrix{<:Vector{<:Real}}, x::Vector{<:Real})
	xâ‚€ = concatenateparameters(mpGLM.Î¸; initialization=true)
	if (x != xâ‚€) || isnan(Q[1])
		sortparameters!(mpGLM.Î¸, x; initialization=true)
		expectation_of_âˆ‡âˆ‡loglikelihood!(D,Q,âˆ‡Q,âˆ‡âˆ‡Q,Î³,mpGLM)
	end
	-Q[1]
end

"""
	negexpectation_of_âˆ‡loglikelihood!(âˆ‡,mpGLM,D,Q,âˆ‡Q,âˆ‡âˆ‡Q,Î³,x)

Gradient of the negative of the expectation of the log-likelihood under the posterior probability of the latent variables

ARGUMENT
For other modified and unmodified arguments see documentation for `negexpectation_of_âˆ‡âˆ‡loglikelihood!(âˆ‡âˆ‡,mpGLM,D,Q,âˆ‡Q,âˆ‡âˆ‡Q,Î³,x)`
"""
function negexpectation_of_âˆ‡loglikelihood!(âˆ‡::Vector{<:Real}, mpGLM::MixturePoissonGLM, D::GLMDerivatives, Q::Vector{<:Real}, âˆ‡Q::Vector{<:Real}, âˆ‡âˆ‡Q::Matrix{<:Real}, Î³::Matrix{<:Vector{<:Real}}, x::Vector{<:Real})
	xâ‚€ = concatenateparameters(mpGLM.Î¸; initialization=true)
	if (x != xâ‚€) || isnan(Q[1])
		sortparameters!(mpGLM.Î¸, x; initialization=true)
		expectation_of_âˆ‡âˆ‡loglikelihood!(D,Q,âˆ‡Q,âˆ‡âˆ‡Q,Î³,mpGLM)
	end
	for i in eachindex(âˆ‡)
		âˆ‡[i] = -âˆ‡Q[i]
	end
	return nothing
end

"""
	expectation_of_âˆ‡âˆ‡loglikelihood!(D, Q,âˆ‡Q,âˆ‡âˆ‡Q,Î³,k,mpGLM)

Compute the expectation of the log-likelihood and its gradient and Hessian

ARGUMENT
-`D`: an object for in-place computation of first and second derivatives of the log-likelihood
-`Q`: expectation of the log-likelihood under the posterior probability of the latent variables. Only the component in the coupling state `k` is included
-`âˆ‡Q`: first-order derivatives of the expectation
-`âˆ‡âˆ‡Q`: second-order derivatives of the expectation

UNMODIFIED ARGUMENT
-`Î³`: posterior probabilities of the latent variables
-`k`: index of the coupling state
-`mpGLM`: a structure containing the data and parameters of the mixture of Poisson GLM of one neuron
"""
function expectation_of_âˆ‡âˆ‡loglikelihood!(D::GLMDerivatives, Q::Vector{<:type}, âˆ‡Q::Vector{<:type}, âˆ‡âˆ‡Q::Matrix{<:type}, Î³::Matrix{<:Vector{<:type}}, mpGLM::MixturePoissonGLM) where {type<:AbstractFloat}
    @unpack Î”t, ğ•, ğ—, ğ², dğ›_dB = mpGLM
<<<<<<< Updated upstream
	@unpack ğ®, ğ¯, ğ›ƒ, fit_ğ›ƒ = mpGLM.Î¸
=======
	@unpack a, ğ®, ğ¯, ğ›ƒ, fit_ğ›ƒ, fit_overdispersion = mpGLM.Î¸
>>>>>>> Stashed changes
	dğ›_dBÂ² = dğ›_dB.^2
	Î, K = size(Î³)
	T = length(ğ²)
	Q[1] = 0.0
	âˆ‡Q .= 0.0
	âˆ‡âˆ‡Q .= 0.0
<<<<<<< Updated upstream
	âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚– = collect(zeros(type,T) for k=1:K)
	âˆ‘áµ¢_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â² = collect(zeros(type,T) for k=1:K)
	if fit_ğ›ƒ
		âˆ‘_post_dQáµ¢â‚–_dLáµ¢â‚–â¨€dÎ¾áµ¢_dB = collect(zeros(type,T) for k=1:K)
		âˆ‘_post_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dB = collect(zeros(type,T) for k=1:K)
		âˆ‘_post_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dBÂ² = collect(zeros(type,T) for k=1:K)
		âˆ‘_pre_dQáµ¢â‚–_dLáµ¢â‚–â¨€dÎ¾áµ¢_dB = collect(zeros(type,T) for k=1:K)
		âˆ‘_pre_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dB = collect(zeros(type,T) for k=1:K)
		âˆ‘_pre_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dBÂ² = collect(zeros(type,T) for k=1:K)
	else
		âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–â¨€dÎ¾áµ¢_dB = collect(zeros(type,T) for k=1:K)
		âˆ‘áµ¢_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dB = collect(zeros(type,T) for k=1:K)
		âˆ‘áµ¢_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dBÂ² = collect(zeros(type,T) for k=1:K)
=======
	âˆ‘áµ¢â‚–_dQáµ¢â‚–_dLáµ¢â‚– = zeros(type,T)
	âˆ‘áµ¢â‚–_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â² = zeros(type,T)
	âˆ‘áµ¢â‚–_dÂ²Qáµ¢â‚–_dadLáµ¢â‚– = zeros(type,T)
	âˆ‘_post_dQáµ¢â‚–_dLáµ¢â‚–â¨€dÎ¾áµ¢_dB = collect(zeros(type,T) for k=1:K)
	âˆ‘_post_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dB = collect(zeros(type,T) for k=1:K)
	âˆ‘_post_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dBÂ² = collect(zeros(type,T) for k=1:K)
	âˆ‘_pre_dQáµ¢â‚–_dLáµ¢â‚–â¨€dÎ¾áµ¢_dB = collect(zeros(type,T) for k=1:K)
	âˆ‘_pre_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dB = collect(zeros(type,T) for k=1:K)
	âˆ‘_pre_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dBÂ² = collect(zeros(type,T) for k=1:K)
	differentiate_twice_overdispersion!(D, a[1])
	if fit_overdispersion
		âˆ‘_dÂ²Q_daÂ² = 0.0
		âˆ‘_dQ_da = 0.0
		âˆ‘_post_dÂ²Qáµ¢â‚–_dadLáµ¢â‚–â¨€dÎ¾áµ¢_dB = collect(zeros(type,T) for k=1:K)
		âˆ‘_pre_dÂ²Qáµ¢â‚–_dadLáµ¢â‚–â¨€dÎ¾áµ¢_dB = collect(zeros(type,T) for k=1:K)
>>>>>>> Stashed changes
	end
	@inbounds for i = 1:Î
		for k = 1:K
			ğ‹ = linearpredictor(mpGLM,i,k)
			for t=1:T
<<<<<<< Updated upstream
				dÂ²â„“_dLÂ², dâ„“_dL, â„“ = differentiate_twice_loglikelihood_wrt_linearpredictor(Î”t, ğ‹[t], ğ²[t])
				Q[1] += Î³[i,k][t]*â„“
				dQáµ¢â‚–_dLáµ¢â‚– = Î³[i,k][t] * dâ„“_dL
				âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–[k][t] += dQáµ¢â‚–_dLáµ¢â‚–
				dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â² = Î³[i,k][t] * dÂ²â„“_dLÂ²
				âˆ‘áµ¢_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²[k][t] += dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²
				if fit_ğ›ƒ
					if (i==1) || (i==Î)
						âˆ‘_post_dQáµ¢â‚–_dLáµ¢â‚–â¨€dÎ¾áµ¢_dB[k][t] += dQáµ¢â‚–_dLáµ¢â‚–*dğ›_dB[i]
						âˆ‘_post_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dB[k][t] += dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²*dğ›_dB[i]
						âˆ‘_post_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dBÂ²[k][t] += dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²*dğ›_dBÂ²[i]
					else
						âˆ‘_pre_dQáµ¢â‚–_dLáµ¢â‚–â¨€dÎ¾áµ¢_dB[k][t] += dQáµ¢â‚–_dLáµ¢â‚–*dğ›_dB[i]
						âˆ‘_pre_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dB[k][t] += dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²*dğ›_dB[i]
						âˆ‘_pre_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dBÂ²[k][t] += dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²*dğ›_dBÂ²[i]
					end
=======
				differentiate_twice_loglikelihood!(D,ğ‹[t],mpGLM.ğ²[t])
				if fit_overdispersion
					âˆ‘_dQ_da += Î³[i,k][t]*D.dâ„“_da[1]
					âˆ‘_dÂ²Q_daÂ² += Î³[i,k][t]*D.dÂ²â„“_daÂ²[1]
					dÂ²Qáµ¢â‚–_dadLáµ¢â‚– = Î³[i,k][t]*D.dÂ²â„“_dadL[1]
					âˆ‘áµ¢â‚–_dÂ²Qáµ¢â‚–_dadLáµ¢â‚–[t] += dÂ²Qáµ¢â‚–_dadLáµ¢â‚–
					if (i==1) || (i==Î)
						âˆ‘_post_dÂ²Qáµ¢â‚–_dadLáµ¢â‚–â¨€dÎ¾áµ¢_dB[k][t] += dÂ²Qáµ¢â‚–_dadLáµ¢â‚–*dğ›_dB[i]
					else
						âˆ‘_pre_dÂ²Qáµ¢â‚–_dadLáµ¢â‚–â¨€dÎ¾áµ¢_dB[k][t] += dÂ²Qáµ¢â‚–_dadLáµ¢â‚–*dğ›_dB[i]
					end
				end
				Q[1] += Î³[i,k][t]*D.â„“[1]
				dQáµ¢â‚–_dLáµ¢â‚– = Î³[i,k][t] * D.dâ„“_dL[1]
				dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â² = Î³[i,k][t] * D.dÂ²â„“_dLÂ²[1]
				âˆ‘áµ¢â‚–_dQáµ¢â‚–_dLáµ¢â‚–[t] += dQáµ¢â‚–_dLáµ¢â‚–
				âˆ‘áµ¢â‚–_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²[t] += dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²
				if (i==1) || (i==Î)
					âˆ‘_post_dQáµ¢â‚–_dLáµ¢â‚–â¨€dÎ¾áµ¢_dB[k][t] += dQáµ¢â‚–_dLáµ¢â‚–*dğ›_dB[i]
					âˆ‘_post_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dB[k][t] += dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²*dğ›_dB[i]
					âˆ‘_post_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dBÂ²[k][t] += dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²*dğ›_dBÂ²[i]
>>>>>>> Stashed changes
				else
					âˆ‘_pre_dQáµ¢â‚–_dLáµ¢â‚–â¨€dÎ¾áµ¢_dB[k][t] += dQáµ¢â‚–_dLáµ¢â‚–*dğ›_dB[i]
					âˆ‘_pre_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dB[k][t] += dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²*dğ›_dB[i]
					âˆ‘_pre_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dBÂ²[k][t] += dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²*dğ›_dBÂ²[i]
				end
			end
		end
	end
	nğ® = length(ğ®)
	nğ¯ = length(ğ¯[1])
	indicesğ® = 1:nğ®
	indicesğ¯ = collect(indicesğ®[end] .+ ((k-1)*nğ¯+1 : k*nğ¯) for k = 1:K)
<<<<<<< Updated upstream
	if fit_ğ›ƒ
		indicesğ›ƒ = collect(indicesğ¯[end][end] .+ ((k-1)*nğ¯+1 : k*nğ¯) for k = 1:K)
	end
=======
	indicesğ›ƒ = collect(indicesğ¯[end][end] .+ ((k-1)*nğ¯+1 : k*nğ¯) for k = 1:K)
	indexa = 1 + (fit_ğ›ƒ ? indicesğ›ƒ[end][end] : indicesğ¯[end][end])
>>>>>>> Stashed changes
	ğ” = @view ğ—[:, 1:nğ®]
	ğ”áµ€, ğ•áµ€ = transpose(ğ”), transpose(ğ•)
	âˆ‡Q[indicesğ®] .= ğ”áµ€*âˆ‘áµ¢â‚–_dQáµ¢â‚–_dLáµ¢â‚–
	âˆ‡âˆ‡Q[indicesğ®, indicesğ®] .= ğ”áµ€*(âˆ‘áµ¢â‚–_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â².*ğ”)
	if fit_ğ›ƒ
		@inbounds for k = 1:K
			âˆ‡Q[indicesğ¯[k]] .= ğ•áµ€*âˆ‘_pre_dQáµ¢â‚–_dLáµ¢â‚–â¨€dÎ¾áµ¢_dB[k]
			âˆ‡Q[indicesğ›ƒ[k]] .= ğ•áµ€*âˆ‘_post_dQáµ¢â‚–_dLáµ¢â‚–â¨€dÎ¾áµ¢_dB[k]
			âˆ‡âˆ‡Q[indicesğ®, indicesğ¯[k]] .= ğ”áµ€*(âˆ‘_pre_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dB[k].*ğ•)
			âˆ‡âˆ‡Q[indicesğ®, indicesğ›ƒ[k]] .= ğ”áµ€*(âˆ‘_post_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dB[k].*ğ•)
			âˆ‡âˆ‡Q[indicesğ¯[k], indicesğ¯[k]] .= ğ•áµ€*(âˆ‘_pre_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dBÂ²[k].*ğ•)
			âˆ‡âˆ‡Q[indicesğ›ƒ[k], indicesğ›ƒ[k]] .= ğ•áµ€*(âˆ‘_post_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dBÂ²[k].*ğ•)
		end
	else
		@inbounds for k = 1:K
<<<<<<< Updated upstream
			âˆ‡Q[indicesğ¯[k]] .= ğ•áµ€*âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–â¨€dÎ¾áµ¢_dB[k]
			âˆ‡âˆ‡Q[indicesğ®, indicesğ¯[k]] .= ğ”áµ€*(âˆ‘áµ¢_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dB[k].*ğ•)
			âˆ‡âˆ‡Q[indicesğ¯[k], indicesğ¯[k]] .= ğ•áµ€*(âˆ‘áµ¢_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dBÂ²[k].*ğ•)
=======
			âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–â¨€dÎ¾áµ¢_dB = âˆ‘_pre_dQáµ¢â‚–_dLáµ¢â‚–â¨€dÎ¾áµ¢_dB[k] + âˆ‘_post_dQáµ¢â‚–_dLáµ¢â‚–â¨€dÎ¾áµ¢_dB[k]
			âˆ‘áµ¢_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dB = âˆ‘_pre_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dB[k] + âˆ‘_post_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dB[k]
			âˆ‘áµ¢_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dBÂ² = âˆ‘_pre_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dBÂ²[k] + âˆ‘_post_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dBÂ²[k]
			âˆ‡Q[indicesğ¯[k]] .= ğ•áµ€*âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–â¨€dÎ¾áµ¢_dB
			âˆ‡âˆ‡Q[indicesğ®, indicesğ¯[k]] .= ğ”áµ€*(âˆ‘áµ¢_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dB.*ğ•)
			âˆ‡âˆ‡Q[indicesğ¯[k], indicesğ¯[k]] .= ğ•áµ€*(âˆ‘áµ¢_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dBÂ².*ğ•)
>>>>>>> Stashed changes
		end
	end
	if fit_overdispersion
		âˆ‡Q[indexa] = âˆ‘_dQ_da
		âˆ‡âˆ‡Q[indicesğ®, indexa] = ğ”áµ€*âˆ‘áµ¢â‚–_dÂ²Qáµ¢â‚–_dadLáµ¢â‚–
		if fit_ğ›ƒ
			@inbounds for k = 1:K
				âˆ‡âˆ‡Q[indicesğ¯[k], indexa] .= ğ•áµ€*âˆ‘_pre_dÂ²Qáµ¢â‚–_dadLáµ¢â‚–â¨€dÎ¾áµ¢_dB[k]
				âˆ‡âˆ‡Q[indicesğ›ƒ[k], indexa] .= ğ•áµ€*âˆ‘_post_dÂ²Qáµ¢â‚–_dadLáµ¢â‚–â¨€dÎ¾áµ¢_dB[k]
			end
		else
			@inbounds for k = 1:K
				âˆ‘áµ¢_dÂ²Qáµ¢â‚–_dadLáµ¢â‚–â¨€dÎ¾áµ¢_dB = âˆ‘_pre_dÂ²Qáµ¢â‚–_dadLáµ¢â‚–â¨€dÎ¾áµ¢_dB[k] + âˆ‘_post_dÂ²Qáµ¢â‚–_dadLáµ¢â‚–â¨€dÎ¾áµ¢_dB[k]
				âˆ‡âˆ‡Q[indicesğ¯[k], indexa] .= ğ•áµ€*âˆ‘áµ¢_dÂ²Qáµ¢â‚–_dadLáµ¢â‚–â¨€dÎ¾áµ¢_dB
			end
		end
		âˆ‡âˆ‡Q[indexa, indexa] = âˆ‘_dÂ²Q_daÂ²
	end
	for i = 1:size(âˆ‡âˆ‡Q,1)
		for j = i+1:size(âˆ‡âˆ‡Q,2)
			âˆ‡âˆ‡Q[j,i] = âˆ‡âˆ‡Q[i,j]
		end
	end
	return nothing
end
