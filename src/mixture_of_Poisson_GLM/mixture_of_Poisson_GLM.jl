"""
	GLMÎ¸(options, ğ®indices_hist, ğ®indices_move, ğ®indices_time, ğ•)

Randomly initiate the parameters for a mixture of Poisson generalized linear model

ARGUMENT
-`options`: settings of the model
-`ğ®indices_hist`: indices in ğ® corresponding to the temporal basis functions of the postspike filter
-`ğ®indices_move`: indices in ğ® corresponding to the temporal basis functions of the premovement filter
-`ğ®indices_time`: indices in ğ® corresponding to the temporal basis functions of the time-in-trial filter
-`ğ•`: constant and time-varying inputs from the accumulator

OUTPUT
-an instance of `GLMÎ¸`
"""
function GLMÎ¸(options::Options, ğ®indices_hist::UnitRange{<:Integer}, ğ®indices_move::UnitRange{<:Integer}, ğ®indices_time::UnitRange{<:Integer}, ğ•::Matrix{<:AbstractFloat})
	nğ® = ğ®indices_move[end]
	nğ¯ =size(ğ•,2)
	Kğ  = options.gain_state_dependent ? options.K : 1
	Kğ¯ = options.tuning_state_dependent ? options.K : 1
	Î¸ = GLMÎ¸(b = fill(NaN,1),
			b_scalefactor = options.b_scalefactor,
			fit_b = options.fit_b,
			ğ  = fill(NaN, Kğ ),
			ğ® = fill(NaN, nğ®),
			ğ®indices_hist=ğ®indices_hist,
			ğ®indices_move=ğ®indices_move,
			ğ®indices_time=ğ®indices_time,
			ğ¯ = collect(fill(NaN,nğ¯) for k=1:Kğ¯))
	randomizeparameters!(Î¸)
	return Î¸
end

"""
	randomizeparameters!(Î¸)

Randomly initialize parameters of a mixture of Poisson GLM
"""
function randomizeparameters!(Î¸::GLMÎ¸)
	Î¸.b[1] = 0.0
	for i in eachindex(Î¸.ğ®)
		Î¸.ğ®[i] = 1.0 .- 2rand()
	end
	Î¸.ğ [1] = 0.0
	for k = 2:length(Î¸.ğ )
		Î¸.ğ [k] = 1.0 .- 2rand()
	end
	if length(Î¸.ğ¯) > 1
		K = length(Î¸.ğ¯)
		ğ¯â‚€ = -1:2/(K-1):1
		for k = 1:K
			Î¸.ğ¯[k] .= ğ¯â‚€[k]
		end
	else
		Î¸.ğ¯[1] .= 1.0
	end
end

"""
	GLMÎ¸(glmÎ¸, elementtype)

Create an uninitialized instance of `GLMÎ¸` with the given element type.

This is for using ForwardDiff

ARGUMENT
-`glmÎ¸`: an instance of GLMÎ¸
-`elementtype`: type of the element in each field of GLMÎ¸

RETURN
-an instance of GLMÎ¸
"""
function GLMÎ¸(glmÎ¸::GLMÎ¸, elementtype)
	GLMÎ¸(b = zeros(elementtype, length(glmÎ¸.b)),
		b_scalefactor = glmÎ¸.b_scalefactor,
		fit_b = glmÎ¸.fit_b,
		ğ  = zeros(elementtype, length(glmÎ¸.ğ )),
		ğ® = zeros(elementtype, length(glmÎ¸.ğ®)),
		ğ¯ = collect(zeros(elementtype, length(ğ¯)) for ğ¯ in glmÎ¸.ğ¯),
		ğ®indices_hist = glmÎ¸.ğ®indices_hist,
		ğ®indices_time = glmÎ¸.ğ®indices_time,
		ğ®indices_move = glmÎ¸.ğ®indices_move)
end

"""
	FHMDDM.copy(glmÎ¸)

Make a copy of a structure containing the parameters of a mixture of Poisson GLM
"""
function FHMDDM.copy(glmÎ¸::GLMÎ¸)
	GLMÎ¸(b = copy(glmÎ¸.b),
		b_scalefactor = glmÎ¸.b_scalefactor,
		fit_b = glmÎ¸.fit_b,
		ğ  = copy(glmÎ¸.ğ ),
		ğ® = copy(glmÎ¸.ğ®),
		ğ¯ = collect(copy(ğ¯â‚–) for ğ¯â‚– in glmÎ¸.ğ¯),
		ğ®indices_hist = copy(glmÎ¸.ğ®indices_hist),
		ğ®indices_time = copy(glmÎ¸.ğ®indices_time),
		ğ®indices_move = copy(glmÎ¸.ğ®indices_move))
end

"""
	initialize(glmÎ¸)

Create an uninitialized instance of `GLMÎ¸`
"""
initialize(glmÎ¸::GLMÎ¸) = GLMÎ¸(glmÎ¸, eltype(glmÎ¸.ğ®))

"""
	MixturePoissonGLM(concatenatedÎ¸, glmÎ¸index, mpGLM)

Create a structure for a mixture of Poisson GLM with updated parameters

ARGUMENT
-`concatenatedÎ¸`: a vector of new parameter values
-`glmÎ¸index`: index of each parameter in the vector of values
-`mpGLM`: a structure containing information on the mixture of Poisson GLM for one neuron

OUTPUT
-a new structure for the mixture of Poisson GLM of a neuron with new parameter values
"""
function MixturePoissonGLM(concatenatedÎ¸::Vector{T},
						   mpGLM::MixturePoissonGLM;
						   offset::Integer=0,
						   omitb::Bool=false) where {T<:Real}
	mpGLM = MixturePoissonGLM(Î”t=mpGLM.Î”t,
							dğ›_dB=mpGLM.dğ›_dB,
							Î¦â‚=mpGLM.Î¦â‚,
                        	Î¦â‚•=mpGLM.Î¦â‚•,
							Î¦â‚˜=mpGLM.Î¦â‚˜,
							Î¦â‚œ=mpGLM.Î¦â‚œ,
							Î¸=GLMÎ¸(mpGLM.Î¸, concatenatedÎ¸; offset=offset, omitb=omitb),
							ğ•=mpGLM.ğ•,
							ğ—=mpGLM.ğ—,
							ğ²=mpGLM.ğ²)
	return mpGLM
end

"""
    likelihood(mpGLM, j, k)

Conditional likelihood of the spike train, given the index of the state of the accumulator `j` and the state of the coupling `k`, and also by the prior likelihood of the regression weights

UNMODIFIED ARGUMENT
-`mpGLM`: the mixture of Poisson generalized linear model for one neuron
-`j`: state of accumulator variable
-`k`: state of the coupling variable

RETURN
-`ğ©`: a vector by which the conditional likelihood of the spike train and the prior likelihood of the regression weights are multiplied against
"""
function likelihood(mpGLM::MixturePoissonGLM, j::Integer, k::Integer)
    @unpack Î”t, ğ² = mpGLM
    ğ‹ = linearpredictor(mpGLM, j, k)
    ğ© = ğ‹ # reuse memory
    @inbounds for i=1:length(ğ©)
        ğ©[i] = poissonlikelihood(Î”t, ğ‹[i], ğ²[i])
    end
    return ğ©
end

"""
    likelihood!(ğ©, mpGLM, j, k)

In-place multiplication of `ğ©` by the conditional likelihood of the spike train, given the index of the state of the accumulator `j` and the state of the coupling `k`, and also by the prior likelihood of the regression weights

MODIFIED ARGUMENT
-`ğ©`: a vector by which the conditional likelihood of the spike train and the prior likelihood of the regression weights are multiplied against

UNMODIFIED ARGUMENT
-`mpGLM`: the mixture of Poisson generalized linear model for one neuron
-`j`: state of accumulator variable
-`k`: state of the coupling variable

RETURN
-`nothing`
"""
function likelihood!(ğ©::Vector{<:Real}, mpGLM::MixturePoissonGLM, j::Integer, k::Integer)
    @unpack Î”t, ğ² = mpGLM
    ğ‹ = linearpredictor(mpGLM, j, k)
    @inbounds for i=1:length(ğ©)
		ğ©[i] *= poissonlikelihood(Î”t, ğ‹[i], ğ²[i])
    end
    return nothing
end

"""
	Poissonlikelihood(Î»Î”t, L, y)

Probability of a Poisson observation

ARGUMENT
-`Î»Î”t`: the expected value
-`y`: the observation
-`y!`: the factorial of the observation

OUTPUT
-the likelihood
"""
function poissonlikelihood(Î”t::Real, L::Real, y::Integer)
	Î»Î”t = softplus(L)*Î”t
	poissonlikelihood(Î»Î”t, y)
end

"""
	poissonlikelihood(Î»Î”t, y)

Likelihood of observation `y` given intensity `Î»Î”t`
"""
function poissonlikelihood(Î»Î”t::Real, y::Integer)
	if y==0
		exp(-Î»Î”t)
	elseif y==1
		Î»Î”t*exp(-Î»Î”t)
	else
		Î»Î”t^y * exp(-Î»Î”t) / factorial(y)
	end
end

"""
    linearpredictor(mpGLM, j, k)

Linear combination of the weights in the j-th accumulator state and k-th coupling state

ARGUMENT
-`mpGLM`: the mixture of Poisson generalized linear model of one neuron
-`j`: state of the accumulator variable
-`k`: state of the coupling variable

RETURN
-`ğ‹`: a vector whose element ğ‹[t] corresponds to the t-th time bin in the trialset
"""
function linearpredictor(mpGLM::MixturePoissonGLM, j::Integer, k::Integer)
    @unpack ğ—, dğ›_dB = mpGLM
    @unpack b, b_scalefactor, ğ , ğ®, ğ¯ = mpGLM.Î¸
	gâ‚– = ğ [min(length(ğ ), k)]
	ğ¯â‚– = ğ¯[min(length(ğ¯), k)]
	transformedÎ¾ = transformaccumulator(b[1]*b_scalefactor, dğ›_dB[j])
	ğ—*vcat(gâ‚–, ğ®, ğ¯â‚–.*transformedÎ¾)
end

"""
	poissonloglikelihood(Î»Î”t, y)

Log-likelihood of an observation under a Poisson GLM

ARGUMENT
-`Î»Î”t`: Poisson intensity per second
-`y`: observation

RETURN
-log-likelihood
"""
function poissonloglikelihood(Î»Î”t::Real, y::Integer)
	if y == 0
		-Î»Î”t
	elseif y == 1
		log(Î»Î”t) - Î»Î”t
	else
		y*log(Î»Î”t) - Î»Î”t
	end
end

"""
    poissonloglikelihood

Log-likelihood of an observation under a Poisson GLM with a softplus nonlinearity

ARGUMENT
-`Î”t`: duration of time step
-`L`: linear predictor
-`y`: observation

RETURN
-log-likelihood
"""
poissonloglikelihood(Î”t::AbstractFloat, L::Real, y::Integer) = poissonloglikelihood(softplus(L)*Î”t, y)

"""
    differentiate_loglikelihood_wrt_linearpredictor

Differentiate the log-likelihood of a Poisson GLM with respect to the linear predictor

The Poisson GLM is assumed to have a a softplus nonlinearity

ARGUMENT
-`Î”t`: duration of time step
-`L`: linear predictor at one time step
-`Î»`: Poisson rate
-`y`: observation at that time step

RETURN
-the first derivative with respect to the linear predictor

EXAMPLE
```julia-repl
julia> using FHMDDM, ForwardDiff, LogExpFunctions
julia> Î”t = 0.01
julia> y = 2
julia> f(x) = let Î»Î”t = softplus(x[1])*Î”t; y*log(Î»Î”t)-Î»Î”t+log(factorial(y)); end
julia> x = rand(1)
julia> d1auto = ForwardDiff.gradient(f, x)
julia> d1hand = FHMDDM.differentiate_loglikelihood_wrt_linearpredictor(Î”t, x[1], softplus(x[1]), y)
julia> abs(d1hand - d1auto[1])
```
"""
function differentiate_loglikelihood_wrt_linearpredictor(Î”t::AbstractFloat, L::Real, Î»::Real, y::Integer)
	dÎ»_dL = logistic(L)
    if y > 0
        if L > -100.0
            dâ„“_dL = dÎ»_dL*(y/Î» - Î”t)
        else
            dâ„“_dL = y - dÎ»_dL*Î”t  # the limit of `dÎ»_dL/Î»` as x goes to -âˆ is 1
        end
    else
        dâ„“_dL = -dÎ»_dL*Î”t
    end
end

"""
    differentiate_loglikelihood_wrt_linearpredictor(Î”t, L, y)

First derivative of the log-likelihood of a Poisson GLM with respect to the linear predictor
"""
differentiate_loglikelihood_wrt_linearpredictor(Î”t::AbstractFloat, L::Real, y::Integer) = differentiate_loglikelihood_wrt_linearpredictor(Î”t, L, softplus(L), y)

"""
    differentiate_twice_loglikelihood_wrt_linearpredictor

Second derivative of the log-likelihood of a Poisson GLM with respect to the linear predictor

The Poisson GLM is assumed to have a a softplus nonlinearity

ARGUMENT
-`Î”t`: duration of time step
-`L`: linear predictor at one time step
-`Î»`: Poisson rate
-`y`: observation at that time step

RETURN
-the first derivative with respect to the linear predictor
-the second derivative with respect to the linear predictor

EXAMPLE
```julia-repl
julia> using FHMDDM, ForwardDiff, LogExpFunctions
julia> Î”t = 0.01
julia> y = 3
julia> f(x) = FHMDDM.poissonloglikelihood(Î”t, x, y)
julia> g(x) = ForwardDiff.derivative(f, x)
julia> h(x) = ForwardDiff.derivative(g, x)
julia> xâ‚€ = 1-2rand()
julia> d1auto = g(xâ‚€)
julia> d2auto = h(xâ‚€)
julia> d2hand, d1hand = FHMDDM.differentiate_twice_loglikelihood_wrt_linearpredictor(Î”t, xâ‚€, softplus(xâ‚€), y)
julia> abs(d1hand - d1auto[1])
julia> abs(d2hand - d2auto[1])
```
"""
function differentiate_twice_loglikelihood_wrt_linearpredictor(Î”t::AbstractFloat, L::Real, Î»::Real, y::Integer)
	dÎ»_dL = logistic(L)
	dÂ²Î»_dLdL = dÎ»_dL*(1-dÎ»_dL)
    if y > 0
        if L > -100.0
            dâ„“_dL = dÎ»_dL*(y/Î» - Î”t)
        else
            dâ„“_dL = y - dÎ»_dL*Î”t  # the limit of `dÎ»_dL/Î»` as x goes to -âˆ is 1
        end
		if L > -50.0
			dÂ²â„“_dLdL = y*(Î»*dÂ²Î»_dLdL - dÎ»_dL^2)/Î»^2 - dÂ²Î»_dLdL*Î”t # the limit of first second term is 0 as L goes to -âˆ
		else
			dÂ²â„“_dLdL = -dÂ²Î»_dLdL*Î”t
		end
    else
        dâ„“_dL = -dÎ»_dL*Î”t
		dÂ²â„“_dLdL = -dÂ²Î»_dLdL*Î”t
    end
	return dÂ²â„“_dLdL, dâ„“_dL
end

"""
	differentiate_twice_loglikelihood_wrt_linearpredictor(Î”t, L, y)

Second derivative of the log-likelihood of a Poisson GLM with respect to the linear predictor

ARGUMENT
-`Î”t`: duration of time step
-`L`: linear predictor at one time step
-`y`: observation at that time step

RETURN
-the second derivative with respect to the linear predictor
-the first derivative with respect to the linear predictor
-the log-likelihood
"""
function differentiate_twice_loglikelihood_wrt_linearpredictor(Î”t::AbstractFloat, L::Real, y::Integer)
	Î» = softplus(L)
	Î»Î”t = Î»*Î”t
	â„“ = poissonloglikelihood(Î»Î”t, y)
	dÂ²â„“_dLÂ², dâ„“_dL = differentiate_twice_loglikelihood_wrt_linearpredictor(Î”t, L, Î», y)
	return dÂ²â„“_dLÂ², dâ„“_dL, â„“
end

"""
	expectation_âˆ‡loglikelihood!(âˆ‡Q, Î³, mpGLM)

Expectation under the posterior probability of the gradient of the log-likelihood.

This function is used for computing the gradient of the log-likelihood of the entire model

MODIFIED ARGUMENT
-`âˆ‡`: The gradient

UNMODIFIED ARGUMENT
-`Î³`: Joint posterior probability of the accumulator and coupling variable. Î³[i,k][t] corresponds to the i-th accumulator state and the k-th coupling state in the t-th time bin in the trialset.
-`mpGLM`: structure containing information for the mixture of Poisson GLM for one neuron

EXAMPLE
```julia-rep
julia> using FHMDDM, Random
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_05_05_test/data.mat"; randomize=true);
julia> mpGLM = model.trialsets[1].mpGLMs[1]
julia> Î³ = FHMDDM.randomposterior(mpGLM; rng=MersenneTwister(1234))
julia> âˆ‡Q = FHMDDM.GLMÎ¸(mpGLM.Î¸, eltype(mpGLM.Î¸.ğ®))
julia> FHMDDM.expectation_âˆ‡loglikelihood!(âˆ‡Q, Î³, mpGLM)
julia> ghand = FHMDDM.concatenateparameters(âˆ‡Q)[1]
julia> using ForwardDiff
julia> concatenatedÎ¸ = FHMDDM.concatenateparameters(mpGLM.Î¸)[1]
julia> f(x) = FHMDDM.expectation_loglikelihood(x, Î³, mpGLM)
julia> gauto = ForwardDiff.gradient(f, concatenatedÎ¸)
julia> maximum(abs.(gauto .- ghand))
```
"""
function expectation_âˆ‡loglikelihood!(âˆ‡Q::GLMÎ¸, Î³::Matrix{<:Vector{<:Real}}, mpGLM::MixturePoissonGLM)
	@unpack Î”t, ğ•, ğ—, ğ² = mpGLM
	@unpack ğ¯ = mpGLM.Î¸
	ğ›š = transformaccumulator(mpGLM)
	dğ›š_db = dtransformaccumulator(mpGLM)
	Î, K = size(Î³)
	T = length(ğ²)
	âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚– = collect(zeros(T) for k=1:K)
	âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–â¨€Ï‰áµ¢ = collect(zeros(T) for k=1:K)
	if âˆ‡Q.fit_b
		âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–â¨€dÏ‰áµ¢_db = collect(zeros(T) for k=1:K)
	end
	@inbounds for i = 1:Î
		for k = 1:K
			ğ‹ = linearpredictor(mpGLM,i,k)
			for t=1:T
				dQáµ¢â‚–_dLáµ¢â‚– = Î³[i,k][t] * differentiate_loglikelihood_wrt_linearpredictor(Î”t, ğ‹[t], ğ²[t])
				âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–[k][t] += dQáµ¢â‚–_dLáµ¢â‚–
				âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–â¨€Ï‰áµ¢[k][t] += dQáµ¢â‚–_dLáµ¢â‚–*ğ›š[i]
				if âˆ‡Q.fit_b
					âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–â¨€dÏ‰áµ¢_db[k][t] += dQáµ¢â‚–_dLáµ¢â‚–*dğ›š_db[i]
				end
			end
		end
	end
	ğ” = @view ğ—[:, 2:1+length(âˆ‡Q.ğ®)]
	âˆ‘áµ¢â‚–_dQáµ¢â‚–_dLáµ¢â‚– = sum(âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–)
    âˆ‡Q.ğ® .= ğ”' * âˆ‘áµ¢â‚–_dQáµ¢â‚–_dLáµ¢â‚–
	@inbounds for k = 2:length(âˆ‡Q.ğ )
		âˆ‡Q.ğ [k] = sum(âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–[k])
	end
	if length(âˆ‡Q.ğ¯) == K
		@inbounds for k = 1:K
			mul!(âˆ‡Q.ğ¯[k], ğ•', âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–â¨€Ï‰áµ¢[k])
		end
	else
		mul!(âˆ‡Q.ğ¯[1], ğ•', sum(âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–â¨€Ï‰áµ¢))
	end
	if âˆ‡Q.fit_b
		if length(âˆ‡Q.ğ¯) == K
			âˆ‡Q.b[1] = 0.0
			@inbounds for k = 1:K
				âˆ‡Q.b[1] += dot(âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–â¨€dÏ‰áµ¢_db[k], ğ•, ğ¯[k])
			end
		else
			âˆ‡Q.b[1] = dot(sum(âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–â¨€dÏ‰áµ¢_db), ğ•, ğ¯[k])
		end
	end
	return nothing
end

"""
    expectation_of_loglikelihood(Î³, mpGLM, x)

ForwardDiff-compatible computation of the expectation of the log-likelihood of the mixture of Poisson generalized model of one neuron

Ignores the log(y!) term, which does not depend on the parameters

ARGUMENT
-`Î³`: posterior probability of the latent variable
-`mpGLM`: the GLM of one neuron
-`x`: weights of the linear filters of the GLM concatenated as a vector of floating-point numbers

RETURN
-expectation of the log-likelihood of the spike train of one neuron
"""
function expectation_of_loglikelihood(Î³::Matrix{<:Vector{<:AbstractFloat}},
									   mpGLM::MixturePoissonGLM,
									   x::Vector{<:Real};
										omitb::Bool=false)
	mpGLM = MixturePoissonGLM(x, mpGLM; omitb=omitb)
    @unpack Î”t, ğ² = mpGLM
    T = length(ğ²)
    Î,K = size(Î³)
    Q = 0.0
    @inbounds for i = 1:Î
	    for k = 1:K
			ğ‹ = linearpredictor(mpGLM,i,k)
            for t = 1:T
				Q += Î³[i,k][t]*poissonloglikelihood(Î”t, ğ‹[t], ğ²[t])
            end
        end
    end
    return Q
end

"""
	âˆ‡negativeloglikelihood!(âˆ‡nâ„“, âˆ‡â„“glms, offset)

Concatenate the first-order partial derivatives of the model's log-likelihood w.r.t. to the parameters in each neuron's GLM

MODIFIED ARGUMENT
-`âˆ‡nâ„“`: a vector representing the gradient of the model's log-likelihood

UNMODIFIED ARGUMENT
-`âˆ‡â„“glm`: a nested vector of the partial derivatives of the model's log-likelihood w.r.t. to the  parameter of each neuron's mixture of Poisson GLM. Element `âˆ‡â„“glms[i][n]` corresponds to the n-th neuron in the i-th trialset
-`offset`: the number of elements at beginning of `âˆ‡nâ„“` that are unrelated to the GLM's
"""
function âˆ‡negativeloglikelihood!(âˆ‡nâ„“::Vector{<:Real}, âˆ‡â„“glm::Vector{<:Vector{<:GLMÎ¸}}, offset::Integer)
	counter = offset
	for âˆ‡â„“glm in âˆ‡â„“glm
		for âˆ‡â„“glm in âˆ‡â„“glm
			if âˆ‡â„“glm.fit_b
				counter+=1
				âˆ‡nâ„“[counter] = -âˆ‡â„“glm.b[1]
			end
			for k = 2:length(âˆ‡â„“glm.ğ )
				counter+=1
				âˆ‡nâ„“[counter] = -âˆ‡â„“glm.ğ [k]
			end
			for u in âˆ‡â„“glm.ğ®
				counter+=1
				âˆ‡nâ„“[counter] = -u
			end
			for ğ¯â‚– in âˆ‡â„“glm.ğ¯
				for v in ğ¯â‚–
					counter+=1
					âˆ‡nâ„“[counter] = -v
				end
			end
		end
	end
	return nothing
end
