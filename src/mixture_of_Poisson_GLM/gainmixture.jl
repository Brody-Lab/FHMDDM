"""
	loglikelihood(glm::GainMixtureGLM, kfold)

Out-of-sample log-likelihood of a mixture of gain GLM

ARGUMENT
-`glm`: an object containing the data and parameters of a mixture of gain GLM
-`kfold`: number of cross-validation folds
"""
function loglikelihood(glm::GainMixtureGLM, kfold::Integer)
    testindices, trainindices = cvpartition(kfold, size(glm.ğ—,1))
    â„“ = 0
    for k = 1:kfold
		trainingglm = GainMixtureGLM(Î”t=glm.Î”t, ğ—=glm.ğ—[trainindices[k],:], ğ²=glm.ğ²[trainindices[k]])
		maximizelikelihood!(trainingglm)
		testglm = GainMixtureGLM(Î”t=glm.Î”t, ğ—=glm.ğ—[testindices[k],:], ğ²=glm.ğ²[testindices[k]])
		for parameter in (:ğ , :Ï€, :ğ®)
			getfield(testglm, parameter) .= getfield(trainingglm, parameter)
		end
		â„“ += loglikelihood(testglm)
    end
	return â„“
end

"""
    maximizelikelihood!(glm::GainMixtureGLM)

Fit a mixture of gain Poisson generalized linear model (GLM)

ARGUMENT
-`glm`: an object containing the data and parameters of a mixture of gain GLM

OPTIONAL ARGUMENT
-`iterations`: maximum number of iterations in the expectation maximization algorithm, and within each M-step, maximum number of iterations within the Newton with trust region algorithm
-`nstarts`: number of repeated optimizations from different initial values

EXAMPLE
```julia-repl
julia> using FHMDDM, Random, Distributions
julia> Random.seed!(1234);
julia> Î”t, Ï€â‚€, T = 0.01, 0.5, 10000;
julia> ğ â‚€ = [50.0, 0.0];
julia> ğ®â‚€ = [1.0, 1.0]
julia> ğ† = fill(1.0, T);
julia> t = collect(1:T)./T*2Ï€
julia> ğ” = 10 .*hcat(sin.(t), cos.(t))
julia> ğ— = hcat(ğ†, ğ”)
julia> ğ‹ = collect(ğ—*vcat(g,ğ®â‚€) for g in ğ â‚€);
julia> ğ›Œ = collect(rand() < Ï€â‚€ ? FHMDDM.inverselink(ğ‹[1][t]) : FHMDDM.inverselink(ğ‹[2][t]) for t=1:T)
julia> ğ² = collect(rand(Poisson(Î»*Î”t)) for Î» in ğ›Œ)
julia> glm = FHMDDM.GainMixtureGLM(Î”t=Î”t, ğ—=ğ—, ğ²=ğ²)
julia> FHMDDM.maximizelikelihood!(glm)
julia> â„“ = FHMDDM.loglikelihood(glm)
julia> glm.ğ , glm.ğ®, glm.Ï€
julia> basicglm = FHMDDM.PoissonGLM(Î”t=Î”t, ğ—=ğ—, ğ²=ğ²)
julia> FHMDDM.maximizelikelihood!(basicglm)
julia> â„“basic = FHMDDM.loglikelihood(basicglm)

julia> testâ„“mixture = FHMDDM.loglikelihood(glm,5)
julia> testâ„“basic = FHMDDM.loglikelihood(basicglm,5)

```
"""
function maximizelikelihood!(glm::GainMixtureGLM; iterations::Integer=20, nstarts::Integer=10, Ï€threshold::Real=1e-3)
	basicglm = PoissonGLM(Î”t=glm.Î”t, ğ—=glm.ğ—, ğ²=glm.ğ²)
	maximizelikelihood!(basicglm)
	bestğ  = fill(NaN,glm.nğ )
	bestğ® = fill(NaN,glm.nğ®)
	bestâ„“ = -Inf
	bestÏ€ = NaN
	for s = 1:nstarts
		glm.ğ  .= basicglm.ğ°[1].*(3 .*rand(2) .- 1)
		glm.ğ® .= basicglm.ğ°[2:end]
	    glm.Ï€[1] = rand()
	    for i = 1:iterations
	        posteriors!(glm; Ï€threshold=Ï€threshold)
	        âˆ‘ğ›„ = map(sum, glm.ğ›„)
	        glm.Ï€[1] = âˆ‘ğ›„[1]/sum(âˆ‘ğ›„)
			if (glm.Ï€[1] <= Ï€threshold) || (glm.Ï€[1] >= 1-Ï€threshold)
				break
			end
	        maximizeECLL!(glm; iterations=iterations)
	    end
		â„“ = loglikelihood(glm)
		if â„“ > bestâ„“
			bestğ  .= glm.ğ 
			bestğ® .= glm.ğ®
			bestÏ€ = glm.Ï€[1]
			bestâ„“ = â„“
		end
	end
	glm.ğ  .= bestğ 
	glm.ğ® .= bestğ®
	glm.Ï€[1] = bestÏ€
	â„“basic = loglikelihood(basicglm)
	if bestâ„“ < â„“basic
		@debug "The in-sample log-likelihood of the mixture-of-gain GLM is lower that of a GLM without any mixture" bestâ„“ â„“basic
	end
	return nothing
end

"""
	loglikelihood(glm::GainMixtureGLM)

Log-likelihood of the parameters of a gain mixture GLM
"""
function loglikelihood(glm::GainMixtureGLM)
	@unpack Î”t, ğ , Ï€, ğ®, ğ—, ğ² = glm
	ğ‹ = collect(ğ—*vcat(g,ğ®) for g in ğ )
	â„“ = 0
	for i in eachindex(ğ²)
		p = glm.Ï€[1]*poissonlikelihood(Î”t, ğ‹[1][i], ğ²[i]) + (1-glm.Ï€[1])*poissonlikelihood(Î”t, ğ‹[2][i], ğ²[i])
		â„“ += log(p)
	end
	return â„“
end

"""
	posteriors!(glm::GainMixtureGLM)

Posterior probability of the gain state
"""
function posteriors!(glm::GainMixtureGLM; Ï€threshold::Real=1e-3)
	@unpack Î”t, ğ , ğ›„, ğ—, Ï€, ğ®, ğ² = glm
	ğ‹ = collect(ğ—*vcat(g,ğ®) for g in ğ )
	if Ï€[1] < Ï€threshold
		ğ›„[1] .= Ï€threshold
		ğ›„[2] .= 1.0-Ï€threshold
	elseif Ï€[1] > 1.0-Ï€threshold
		ğ›„[1] .= 1.0-Ï€threshold
		ğ›„[2] .= Ï€threshold
	else
	    for t = eachindex(ğ²)
	        py_c1 = poissonlikelihood(Î”t,ğ‹[1][t],ğ²[t])
	        py_c2 = poissonlikelihood(Î”t,ğ‹[2][t],ğ²[t])
	        pyc1 = Ï€[1]*py_c1
	        pyc2 = (1-Ï€[1])*py_c2
	        py = pyc1+pyc2
	        ğ›„[1][t] = pyc1/py
	        ğ›„[2][t] = pyc2/py
	    end
	end
    return nothing
end

"""
	maximizeECLL!(glm)

Update weights by maximizing the expectation of the conditional log-likelihood

MODIFIED ARGUMENT
-`glm`: a structure containing the glm of the mixture of gain generalized linear model
"""
function maximizeECLL!(glm::GainMixtureGLM; iterations::Integer=20)
	@unpack Î”t, ğ†, ğ , ğ›„, nğ , ğ”, ğ®, ğ² = glm
    f(ğ°) = negativeECLL!(glm, ğ°)
    âˆ‡f!(âˆ‡, ğ°) = âˆ‡negativeECLL!(âˆ‡, glm, ğ°)
    âˆ‡âˆ‡f!(âˆ‡âˆ‡, ğ°) = âˆ‡âˆ‡negativeECLL!(âˆ‡âˆ‡, glm, ğ°)
    results = Optim.optimize(f, âˆ‡f!, âˆ‡âˆ‡f!, vcat(ğ ,ğ®), NewtonTrustRegion(), Optim.Options(iterations=iterations))
	ğ°mle = Optim.minimizer(results)
	ğ  .= ğ°mle[1:nğ ]
	ğ® .= ğ°mle[nğ +1:end]
	return nothing
end

"""
	negativeECLL!(glm,ğ°)

Negative of the expectaton of the conditional log-likelihood
"""
function negativeECLL!(glm::GainMixtureGLM, ğ°::Vector{<:Real})
	update!(glm, ğ°)
	return -glm.Q[1]
end

"""
	âˆ‡negativeECLL!(âˆ‡, glm,ğ°)

Gradient of the negative of the expectaton of the conditional log-likelihood
"""
function âˆ‡negativeECLL!(âˆ‡::Vector{<:Real}, glm::GainMixtureGLM, ğ°::Vector{<:Real})
	update!(glm, ğ°)
	for i in eachindex(âˆ‡)
		âˆ‡[i] = -glm.âˆ‡Q[i]
	end
	return nothing
end

"""
	âˆ‡âˆ‡negativeECLL!(âˆ‡, glm,ğ°)

Hessian of the negative of the expectaton of the conditional log-likelihood
"""
function âˆ‡âˆ‡negativeECLL!(âˆ‡âˆ‡::Matrix{<:Real}, glm::GainMixtureGLM, ğ°::Vector{<:Real})
	update!(glm, ğ°)
	for i in eachindex(âˆ‡âˆ‡)
		âˆ‡âˆ‡[i] = -glm.âˆ‡âˆ‡Q[i]
	end
	return nothing
end

"""
	update!(glm, ğ°)

update the parameters and glm of the gain mixture GLM
"""
function update!(glm::GainMixtureGLM, ğ°::Vector{<:Real})
	if (ğ° != vcat(glm.ğ , glm.ğ®)) || isnan(glm.Q[1])
		glm.ğ  .= ğ°[1:glm.nğ ]
		glm.ğ® .= ğ°[glm.nğ +1:end]
		computederivatives!(glm)
	end
end

"""
	computederivatives!(glm)

Compute the expectation of the conditional log-likelihood and its glm

MODIFIED ARGUMENT
-`glm`: a structure containing the glm of the mixture of gain generalized linear model
"""
function computederivatives!(glm::GainMixtureGLM)
	@unpack Î”t, ğ†, ğ , ğ›„, âˆ‡Q, âˆ‡âˆ‡Q, nğ , ntimesteps, nğ®, Q, ğ—, ğ”, ğ®, ğ², Î³â‚–dâ„“_dLâ‚–, Î³â‚–dÂ²â„“_dLâ‚–Â² = glm
	Q[1] = 0.0
	âˆ‡Q .= 0.0
	âˆ‡âˆ‡Q .= 0.0
	@inbounds for k = 1:nğ 
		ğ°â‚– = vcat(ğ [k], ğ®)
		ğ‹â‚– = ğ—*ğ°â‚–
		ğ¥â‚– = ğ‹â‚–
		for t=1:ntimesteps
			Î» = inverselink(ğ‹â‚–[t])
			dÂ²â„“_dLâ‚–Â², dâ„“_dLâ‚– = differentiate_twice_loglikelihood_wrt_linearpredictor(Î”t, ğ‹â‚–[t], Î», ğ²[t])
			ğ¥â‚–[t] = poissonloglikelihood(Î»*Î”t, ğ²[t])
			Î³â‚–dâ„“_dLâ‚–[k][t] = ğ›„[k][t]*dâ„“_dLâ‚–
			Î³â‚–dÂ²â„“_dLâ‚–Â²[k][t] = ğ›„[k][t]*dÂ²â„“_dLâ‚–Â²
		end
		Q[1] += dot(ğ›„[k], ğ¥â‚–)
	end
	ğ”áµ€ = transpose(ğ”)
	for k = 1:nğ 
		âˆ‡Q[k] += dot(Î³â‚–dâ„“_dLâ‚–[k], ğ†)
		x = Î³â‚–dÂ²â„“_dLâ‚–Â²[k].*ğ†
		âˆ‡âˆ‡Q[k,k] += dot(ğ†, x)
		âˆ‡Q[nğ +1:end] .+= ğ”áµ€*Î³â‚–dâ„“_dLâ‚–[k]
		âˆ‡âˆ‡Q[k,nğ +1:end] .+= ğ”áµ€*x
		âˆ‡âˆ‡Q[nğ +1:end,nğ +1:end] .+= ğ”áµ€*(Î³â‚–dÂ²â„“_dLâ‚–Â²[k].*ğ”)
	end
	for i = 1:size(âˆ‡âˆ‡Q,1)
		for j = i+1:size(âˆ‡âˆ‡Q,2)
			âˆ‡âˆ‡Q[j,i] = âˆ‡âˆ‡Q[i,j]
		end
	end
	return nothing
end

"""
	negativeECLL(glm)

ForwardDiff-compatible computation of the negative expectation of the conditional log-likelihood

ARGUMENT
-`glm`: a structure containing the glm of the mixture of gain generalized linear model

EXAMPLE
```julia-repl
julia> using FHMDDM, Random, Distributions, LogExpFunctions, ForwardDiff, LinearAlgebra
julia> Random.seed!(1234);
julia> Î”t, Ï€, T, nğ® = 0.01, 0.75, 1000, 5;
julia> ğ  = [100.0, -100.0];
julia> ğ® = rand(nğ®);
julia> ğ† = rand(T);
julia> ğ” = svd(rand(T,nğ®)).U.*10
julia> ğ— = hcat(ğ†, ğ”)
julia> ğ‹ = collect(ğ—*vcat(g,ğ®) for g in ğ );
julia> ğ›Œ = collect(rand() < Ï€ ? softplus(ğ‹[1][t]) : softplus(ğ‹[2][t]) for t=1:T);
julia> ğ² = collect(rand(Poisson(Î»*Î”t)) for Î» in ğ›Œ)
julia> glm = FHMDDM.GainMixtureGLM(Î”t=Î”t, ğ =rand(length(ğ )), Ï€=rand(1), ğ®=rand(length(ğ®)), ğ—=ğ—, ğ²=ğ²);
julia> ğ°â‚€ = vcat(glm.ğ , glm.ğ®);
julia> FHMDDM.posteriors!(glm)
julia> FHMDDM.computederivatives!(glm)
julia> f(ğ°) = FHMDDM.negativeECLL(glm, ğ°);
julia> fauto = f(ğ°â‚€);
julia> gauto = ForwardDiff.gradient(f, ğ°â‚€);
julia> hauto = ForwardDiff.hessian(f, ğ°â‚€);
julia> absÎ”Q = abs(fauto - glm.Q[1])
julia> maxabsÎ”âˆ‡Q = maximum(abs.(gauto .- glm.âˆ‡Q))
julia> maxabsÎ”âˆ‡âˆ‡Q = maximum(abs.(hauto .- glm.âˆ‡âˆ‡Q))
julia> println("   |Î”Q|: ", absÎ”Q)
julia> println("   max(|Î”gradient|): ", maxabsÎ”âˆ‡Q)
julia> println("   max(|Î”hessian|): ", maxabsÎ”âˆ‡âˆ‡Q)
```
"""
function negativeECLL(glm::GainMixtureGLM, ğ°::Vector{<:Real})
	@unpack Î”t, ğ›„, nğ , ntimesteps, ğ—, ğ² = glm
	ğ  = ğ°[1:nğ ]
	ğ® = ğ°[nğ +1:end]
	Q = 0
	@inbounds for k = 1:nğ 
		ğ°â‚– = vcat(ğ [k], ğ®)
		ğ‹â‚– = ğ—*ğ°â‚–
		ğ¥â‚– = ğ‹â‚–
		for t=1:ntimesteps
			ğ¥â‚–[t] = poissonloglikelihood(Î”t, ğ‹â‚–[t], ğ²[t])
		end
		Q -= dot(ğ›„[k], ğ¥â‚–)
	end
	return -Q
end
