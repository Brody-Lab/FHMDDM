"""
	check_twopasshessian(model)

Compare the automatically computed and hand-coded gradients and hessians with respect to the parameters being fitted in their real space

ARGUMENT
-`model`: a structure containing the data, parameters, and hyperparameters of a factorial hidden-Markov drift-diffusion model

RETURN
-`absdiffâ„“`: absolute difference in the log-likelihood evaluted using the algorithm bein automatically differentiated and the hand-coded algorithm
-`absdiffâˆ‡`: absolute difference in the gradients
-`absdiffâˆ‡âˆ‡`: absolute difference in the hessians

EXAMPLE
```julia-repl
julia> using FHMDDM
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_05_19_test/T176_2018_05_03/data.mat")
julia> absdiffâ„“, absdiffâˆ‡, absdiffâˆ‡âˆ‡ = FHMDDM.check_twopasshessian(model)
```
"""
function check_twopasshessian(model::Model)
	concatenatedÎ¸, indexÎ¸ = concatenateparameters(model)
	â„“hand, âˆ‡hand, âˆ‡âˆ‡hand = twopasshessian!(model,concatenatedÎ¸,indexÎ¸)
	f(x) = loglikelihood(x, indexÎ¸, model)
	â„“auto = f(concatenatedÎ¸)
	âˆ‡auto = ForwardDiff.gradient(f, concatenatedÎ¸)
	âˆ‡âˆ‡auto = ForwardDiff.hessian(f, concatenatedÎ¸)
	return abs(â„“auto-â„“hand), abs.(âˆ‡auto .- âˆ‡hand), abs.(âˆ‡âˆ‡auto .- âˆ‡âˆ‡hand)
end

"""
	twopasshessian!(model, concatenatedÎ¸, indexÎ¸)

Sort a vector of parameters and compute the log-likelihood, its gradient, and its hessian

ARGUMENT
-`model`: a structure containing the data and hyperparameters of the factorial hidden drift-diffusion model
-`concatenatedÎ¸`: values of the parameters in real space concatenated as a vector
-`indexÎ¸`: a structure for sorting (i.e., un-concatenating) the parameters

RETURN
-`â„“`: log-likelihood
-`âˆ‡â„“`: gradient of the log-likelihood with respect to fitted parameters in real space
-`âˆ‡âˆ‡â„“`: Hessian matrix of the log-likelihood with respect to fitted parameters in real space

EXAMPLE
```julia-repl
julia> using FHMDDM
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_05_05_test/data.mat"; randomize=true)
julia> concatenatedÎ¸, indexÎ¸ = FHMDDM.concatenateparameters(model)
julia> â„“, âˆ‡â„“, âˆ‡âˆ‡â„“ = FHMDDM.twopasshessian!(model, concatenatedÎ¸, indexÎ¸)
```
"""
function twopasshessian!(model::Model, concatenatedÎ¸::Vector{<:Real}, indexÎ¸::IndexÎ¸)
	sortparameters!(model, concatenatedÎ¸, indexÎ¸)
	â„“, âˆ‡â„“, âˆ‡âˆ‡â„“ = twopasshessian(model)
	native2real!(âˆ‡â„“, âˆ‡âˆ‡â„“, model)
	âˆ‡â„“ = sortparameters(indexÎ¸.latentÎ¸, âˆ‡â„“)
	âˆ‡âˆ‡â„“ = sortparameters(indexÎ¸.latentÎ¸, âˆ‡âˆ‡â„“)
	return â„“, âˆ‡â„“, âˆ‡âˆ‡â„“
end

"""
	twopasshessian(model)

Compute the hessian as the Jacobian of the expectation conjugate gradient

ARGUMENT
-`model`: a structure containing the parameters, data, and hyperparameters of a factorial hidden Markov drift-diffusion model

RETURN
-`â„“`: log-likelihood
-`âˆ‡â„“`: gradient of the log-likelihood with respect to fitted parameters in real space
-`âˆ‡âˆ‡â„“`: Hessian matrix of the log-likelihood with respect to fitted parameters in real space

EXAMPLE
```julia-repl
julia> using FHMDDM
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_05_05_test/data.mat")
julia> â„“, âˆ‡â„“, âˆ‡âˆ‡â„“ = FHMDDM.twopasshessian(model)
```
"""
function twopasshessian(model::Model)
	@unpack trialsets = model
	sameacrosstrials = Sameacrosstrials(model)
	memoryforhessian = Memoryforhessian(model, sameacrosstrials)
	@inbounds for trialsetindex in eachindex(trialsets)
		ğ‹ = linearpredictor(trialsets[trialsetindex].mpGLMs)
		offset = 0
		for trialindex in eachindex(trialsets[trialsetindex].trials)
			twopasshessian!(memoryforhessian, ğ‹, model, sameacrosstrials, offset, trialindex, trialsetindex)
			offset+=model.trialsets[trialsetindex].trials[trialindex].ntimesteps
		end
	end
	@unpack â„“, âˆ‡â„“, âˆ‡âˆ‡â„“ = memoryforhessian
	@inbounds for i = 1:size(âˆ‡âˆ‡â„“,1)
		for j = i+1:size(âˆ‡âˆ‡â„“,2)
			âˆ‡âˆ‡â„“[j,i] = âˆ‡âˆ‡â„“[i,j]
		end
	end
	return â„“[1], âˆ‡â„“, âˆ‡âˆ‡â„“
end

"""
	twopasshessian!

Compute the hessian for one trial as the Jacobian of the expectation conjugate gradient

MODIFIED ARGUMENT
-`memoryforhessian`: a structure containing quantities used in each trial

UNMODIFIED ARGUMENT
-`ğ‹`: a nested array whose element ğ‹[n][j,k][t] corresponds to n-th neuron, j-th accumualtor state, k-th coupling state, and the t-th time bin in the trialset
"""
function twopasshessian!(memoryforhessian::Memoryforhessian,
						 ğ‹::Vector{<:Matrix{<:Vector{<:Real}}},
						 model::Model,
						 sameacrosstrials::Sameacrosstrials,
						 offset::Integer,
						 trialindex::Integer,
						 trialsetindex::Integer)
	trial = model.trialsets[trialsetindex].trials[trialindex]
	@unpack mpGLMs = model.trialsets[trialsetindex]
	@unpack clicks = trial
	@unpack Î¸native = model
	@unpack â„“, âˆ‡â„“, âˆ‡âˆ‡â„“, f, âˆ‡f, D, âˆ‡D, âˆ‡b = memoryforhessian
	@unpack P, âˆ‡paâ‚, âˆ‡âˆ‡paâ‚, Aáµƒinput, âˆ‡Aáµƒinput, âˆ‡âˆ‡Aáµƒinput = memoryforhessian
	@unpack Î», âˆ‡logpy, âˆ‡âˆ‡logpy, pY, âˆ‡pY, âˆ‚pYğ‘‘_âˆ‚Ïˆ = memoryforhessian
	@unpack dğ›_dB, Î”t, K, Î = sameacrosstrials
	@unpack Aáµƒsilent, âˆ‡Aáµƒsilent, âˆ‡âˆ‡Aáµƒsilent, Aá¶œ, Aá¶œáµ€, âˆ‡Aá¶œ, âˆ‡Aá¶œáµ€, Ï€á¶œ, Ï€á¶œáµ€, âˆ‡Ï€á¶œ, âˆ‡Ï€á¶œáµ€ = sameacrosstrials
	@unpack indexÎ¸_paâ‚, indexÎ¸_paâ‚œaâ‚œâ‚‹â‚, indexÎ¸_paâ‚œaâ‚œâ‚‹â‚only, indexÎ¸_pcâ‚, indexÎ¸_pcâ‚œcâ‚œâ‚‹â‚, indexÎ¸_Ïˆ,  nÎ¸_paâ‚, nÎ¸_paâ‚œaâ‚œâ‚‹â‚, nÎ¸_pcâ‚, nÎ¸_pcâ‚œcâ‚œâ‚‹â‚, nÎ¸_Ïˆ, index_paâ‚_in_Î¸, index_paâ‚œaâ‚œâ‚‹â‚_in_Î¸, index_pcâ‚_in_Î¸, index_pcâ‚œcâ‚œâ‚‹â‚_in_Î¸, index_Ïˆ_in_Î¸ = sameacrosstrials
	indexÎ¸_py = sameacrosstrials.indexÎ¸_py[trialsetindex]
	nÎ¸_py = sameacrosstrials.nÎ¸_py[trialsetindex]
	indexÎ¸_pY = sameacrosstrials.indexÎ¸_pY[trialsetindex]
	nÎ¸_pY = sameacrosstrials.nÎ¸_pY[trialsetindex]
	index_pY_in_Î¸ = sameacrosstrials.index_pY_in_Î¸[trialsetindex]
	indexÎ¸_trialset = sameacrosstrials.indexÎ¸_trialset[trialsetindex]
	nÎ¸_trialset = sameacrosstrials.nÎ¸_trialset[trialsetindex]
	adaptedclicks = âˆ‡âˆ‡adapt(clicks, Î¸native.k[1], Î¸native.Ï•[1])
	update_emissions!(Î», âˆ‡logpy, âˆ‡âˆ‡logpy, pY, âˆ‡pY, Î”t, ğ‹, mpGLMs, trial.ntimesteps, offset)
	update_emissions!(âˆ‚pYğ‘‘_âˆ‚Ïˆ, pY[trial.ntimesteps], âˆ‡pY[trial.ntimesteps], trial.choice, Î¸native.Ïˆ[1])
	@inbounds for q in eachindex(âˆ‡f[1])
		âˆ‡f[1][q] .= 0
	end
	âˆ‡âˆ‡priorprobability!(âˆ‡âˆ‡paâ‚, âˆ‡paâ‚, P, trial.previousanswer)
	paâ‚ = copy(P.ğ›‘)
	pYâ‚â¨€pcâ‚ = pY[1] .* Ï€á¶œáµ€
	paâ‚â¨€pcâ‚ = paâ‚ .* Ï€á¶œáµ€
	@inbounds for i = 1:nÎ¸_pY
		for j=1:Î
			for k = 1:K
				âˆ‡f[1][indexÎ¸_pY[i]][j,k] = âˆ‡pY[1][i][j,k]*paâ‚â¨€pcâ‚[j,k]
			end
		end
	end
	@inbounds for i = 1:nÎ¸_paâ‚
		for j=1:Î
			for k = 1:K
				âˆ‡f[1][indexÎ¸_paâ‚[i]][j,k] = pYâ‚â¨€pcâ‚[j,k]*âˆ‡paâ‚[i][j]
			end
		end
	end
	@inbounds for i = 1:nÎ¸_pcâ‚
		for j=1:Î
			for k = 1:K
				âˆ‡f[1][indexÎ¸_pcâ‚[1]][j,k] = pY[1][j,k]*paâ‚[j]*âˆ‡Ï€á¶œ[i][k]
			end
		end
	end
	@inbounds for j=1:Î
		for k = 1:K
			f[1][j,k] = pY[1][j,k] * paâ‚â¨€pcâ‚[j,k]
		end
	end
	D[1] = sum(f[1])
	forward!(âˆ‡D[1], f[1], âˆ‡f[1], â„“, D[1])
	@inbounds for t=2:trial.ntimesteps
		if t âˆˆ clicks.inputtimesteps
			update_for_âˆ‡âˆ‡transition_probabilities!(P, adaptedclicks, clicks, t)
			clickindex = clicks.inputindex[t][1]
			Aáµƒ = Aáµƒinput[clickindex]
			âˆ‡Aáµƒ = âˆ‡Aáµƒinput[clickindex]
			âˆ‡âˆ‡Aáµƒ = âˆ‡âˆ‡Aáµƒinput[clickindex]
			âˆ‡âˆ‡transitionmatrix!(âˆ‡âˆ‡Aáµƒ, âˆ‡Aáµƒ, Aáµƒ, P)
		else
			Aáµƒ = Aáµƒsilent
			âˆ‡Aáµƒ = âˆ‡Aáµƒsilent
			âˆ‡âˆ‡Aáµƒ = âˆ‡âˆ‡Aáµƒsilent
		end
		fâ¨‰Aá¶œáµ€ = f[t-1] * Aá¶œáµ€
		Aáµƒâ¨‰fâ¨‰Aá¶œáµ€ = Aáµƒ * fâ¨‰Aá¶œáµ€
		Aáµƒâ¨‰f = Aáµƒ * f[t-1]
		for q in indexÎ¸_trialset
			i_aâ‚œ = index_paâ‚œaâ‚œâ‚‹â‚_in_Î¸[q]
			i_câ‚œ = index_pcâ‚œcâ‚œâ‚‹â‚_in_Î¸[q]
			i_y = index_pY_in_Î¸[q]
			i_Ïˆ = index_Ïˆ_in_Î¸[q]
			if i_aâ‚œ > 0
				âˆ‡f[t][q] = pY[t] .* (âˆ‡Aáµƒ[i_aâ‚œ] * fâ¨‰Aá¶œáµ€ .+ Aáµƒ * âˆ‡f[t-1][q] * Aá¶œáµ€)
			elseif i_câ‚œ > 0
				âˆ‡f[t][q] = pY[t] .* (Aáµƒâ¨‰f * âˆ‡Aá¶œáµ€[i_câ‚œ] .+ Aáµƒ * âˆ‡f[t-1][q] * Aá¶œáµ€)
			elseif i_y > 0
				âˆ‡f[t][q] = âˆ‡pY[t][i_y] .* Aáµƒâ¨‰fâ¨‰Aá¶œáµ€ .+ pY[t] .* (Aáµƒ * âˆ‡f[t-1][q] * Aá¶œáµ€)
			elseif i_Ïˆ > 0
				if t==trial.ntimesteps
					âˆ‡f[t][q] = âˆ‚pYğ‘‘_âˆ‚Ïˆ .* Aáµƒâ¨‰fâ¨‰Aá¶œáµ€
				else
					âˆ‡f[t][q] .= 0.0
				end
			else
				âˆ‡f[t][q] = pY[t] .* (Aáµƒ * âˆ‡f[t-1][q] * Aá¶œáµ€)
			end
		end
		f[t] = pY[t] .* Aáµƒâ¨‰fâ¨‰Aá¶œáµ€
		D[t] = sum(f[t])
		forward!(âˆ‡D[t], f[t], âˆ‡f[t], â„“, D[t])
	end
	bâ‚œ = ones(Î,K)
	@inbounds for t = trial.ntimesteps:-1:1
		Î³ = f[t] # resuse memory
		âˆ‡Î³ = âˆ‡f[trial.ntimesteps] # resuse memory
		if t == trial.ntimesteps
			for q in indexÎ¸_trialset
				âˆ‡b[q] .= 0
			end
			# the p(ğ‘‘ âˆ£ aâ‚œ, câ‚œ) term
			q = indexÎ¸_Ïˆ[1]
			âˆ‡â„“[q] += expectation_derivative_logpğ‘‘_wrt_Ïˆ(trial.choice, Î³, Î¸native.Ïˆ[1])
			âˆ‡âˆ‡â„“[q,q] += expectation_second_derivative_logpğ‘‘_wrt_Ïˆ(trial.choice, Î³, Î¸native.Ïˆ[1])
			for r in indexÎ¸_trialset
				if r < q
					âˆ‡âˆ‡â„“[r,q] += expectation_derivative_logpğ‘‘_wrt_Ïˆ(trial.choice, âˆ‡Î³[r], Î¸native.Ïˆ[1])
				else
					âˆ‡âˆ‡â„“[q,r] += expectation_derivative_logpğ‘‘_wrt_Ïˆ(trial.choice, âˆ‡Î³[r], Î¸native.Ïˆ[1])
				end
			end
		else
			if t+1 âˆˆ clicks.inputtimesteps
				clickindex = clicks.inputindex[t+1][1]
				Aáµƒâ‚œâ‚Šâ‚ = Aáµƒinput[clickindex]
				âˆ‡Aáµƒâ‚œâ‚Šâ‚ = âˆ‡Aáµƒinput[clickindex]
			else
				Aáµƒâ‚œâ‚Šâ‚ = Aáµƒsilent
				âˆ‡Aáµƒâ‚œâ‚Šâ‚ = âˆ‡Aáµƒsilent
			end
			Aáµƒâ‚œâ‚Šâ‚áµ€ = transpose(Aáµƒâ‚œâ‚Šâ‚)
			bâ‚œâ‚Šâ‚ = bâ‚œ # rename
			bâ‚œâ‚Šâ‚â¨€pYâ‚œâ‚Šâ‚ = bâ‚œâ‚Šâ‚ .* pY[t+1]
			bâ‚œ = transpose(Aáµƒâ‚œâ‚Šâ‚) * (bâ‚œâ‚Šâ‚â¨€pYâ‚œâ‚Šâ‚./D[t+1]) * Aá¶œ
			for q in indexÎ¸_trialset
				i_aâ‚œ = index_paâ‚œaâ‚œâ‚‹â‚_in_Î¸[q]
				i_câ‚œ = index_pcâ‚œcâ‚œâ‚‹â‚_in_Î¸[q]
				i_y = index_pY_in_Î¸[q]
				i_Ïˆ = index_Ïˆ_in_Î¸[q]
				if i_aâ‚œ > 0
					dAáµƒâ‚œâ‚Šâ‚áµ€_dÎ¸ = transpose(âˆ‡Aáµƒâ‚œâ‚Šâ‚[i_aâ‚œ])
					âˆ‡b[q] = ((dAáµƒâ‚œâ‚Šâ‚áµ€_dÎ¸*bâ‚œâ‚Šâ‚â¨€pYâ‚œâ‚Šâ‚ .+ Aáµƒâ‚œâ‚Šâ‚áµ€*(âˆ‡b[q].*pY[t+1])) * Aá¶œ .- bâ‚œ.*âˆ‡D[t+1][q])./D[t+1]
				elseif i_câ‚œ > 0
					âˆ‡b[q] = (Aáµƒâ‚œâ‚Šâ‚áµ€ * (bâ‚œâ‚Šâ‚â¨€pYâ‚œâ‚Šâ‚*âˆ‡Aá¶œ[i_câ‚œ] .+ (âˆ‡b[q].*pY[t+1]) * Aá¶œ) .- bâ‚œ.*âˆ‡D[t+1][q])./D[t+1]
				elseif i_y > 0
					âˆ‡b[q] = (Aáµƒâ‚œâ‚Šâ‚áµ€ * (bâ‚œâ‚Šâ‚.*âˆ‡pY[t+1][i_y] .+ (âˆ‡b[q].*pY[t+1])) * Aá¶œ .- bâ‚œ.*âˆ‡D[t+1][q])./D[t+1]
				elseif i_Ïˆ > 0
					if t == trial.ntimesteps-1
						âˆ‡b[q] = (Aáµƒâ‚œâ‚Šâ‚áµ€ * (bâ‚œâ‚Šâ‚.*âˆ‚pYğ‘‘_âˆ‚Ïˆ) * Aá¶œ .- bâ‚œ.*âˆ‡D[t+1][q])./D[t+1]
					end
				else
					âˆ‡b[q] = (Aáµƒâ‚œâ‚Šâ‚áµ€*(âˆ‡b[q].*pY[t+1])*Aá¶œ .- bâ‚œ.*âˆ‡D[t+1][q])./D[t+1]
				end
			end
			for q in indexÎ¸_trialset
				for ij in eachindex(âˆ‡Î³[q])
					âˆ‡Î³[q][ij] = âˆ‡f[t][q][ij]*bâ‚œ[ij] + f[t][ij]*âˆ‡b[q][ij]
				end
			end
			Î³ .*= bâ‚œ # modify Î³ only after modifying âˆ‡Î³ because Î³ shares memory with f[t]
		end
		if t > 1
			if t âˆˆ clicks.inputtimesteps
				clickindex = clicks.inputindex[t][1]
				Aáµƒ = Aáµƒinput[clickindex]
				âˆ‡Aáµƒ = âˆ‡Aáµƒinput[clickindex]
				âˆ‡âˆ‡Aáµƒ = âˆ‡âˆ‡Aáµƒinput[clickindex]
			else
				Aáµƒ = Aáµƒsilent
				âˆ‡Aáµƒ = âˆ‡Aáµƒsilent
				âˆ‡âˆ‡Aáµƒ = âˆ‡âˆ‡Aáµƒsilent
			end
			# the p(aâ‚œ âˆ£ aâ‚œâ‚‹â‚) term
			for i = 1:nÎ¸_paâ‚œaâ‚œâ‚‹â‚
				q = indexÎ¸_paâ‚œaâ‚œâ‚‹â‚[i]
				Î· = sum_product_over_states(D[t], f[t-1], bâ‚œ, pY[t], âˆ‡Aáµƒ[i], Aá¶œ)
				âˆ‡â„“[q] += Î·
				for j=i:nÎ¸_paâ‚œaâ‚œâ‚‹â‚
					r = indexÎ¸_paâ‚œaâ‚œâ‚‹â‚[j]
					âˆ‡âˆ‡â„“[q,r] += sum_product_over_states(D[t], âˆ‡f[t-1][r], bâ‚œ, pY[t], âˆ‡Aáµƒ[i], Aá¶œ)
					âˆ‡âˆ‡â„“[q,r] += sum_product_over_states(D[t], f[t-1], âˆ‡b[r], pY[t], âˆ‡Aáµƒ[i], Aá¶œ)
					âˆ‡âˆ‡â„“[q,r] -= Î·*âˆ‡D[t][r]/D[t]
					âˆ‡âˆ‡â„“[q,r] += sum_product_over_states(D[t], f[t-1], bâ‚œ, pY[t], âˆ‡âˆ‡Aáµƒ[i,j], Aá¶œ)
				end
				for j = 1:nÎ¸_pcâ‚œcâ‚œâ‚‹â‚
					r = indexÎ¸_pcâ‚œcâ‚œâ‚‹â‚[j]
					indices = (r > q) ? CartesianIndex(q,r) : CartesianIndex(r,q)
					âˆ‡âˆ‡â„“[indices] += sum_product_over_states(D[t], âˆ‡f[t-1][r], bâ‚œ, pY[t], âˆ‡Aáµƒ[i], Aá¶œ)
					âˆ‡âˆ‡â„“[indices] += sum_product_over_states(D[t], f[t-1], âˆ‡b[r], pY[t], âˆ‡Aáµƒ[i], Aá¶œ)
					âˆ‡âˆ‡â„“[indices] -= Î·*âˆ‡D[t][r]/D[t]
					âˆ‡âˆ‡â„“[indices] += sum_product_over_states(D[t], f[t-1], bâ‚œ, pY[t], âˆ‡Aáµƒ[i], âˆ‡Aá¶œ[j])
				end
			end
			# the p(câ‚œ âˆ£ câ‚œâ‚‹â‚) term
			for i = 1:nÎ¸_pcâ‚œcâ‚œâ‚‹â‚
				q = indexÎ¸_pcâ‚œcâ‚œâ‚‹â‚[i]
				Î· = sum_product_over_states(D[t], f[t-1], bâ‚œ, pY[t], Aáµƒ, âˆ‡Aá¶œ[i])
				âˆ‡â„“[q] += Î·
				for j = i:nÎ¸_pcâ‚œcâ‚œâ‚‹â‚
					r = indexÎ¸_pcâ‚œcâ‚œâ‚‹â‚[j]
					âˆ‡âˆ‡â„“[q,r] += sum_product_over_states(D[t], âˆ‡f[t-1][r], bâ‚œ, pY[t], Aáµƒ, âˆ‡Aá¶œ[i])
					âˆ‡âˆ‡â„“[q,r] += sum_product_over_states(D[t], f[t-1], âˆ‡b[r], pY[t], Aáµƒ, âˆ‡Aá¶œ[i])
					âˆ‡âˆ‡â„“[q,r] -= Î·*âˆ‡D[t][r]/D[t]
				end
			end
		end
		# the p(yâ‚™â‚œ âˆ£ aâ‚œ, câ‚œ) term
		for n = 1:length(indexÎ¸_py)
			for i = 1:nÎ¸_py[n]
				q = indexÎ¸_py[n][i]
				âˆ‡â„“[q] += dot(Î³, âˆ‡logpy[t][n][i])
				for j = i:nÎ¸_py[n]
					r = indexÎ¸_py[n][j]
					âˆ‡âˆ‡â„“[q,r] += dot(âˆ‡Î³[r], âˆ‡logpy[t][n][i]) + dot(Î³, âˆ‡âˆ‡logpy[t][n][i,j])
				end
				for m = n+1:length(indexÎ¸_py)
					for j = 1:nÎ¸_py[m]
						r = indexÎ¸_py[m][j]
						âˆ‡âˆ‡â„“[q,r] += dot(âˆ‡Î³[r], âˆ‡logpy[t][n][i])
					end
				end
				for r in indexÎ¸_paâ‚œaâ‚œâ‚‹â‚
					if r > q
						âˆ‡âˆ‡â„“[q,r] += dot(âˆ‡Î³[r], âˆ‡logpy[t][n][i])
					else
						âˆ‡âˆ‡â„“[r,q] += dot(âˆ‡Î³[r], âˆ‡logpy[t][n][i])
					end
				end
				for r in indexÎ¸_pcâ‚œcâ‚œâ‚‹â‚
					if r > q
						âˆ‡âˆ‡â„“[q,r] += dot(âˆ‡Î³[r], âˆ‡logpy[t][n][i])
					else
						âˆ‡âˆ‡â„“[r,q] += dot(âˆ‡Î³[r], âˆ‡logpy[t][n][i])
					end
				end
			end
		end
	end
	#last backward step
	t = 1
	# the p(aâ‚) term
	@inbounds for i = nÎ¸_paâ‚:-1:1 # because B comes first
		q = indexÎ¸_paâ‚[i]
		Î· = sum_product_over_states(D[t], bâ‚œ, pY[t], âˆ‡paâ‚[i], Ï€á¶œ)
		âˆ‡â„“[q] += Î·
		for j = i:-1:1
			r = indexÎ¸_paâ‚[j]
			âˆ‡âˆ‡â„“[r,q] += sum_product_over_states(D[t], âˆ‡b[r], pY[t], âˆ‡paâ‚[i], Ï€á¶œ)
			âˆ‡âˆ‡â„“[r,q] -= Î·*âˆ‡D[t][r]/D[t]
			âˆ‡âˆ‡â„“[r,q] += sum_product_over_states(D[t], bâ‚œ, pY[t], âˆ‡âˆ‡paâ‚[j,i], Ï€á¶œ)
		end
		for index in (indexÎ¸_paâ‚œaâ‚œâ‚‹â‚only, indexÎ¸_pcâ‚œcâ‚œâ‚‹â‚)
			for r in index
				indices = r < q ? CartesianIndex(r,q) : CartesianIndex(q,r)
				âˆ‡âˆ‡â„“[indices] += sum_product_over_states(D[t], âˆ‡b[r], pY[t], âˆ‡paâ‚[i], Ï€á¶œ)
				âˆ‡âˆ‡â„“[indices] -= Î·*âˆ‡D[t][r]/D[t]
			end
		end
		if q âˆ‰indexÎ¸_paâ‚œaâ‚œâ‚‹â‚
			for r in indexÎ¸_pY
				âˆ‡âˆ‡â„“[q,r] += sum_product_over_states(D[t], âˆ‡b[r], pY[t], âˆ‡paâ‚[i], Ï€á¶œ)
				âˆ‡âˆ‡â„“[q,r] -= Î·*âˆ‡D[t][r]/D[t]
				j = index_pY_in_Î¸[r]
				âˆ‡âˆ‡â„“[q,r] += sum_product_over_states(D[t], bâ‚œ, âˆ‡pY[t][j], âˆ‡paâ‚[i], Ï€á¶œ)
			end
		end
	end
	# the p(câ‚) term
	@inbounds for i = 1:nÎ¸_pcâ‚
		q = indexÎ¸_pcâ‚[i]
		Î· = sum_product_over_states(D[t], bâ‚œ, pY[t], paâ‚, âˆ‡Ï€á¶œ[i])
		âˆ‡â„“[q] += Î·
		for j = i:nÎ¸_pcâ‚
			r = indexÎ¸_pcâ‚[j]
			âˆ‡âˆ‡â„“[q,r] += sum_product_over_states(D[t], âˆ‡b[r], pY[t], paâ‚, âˆ‡Ï€á¶œ[i])
			âˆ‡âˆ‡â„“[q,r] -= Î·*âˆ‡D[t][r]/D[t]
		end
		for index in (indexÎ¸_pY, indexÎ¸_paâ‚œaâ‚œâ‚‹â‚only, indexÎ¸_pcâ‚œcâ‚œâ‚‹â‚, indexÎ¸_paâ‚)
			for r in index
				indices = r < q ? CartesianIndex(r,q) : CartesianIndex(q,r)
				âˆ‡âˆ‡â„“[indices] += sum_product_over_states(D[t], âˆ‡b[r], pY[t], paâ‚, âˆ‡Ï€á¶œ[i])
				âˆ‡âˆ‡â„“[indices] -= Î·*âˆ‡D[t][r]/D[t]
				j = index_pY_in_Î¸[r]
				if j > 0
					âˆ‡âˆ‡â„“[indices] += sum_product_over_states(D[t], bâ‚œ, âˆ‡pY[t][j], paâ‚, âˆ‡Ï€á¶œ[i])
				end
				j = index_paâ‚_in_Î¸[r]
				if j > 0
					âˆ‡âˆ‡â„“[indices] += sum_product_over_states(D[t], bâ‚œ, pY[t], âˆ‡paâ‚[j], âˆ‡Ï€á¶œ[i])
				end
			end
		end
	end
	return nothing
end

"""
	forward!(D,âˆ‡D,f,âˆ‡f,â„“,âˆ‡â„“,t)

Normalize the forward term and its first-order partial derivatives and update the log-likelihood and its first-partial derivatives

PRE-MODIFIED ARGUMENT
-`f`: un-normalized forward term. Element `f[i,j]` corresponds to the un-normalized forward term in the i-th accumulator state and j-th coupling state
-`âˆ‡f`: first-order partial derivatives of the un-normalized forward term. Element `âˆ‡f[q][i,j]` corresponds to the derivative of the un-normalized forward term in the i-th accumulator state and j-th coupling state with respect to the q-th parameter.
-`â„“`: log-likelihood for the data before the current time-step
-`âˆ‡â„“`: gradient of the log-likelihood for the data before the current time-step

POST-MODIFICATION ARGUMENT
-`âˆ‡D`: gradient of the past-conditioned emissions likelihood for the current time step
-`âˆ‡f`: first-order partial derivatives of the normalized forward term. Element `âˆ‡f[q][i,j]` corresponds to `âˆ‚p{a(t)=Î¾(i), c(t)=j âˆ£ ğ˜(1:t)} / âˆ‚Î¸(q)`
-`f`: normalized forward term, which represents the joint conditional probability of the latent variables given the elapsed emissions. Element `f[i,j]` corresponds to `p{a(t)=Î¾(i), c(t)=j âˆ£ ğ˜(1:t)}`
-`â„“`: log-likelihood for the data before the up to the current time-step

UNMODIFIED ARGUMENT
-`D`: past-conditioned emissions likelihood: `p{ğ˜(t) âˆ£ ğ˜(1:t-1))`

"""
function forward!(âˆ‡D::Vector{<:Real},
				f::Matrix{<:Real},
				âˆ‡f::Vector{<:Matrix{<:Real}},
				â„“::Vector{<:Real},
				D::Real)
	f ./= D
	@inbounds for i in eachindex(âˆ‡D)
		âˆ‡D[i] = sum(âˆ‡f[i])
	end
	@inbounds for i in eachindex(âˆ‡f)
		for jk in eachindex(âˆ‡f[i])
			âˆ‡f[i][jk] = (âˆ‡f[i][jk] - f[jk]*âˆ‡D[i])/D
		end
	end
	â„“[1] += log(D)
end

"""
    linearpredictor(mpGLMs)

Linear combination of the weights in the j-th accumulator state and k-th coupling state

ARGUMENT
-`mpGLMs`: a vector of mixture of Poisson generalized linear models

RETURN
-`ğ‹`: a nested array whose element ğ‹[n][j,k][t] corresponds to n-th neuron, j-th accumualtor state, k-th coupling state, and the t-th time bin in the trialset
"""
function linearpredictor(mpGLMs::Vector{<:MixturePoissonGLM})
	map(mpGLMs) do mpGLM
		Î = length(mpGLM.dğ›_dB)
		K = length(mpGLM.Î¸.ğ¯)
		map(CartesianIndices((Î,K))) do index
			j = index[1]
			k = index[2]
			linearpredictor(mpGLM, j, k)
		end
	end
end

"""
	update_emissions!(Î», âˆ‡logpy, âˆ‡âˆ‡logpy, pY, âˆ‡pY, Î”t, ğ‹, mpGLMs, offset)

Update the conditional likelihood of spiking and its gradient and the gradient and Hessian of the conditional log-likelihoods

MODIFIED ARGUMENT
-`Î»`:: conditional rate of each neuron at each time step. Element `Î»[n][t][i,j]` corresponds to the n-th neuron, t-th time step in a trial, i-th accumulator state, and j-th coupling state
-`âˆ‡logpy`: partial derivatives of the conditional log-likelihood of each neuron at each time step. Element `âˆ‡logpy[t][n][q][i,j]` corresponds to the t-th time step in a trial, n-th neuron, q-th parameter, i-th accumulator state, and j-th coupling state.
 -`âˆ‡âˆ‡logpy`: partial derivatives of the conditional log-likelihood of each neuron at each time step. Element `âˆ‡âˆ‡logpy[t][n][q,r][i,j]` corresponds to the t-th time step in a trial, n-th neuron, q-th and r-th parameter, i-th accumulator state, and j-th coupling state.
-`pY`: conditional likelihood of the spiking of all neurons. Element `pY[t][i,j]` corresponds to the t-th time step in a trial, i-th accumulator state, and j-th coupling state
-`âˆ‡pY`: partial derivative of the conditional likelihood of the spiking of all neurons. Element `âˆ‡pY[t][q][i,j]` corresponds to the t-th time step in a trial, q-th paramaeter,  i-th accumulator state, and j-th coupling state

UNMODIFIED ARGUMENT
-`Î”t`: duration of each time step, in second
-`ğ‹`: linear predictors. Element `ğ‹[n][i,j][Ï„]` corresponds to the n-th neuron, the i-th accumulator state and j-th coupling state for the Ï„-timestep in the trialset
-`mpGLMs`: Mixture of Poisson GLM of each neuron
-`ntimesteps`: number of time steps in the trial
-`offset`: the time index in the trialset corresponding to the time index 0 in the trial
"""
function update_emissions!(Î»::Vector{<:Vector{<:Matrix{<:Real}}},
						âˆ‡logpy::Vector{<:Vector{<:Vector{<:Matrix{<:Real}}}},
						âˆ‡âˆ‡logpy::Vector{<:Vector{<:Matrix{<:Matrix{<:Real}}}},
						pY::Vector{<:Matrix{<:Real}},
						âˆ‡pY::Vector{<:Vector{<:Matrix{<:Real}}},
						Î”t::Real,
						ğ‹::Vector{<:Matrix{<:Vector{<:Real}}},
						mpGLMs::Vector{<:MixturePoissonGLM},
						ntimesteps::Integer,
						offset::Integer)
	dL_dğ¯ = zeros(length(mpGLMs[1].Î¸.ğ¯[1]))
	@inbounds for n in eachindex(mpGLMs)
		conditionalrate!(Î»[n], ğ‹[n], ntimesteps, offset)
		for t = 1:ntimesteps
			Ï„ = t + offset
			âˆ‡âˆ‡conditional_log_likelihood!(âˆ‡logpy[t][n], âˆ‡âˆ‡logpy[t][n], dL_dğ¯, Î”t, ğ‹[n], Î»[n][t], mpGLMs[n], Ï„)
		end
	end
	nneurons = length(mpGLMs)
	Î = length(mpGLMs[1].dğ›_dB)
	K = length(mpGLMs[1].Î¸.ğ¯)
	@inbounds for t = 1:ntimesteps
		Ï„ = t + offset
		for ij in eachindex(pY[t])
			pY[t][ij] = 1.0
			for n=1:nneurons
				pY[t][ij] *= poissonlikelihood(Î»[n][t][ij]*Î”t, mpGLMs[n].ğ²[Ï„])
			end
		end
		r = 0
		for n=1:nneurons
			for q in eachindex(âˆ‡logpy[t][n])
				r+=1
				for i=1:Î
					for j=1:K
						âˆ‡pY[t][r][i,j] = âˆ‡logpy[t][n][q][i,j]*pY[t][i,j]
					end
				end
			end
		end
	end
	return nothing
end

"""
	update_emissions!(âˆ‚pYğ‘‘_âˆ‚Ïˆ, pY, âˆ‡pY, choice, Ïˆ)

Update the conditional likelihood of the emissions as well as its gradient

MODIFIED ARGUMENT
-`âˆ‚pYğ‘‘_âˆ‚Ïˆ`: derivative of the conditional likelihood of the emissions at the last time step of a trial with respect to the lapse parameter Ïˆ. Element `âˆ‚pYğ‘‘_âˆ‚Ïˆ[i,j]` corresponds to the i-th accumulator state and j-th coupling state
-`pY`: conditional likelihood of the emissions at the last time step of the trial. Element `pY[i,j]` corresponds to the i-th accumulator state and j-th coupling state
-`âˆ‡pY`: gradient of the conditional likelihood of the emissions at the last time step of the trial. Element `âˆ‡pY[q][i,j]` corresponds to the q-th parameter among all GLMs, i-th accumulator state and j-th coupling state

UNMODIFIED ARGUMENT
-`choice`: a Bool indicating left (false) or right (true)
-`Ïˆ`: lapse rate
"""
function update_emissions!(âˆ‚pYğ‘‘_âˆ‚Ïˆ::Matrix{<:Real}, pY::Matrix{<:Real}, âˆ‡pY::Vector{<:Matrix{<:Real}}, choice::Bool, Ïˆ::Real)
	differentiate_pYğ‘‘_wrt_Ïˆ!(âˆ‚pYğ‘‘_âˆ‚Ïˆ, pY, choice)
	conditionallikelihood!(pY, choice, Ïˆ)
	@inbounds for q in eachindex(âˆ‡pY)
		conditionallikelihood!(âˆ‡pY[q], choice, Ïˆ)
	end
	return nothing
end

"""
	conditionalrate!(Î», ğ‹, offset)

MODIFIED ARGUMENT
-`Î»`: matrix whose element `Î»[t][i,j]` is the Poisson rate at the t-th timestep of a trial given the i-th accumulator state and j-th coupling state

UNMODIFIED ARGUMENT
-`ğ‹`: matrix whose element `ğ‹[i,j][Ï„]` is the linear predictor given the i-th accumulator state and j-th coupling state for the Ï„-timestep in the trialset
-`offset`: the time index in the trialset corresponding to the time index 0 in the trial
"""
function conditionalrate!(Î»::Vector{<:Matrix{<:Real}},
						  ğ‹::Matrix{<:Vector{<:Real}},
						  ntimesteps::Integer,
						  offset::Integer)
	for t = 1:ntimesteps
		Ï„ = t + offset
		for jk in eachindex(Î»[t])
			Î»[t][jk] = softplus(ğ‹[jk][Ï„])
		end
	end
	return nothing
end

"""
	âˆ‡âˆ‡conditional_log_likelihood!(âˆ‡logpy, âˆ‡âˆ‡logpy, dL_dğ¯, Î”t, ğ‹, Î», mpGLM, Ï„)

Gradient and Hessian of the conditional log-likelihood of one neuron at single timestep

MODIFIED ARGUMENT
-`âˆ‡logpy`: Gradient of the conditional log-likelihood of a neuron's response at a single time. Element âˆ‡logpy[i][j,k] correponds to the partial derivative of log p{y(n,t) âˆ£ a(t)=Î¾(j), c(t)=k} with respect to the i-th parameter of the neuron's GLM
-`âˆ‡âˆ‡logpy`: Hessian of the conditional log-likelihood. Element âˆ‡âˆ‡logpy[i,j][k,l] correponds to the partial derivative of log p{y(n,t) âˆ£ a(t)=Î¾(k), c(t)=l} with respect to the i-th and j-th parameters of the neuron's GLM
-`dL_dğ¯`: memory for computing the derivative of the linear predictor with respect to the linear filters of the accumulator. The element `dL_dğ¯[q]` corresponds to the q-th linear filter in one of the coupling states.

UNMODIFIED ARGUMENT
-`Î”t`: width of time step
-`ğ‹`: linear predictors. Element `ğ‹[i,j][Ï„]` corresponds to the i-th accumulator state and j-th coupling state for the Ï„-timestep in the trialset
-`Î»`: Conditional Poisson whose element Î»[i,j] corresponds to a(t)=Î¾(i), c(t)=j
-`offset`: the time index in the trialset corresponding to the time index 0 in the trial
-`Ï„` time step in the trialset
"""
function âˆ‡âˆ‡conditional_log_likelihood!(âˆ‡logpy::Vector{<:Matrix{<:Real}},
										âˆ‡âˆ‡logpy::Matrix{<:Matrix{<:Real}},
										dL_dğ¯::Vector{<:Real},
										Î”t::Real,
										ğ‹::Matrix{<:Vector{<:Real}},
										Î»::Matrix{<:Real},
										mpGLM::MixturePoissonGLM,
										Ï„::Integer)
	@unpack dğ›_dB, ğ—, ğ•, ğ² = mpGLM
	@unpack ğ®, ğ¯ = mpGLM.Î¸
	nğ® = length(ğ®)
	K = length(ğ¯)
	nğ¯ = length(ğ¯[1])
	Î = length(dğ›_dB)
	for i = 1:Î
		for q=1:nğ¯
			dL_dğ¯[q] = ğ•[Ï„,q]*dğ›_dB[i]
		end
		for j = 1:K
			dlogp_dL, dÂ²logp_dL = differentiate_twice_loglikelihood_wrt_linearpredictor(Î”t, ğ‹[i,j][Ï„], Î»[i,j], ğ²[Ï„])
			for q=1:nğ®
				âˆ‡logpy[q][i,j] = dlogp_dL*ğ—[Ï„,q]
			end
			for q=1:nğ¯
				s = nğ®+(j-1)*nğ¯+q
				âˆ‡logpy[s][i,j] = dlogp_dL*dL_dğ¯[q]
			end
			for q=1:nğ®
				for r=q:nğ®
					âˆ‡âˆ‡logpy[q,r][i,j] = dÂ²logp_dL*ğ—[Ï„,q]*ğ—[Ï„,r]
				end
				for r=1:nğ¯
					s = nğ®+(j-1)*nğ¯+r
					âˆ‡âˆ‡logpy[q,s][i,j] = dÂ²logp_dL*ğ—[Ï„,q]*dL_dğ¯[r]
				end
			end
			for q=1:nğ¯
				for r=q:nğ¯
					s1 = nğ®+(j-1)*nğ¯+q
					s2 = nğ®+(j-1)*nğ¯+r
					âˆ‡âˆ‡logpy[s1,s2][i,j] = dÂ²logp_dL * dL_dğ¯[q] * dL_dğ¯[r]
				end
			end
		end
	end
	return nothing
end

"""
    differentiate_twice_loglikelihood_wrt_linearpredictor

Differentiate the log-likelihood of a Poisson GLM with respect to the linear predictor

The Poisson GLM is assumed to have a a softplus nonlinearity

ARGUMENT
-`Î”t`: duration of time step
-`L`: linear predictor at one time step
-`Î»`: Poisson rate
-`y`: observation at that time step

RETURN
-the first derivative with respect to the linear predictor
-the second derivative with respect to the linear predictor

EXAMPLE
```julia-repl
julia> using FHMDDM, ForwardDiff, LogExpFunctions
julia> Î”t = 0.01
julia> y = 2
julia> f(x) = let Î»Î”t = softplus(x[1])*Î”t; y*log(Î»Î”t)-Î»Î”t+log(factorial(y)); end
julia> x = rand(1)
julia> d1auto = ForwardDiff.gradient(f, x)
julia> d2auto = ForwardDiff.hessian(f, x)
julia> d1hand, d2hand = FHMDDM.differentiate_twice_loglikelihood_wrt_linearpredictor(Î”t, x[1], softplus(x[1]), y)
julia> abs(d1hand - d1auto[1])
julia> abs(d2hand - d2auto[1])
```
"""
function differentiate_twice_loglikelihood_wrt_linearpredictor(Î”t::AbstractFloat, L::Real, Î»::Real, y::Integer)
	dÎ»_dL = logistic(L)
	dÂ²Î»_dLdL = dÎ»_dL*(1-dÎ»_dL)
    if y > 0
        if L > -100.0
            dâ„“_dL = dÎ»_dL*(y/Î» - Î”t)
        else
            dâ„“_dL = y - dÎ»_dL*Î”t  # the limit of `dÎ»_dL/Î»` as x goes to -âˆ is 1
        end
		if L > -50.0
			dÂ²â„“_dLdL = y*(Î»*dÂ²Î»_dLdL - dÎ»_dL^2)/Î»^2 - dÂ²Î»_dLdL*Î”t # the limit of first second term is 0 as L goes to -âˆ
		else
			dÂ²â„“_dLdL = -dÂ²Î»_dLdL*Î”t
		end
    else
        dâ„“_dL = -dÎ»_dL*Î”t
		dÂ²â„“_dLdL = -dÂ²Î»_dLdL*Î”t
    end
	return dâ„“_dL, dÂ²â„“_dLdL
end

"""
    differentiate_loglikelihood_wrt_linearpredictor

Differentiate the log-likelihood of a Poisson GLM with respect to the linear predictor

The Poisson GLM is assumed to have a a softplus nonlinearity

ARGUMENT
-`Î”t`: duration of time step
-`L`: linear predictor at one time step
-`Î»`: Poisson rate
-`y`: observation at that time step

RETURN
-the first derivative with respect to the linear predictor

EXAMPLE
```julia-repl
julia> using FHMDDM, ForwardDiff, LogExpFunctions
julia> Î”t = 0.01
julia> y = 2
julia> f(x) = let Î»Î”t = softplus(x[1])*Î”t; y*log(Î»Î”t)-Î»Î”t+log(factorial(y)); end
julia> x = rand(1)
julia> d1auto = ForwardDiff.gradient(f, x)
julia> d1hand = FHMDDM.differentiate_loglikelihood_wrt_linearpredictor(Î”t, x[1], softplus(x[1]), y)
julia> abs(d1hand - d1auto[1])
```
"""
function differentiate_loglikelihood_wrt_linearpredictor(Î”t::AbstractFloat, L::Real, Î»::Real, y::Integer)
	dÎ»_dL = logistic(L)
    if y > 0
        if L > -100.0
            dâ„“_dL = dÎ»_dL*(y/Î» - Î”t)
        else
            dâ„“_dL = y - dÎ»_dL*Î”t  # the limit of `dÎ»_dL/Î»` as x goes to -âˆ is 1
        end
    else
        dâ„“_dL = -dÎ»_dL*Î”t
    end
	return dâ„“_dL
end

"""
	conditionallikelihood!(pY, ğ‘‘, Ïˆ)

Multiply the conditional likelihood of the choice to the conditional likelihood of spiking

ARGUMENT
-`pY`: a matrix whose element `pY[i,j]` corresponds to the i-th accumulator state and j-th coupling state and represents p{Y âˆ£ a(T)=Î¾(i), c(T)=j}
-`ğ‘‘`: left (false) or right (true) choice of the animal
-`Ïˆ`: lapse rate

MODIFIED ARGUMENT
-`pY`: p{Y, ğ‘‘ âˆ£ a(T)=Î¾(i), c(T)=j}
"""
function conditionallikelihood!(pY::Matrix{<:Real}, ğ‘‘::Bool, Ïˆ::Real)
	if ğ‘‘
		pğ‘‘_Î¾â» = Ïˆ/2
		pğ‘‘_Î¾âº = 1-Ïˆ/2
	else
		pğ‘‘_Î¾â» = 1-Ïˆ/2
		pğ‘‘_Î¾âº = Ïˆ/2
	end
	Î,K = size(pY)
	zeroindex = cld(Î,2)
	@inbounds for j = 1:K
		for i = 1:zeroindex-1
			pY[i,j] *= pğ‘‘_Î¾â»
		end
		pY[zeroindex,j] *= 0.5
		for i = zeroindex+1:Î
			pY[i,j] *= pğ‘‘_Î¾âº
		end
	end
	return nothing
end

"""
	sum_product_over_states(D,fâ‚œâ‚‹â‚,bâ‚œ,Y,A,C)

Multiply terms across different states of the latent variables at consecutive time step and sum

ARGUMENT
-`Y`: similar to Î·, element Y[i,j] corresponds to i-th state of the accumulator at time t and the j-th state of the coupling at time t
-`A`: element A[i,j] corresponds to i-th state of the accumulator at time t and the j-th state of the accumulator at time t-1
-`C`: element C[i,j] corresponds to i-th state of the coupling at time t and the j-th state of the coupling at time t-1

RETURN
-`s`: sum of the product across all states of the two latent variables at two consecutive time steps
"""
function sum_product_over_states(D::Real, fâ‚œâ‚‹â‚::Matrix{<:Real}, bâ‚œ::Matrix{<:Real}, Y::Matrix{<:Real}, A::Matrix{<:Real}, C::Matrix{<:Real})
	s = 0.0
	Î,K = size(fâ‚œâ‚‹â‚)
	@inbounds for iaâ‚œ = 1:Î
		for icâ‚œ = 1:K
			for iaâ‚œâ‚‹â‚ = 1:Î
				for icâ‚œâ‚‹â‚ = 1:K
					s += fâ‚œâ‚‹â‚[iaâ‚œâ‚‹â‚,icâ‚œâ‚‹â‚]*bâ‚œ[iaâ‚œ,icâ‚œ]*Y[iaâ‚œ,icâ‚œ]*A[iaâ‚œ,iaâ‚œâ‚‹â‚]*C[icâ‚œ, icâ‚œâ‚‹â‚]
				end
			end
		end
	end
	return s/D
end

"""
	sum_product_over_states(D, b,Y,A,C)

Multiply terms across different states of the latent variables at a single time step and sum

ARGUMENT
-`Y`: similar to Î·, element Y[i,j] corresponds to i-th state of the accumulator at time t and the j-th state of the coupling at time t
-`A`: element A[i] corresponds to the i-th state of the accumulator at time t
-`C`: element C[j] corresponds to the j-th state of the coupling at time t-1

RETURN
-`s`: sum of the product across all states of the two latent variables at a single time step
"""
function sum_product_over_states(D::Real, b::Matrix{<:Real}, Y::Matrix{<:Real}, A::Vector{<:Real}, C::Vector{<:Real})
	s = 0.0
	Î,K = size(b)
	@inbounds for iaâ‚œ = 1:Î
		for icâ‚œ = 1:K
			s += b[iaâ‚œ,icâ‚œ]*Y[iaâ‚œ,icâ‚œ]*A[iaâ‚œ]*C[icâ‚œ]
		end
	end
	return s/D
end

"""
	expectation_derivative_logpğ‘‘_wrt_Ïˆ(choice, Î³, Ïˆ)

Expectation of the derivative of the choice log-likelihood with respect to the lapse rate

ARGUMENT
-`choice`: a Boolean specifying whether the choice was to the right
-`Î³`: a matrix or vector of representing the posterior probability of the latent variables at the end of the trial (i.e., last time step). Element `Î³_end[i,j]` = p(aáµ¢=1, câ±¼=1 âˆ£ ğ˜, d). Rows correspond to states of the accumulator state variable ğš, and columns to states of the coupling variable ğœ.
-`Ïˆ`: a floating-point number specifying the lapse rate

RETURN
-the expectation under the posterior probability of the late variables
"""
function expectation_derivative_logpğ‘‘_wrt_Ïˆ(choice::Bool, Î³::Array{<:Real}, Ïˆ::Real)
	if choice
		dlogpğ‘‘_dÏˆ_Î¾â» = 1/Ïˆ
		dlogpğ‘‘_dÏˆ_Î¾âº = 1/(Ïˆ-2)
	else
		dlogpğ‘‘_dÏˆ_Î¾â» = 1/(Ïˆ-2)
		dlogpğ‘‘_dÏˆ_Î¾âº = 1/Ïˆ
	end
	Î = size(Î³,1)
	zeroindex = cld(Î,2)
	edll = 0.0 # expectation of the derivative of the log-likelihood
	@inbounds for j = 1:size(Î³,2)
		for i = 1:zeroindex-1
			edll += Î³[i,j]*dlogpğ‘‘_dÏˆ_Î¾â»
		end
		for i = zeroindex+1:Î
			edll += Î³[i,j]*dlogpğ‘‘_dÏˆ_Î¾âº
		end
	end
	return edll
end

"""
	expectation_second_derivative_logpğ‘‘_wrt_Ïˆ(choice, Î³, Ïˆ)

Expectation of the second derivative of the choice log-likelihood with respect to the lapse rate

ARGUMENT
-`choice`: a Boolean specifying whether the choice was to the right
-`Î³`: a matrix or vector of representing the posterior probability of the latent variables at the end of the trial (i.e., last time step). Element `Î³_end[i,j]` = p(aáµ¢=1, câ±¼=1 âˆ£ ğ˜, d). Rows correspond to states of the accumulator state variable ğš, and columns to states of the coupling variable ğœ.
-`Ïˆ`: a floating-point number specifying the lapse rate

RETURN
-the expectation under the posterior probability of the late variables
"""
function expectation_second_derivative_logpğ‘‘_wrt_Ïˆ(choice::Bool, Î³::Array{<:Real}, Ïˆ::Real)
	if choice
		dÂ²logpğ‘‘_dÏˆdÏˆ_Î¾â» = -Ïˆ^-2
		dÂ²logpğ‘‘_dÏˆdÏˆ_Î¾âº = -(Ïˆ-2)^-2
	else
		dÂ²logpğ‘‘_dÏˆdÏˆ_Î¾â» = -(Ïˆ-2)^-2
		dÂ²logpğ‘‘_dÏˆdÏˆ_Î¾âº = -Ïˆ^-2
	end
	Î = size(Î³,1)
	zeroindex = cld(Î,2)
	edÂ²ll = 0.0 # expectation of the second derivative of the log-likelihood
	@inbounds for j = 1:size(Î³,2)
		for i = 1:zeroindex-1
			edÂ²ll += Î³[i,j]*dÂ²logpğ‘‘_dÏˆdÏˆ_Î¾â»
		end
		for i = zeroindex+1:Î
			edÂ²ll += Î³[i,j]*dÂ²logpğ‘‘_dÏˆdÏˆ_Î¾âº
		end
	end
	return edÂ²ll
end


"""
	differentiate_pYğ‘‘_wrt_Ïˆ!(âˆ‚pYğ‘‘_âˆ‚Ïˆ, pY, ğ‘‘, Ïˆ)

Partial derivatives of the conditional likelihood of the emissions at the last time step with respect to the lapse rate

ARGUMENT
-`pY`: conditional likelihood of the population spiking at the last time step `T`. Element `pY[i,j]` represents p{Y(T) âˆ£ a(T)=Î¾(i), c(T)=j}
-`ğ‘‘`: left (false) or right (true) choice of the animal

MODIFIED ARGUMENT
-`âˆ‚pYğ‘‘_âˆ‚Ïˆ`: partial derivative of the emissions at the last time step (population spiking and the choice) with respect to the lapse rate. Element `âˆ‚pYğ‘‘_âˆ‚Ïˆ[i,j]` represents:
	âˆ‚p{Y(T), ğ‘‘ âˆ£ a(T)=Î¾(i), c(T)=j}}/âˆ‚Ïˆ
"""
function differentiate_pYğ‘‘_wrt_Ïˆ!(âˆ‚pYğ‘‘_âˆ‚Ïˆ::Matrix{<:Real}, pY::Matrix{<:Real}, ğ‘‘::Bool)
	if ğ‘‘
		âˆ‚pğ‘‘_Î¾â»_âˆ‚Ïˆ = 0.5
		âˆ‚pğ‘‘_Î¾âº_âˆ‚Ïˆ = -0.5
	else
		âˆ‚pğ‘‘_Î¾â»_âˆ‚Ïˆ = -0.5
		âˆ‚pğ‘‘_Î¾âº_âˆ‚Ïˆ = 0.5
	end
	Î,K = size(pY)
	zeroindex = cld(Î,2)
	for j = 1:K
		for i = 1:zeroindex-1
			âˆ‚pYğ‘‘_âˆ‚Ïˆ[i,j] = pY[i,j]*âˆ‚pğ‘‘_Î¾â»_âˆ‚Ïˆ
		end
		âˆ‚pYğ‘‘_âˆ‚Ïˆ[zeroindex,j] = 0.0
		for i = zeroindex+1:Î
			âˆ‚pYğ‘‘_âˆ‚Ïˆ[i,j] = pY[i,j]*âˆ‚pğ‘‘_Î¾âº_âˆ‚Ïˆ
		end
	end
end

"""
	Sameacrosstrials(model)

Make a structure containing quantities that are used in each trial

ARGUMENT
-`model`: data, parameters, and hyperparameters of the factorial hidden-Markov drift-diffusion model

RETURN
-a structure containing quantities that are used in each trial

EXAMPLE
```julia-repl
julia> using FHMDDM
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_04_14_test/data.mat"; randomize=true)
julia> S = FHMDDM.Sameacrosstrials(model)
```
"""
function Sameacrosstrials(model::Model)
	@unpack options, Î¸native, Î¸real, trialsets = model
	@unpack Î”t, K, Î = options
	Aá¶œâ‚â‚ = Î¸native.Aá¶œâ‚â‚[1]
	Aá¶œâ‚‚â‚‚ = Î¸native.Aá¶œâ‚‚â‚‚[1]
	Ï€á¶œâ‚ = Î¸native.Ï€á¶œâ‚[1]
	if K == 2
		Aá¶œ = [Aá¶œâ‚â‚ 1-Aá¶œâ‚‚â‚‚; 1-Aá¶œâ‚â‚ Aá¶œâ‚‚â‚‚]
		âˆ‡Aá¶œ = [[1.0 0.0; -1.0 0.0], [0.0 -1.0; 0.0 1.0]]
		Ï€á¶œ = [Ï€á¶œâ‚, 1-Ï€á¶œâ‚]
		âˆ‡Ï€á¶œ = [[1.0, -1.0]]
	else
		Aá¶œ = ones(1,1)
		âˆ‡Aá¶œ = [zeros(1,1), zeros(1,1)]
		Ï€á¶œ = ones(1)
		âˆ‡Ï€á¶œ = [zeros(1)]
	end
	indexÎ¸_paâ‚ = [3,6,11,13]
	indexÎ¸_paâ‚œaâ‚œâ‚‹â‚ = [3,4,5,7,10,12]
	indexÎ¸_pcâ‚ = [8]
	indexÎ¸_pcâ‚œcâ‚œâ‚‹â‚ = [1,2]
	indexÎ¸_Ïˆ = [9]
	counter = 13
	indexÎ¸_py = map(trialsets) do trialset
					map(trialset.mpGLMs) do mpGLM
						q = length(mpGLM.Î¸.ğ®) + sum(length.(mpGLM.Î¸.ğ¯))
						zeros(Int,q)
					end
				end
	for s in eachindex(indexÎ¸_py)
		for n in eachindex(indexÎ¸_py[s])
			for q in eachindex(indexÎ¸_py[s][n])
				counter += 1
				indexÎ¸_py[s][n][q] = counter
			end
		end
	end
	indexÎ¸_pY = map(x->vcat(x...), indexÎ¸_py)
	nÎ¸_trialset = indexÎ¸_pY[end][end]
	index_paâ‚_in_Î¸, index_paâ‚œaâ‚œâ‚‹â‚_in_Î¸, index_pcâ‚_in_Î¸, index_pcâ‚œcâ‚œâ‚‹â‚_in_Î¸, index_Ïˆ_in_Î¸ = zeros(Int, nÎ¸_trialset), zeros(Int, nÎ¸_trialset), zeros(Int, nÎ¸_trialset), zeros(Int, nÎ¸_trialset), zeros(Int, nÎ¸_trialset)
	index_paâ‚_in_Î¸[indexÎ¸_paâ‚] .= 1:length(indexÎ¸_paâ‚)
	index_paâ‚œaâ‚œâ‚‹â‚_in_Î¸[indexÎ¸_paâ‚œaâ‚œâ‚‹â‚] .= 1:length(indexÎ¸_paâ‚œaâ‚œâ‚‹â‚)
	index_pcâ‚_in_Î¸[indexÎ¸_pcâ‚] .= 1:length(indexÎ¸_pcâ‚)
	index_pcâ‚œcâ‚œâ‚‹â‚_in_Î¸[indexÎ¸_pcâ‚œcâ‚œâ‚‹â‚] .= 1:length(indexÎ¸_pcâ‚œcâ‚œâ‚‹â‚)
	index_Ïˆ_in_Î¸[indexÎ¸_Ïˆ] .= 1:length(indexÎ¸_Ïˆ)
	index_pY_in_Î¸ = map(x->zeros(Int, nÎ¸_trialset), indexÎ¸_pY)
	for i = 1:length(index_pY_in_Î¸)
		index_pY_in_Î¸[i][indexÎ¸_pY[i]] = 1:length(indexÎ¸_pY[i])
	end
	nÎ¸_paâ‚œaâ‚œâ‚‹â‚ = length(indexÎ¸_paâ‚œaâ‚œâ‚‹â‚)
	P = Probabilityvector(Î”t, Î¸native, Î)
	update_for_âˆ‡âˆ‡transition_probabilities!(P)
	âˆ‡âˆ‡Aáµƒsilent = map(i->zeros(Î,Î), CartesianIndices((nÎ¸_paâ‚œaâ‚œâ‚‹â‚,nÎ¸_paâ‚œaâ‚œâ‚‹â‚)))
	âˆ‡Aáµƒsilent = map(i->zeros(Î,Î), 1:nÎ¸_paâ‚œaâ‚œâ‚‹â‚)
	Aáµƒsilent = zeros(typeof(Î¸native.B[1]), Î, Î)
	Aáµƒsilent[1,1] = Aáµƒsilent[Î, Î] = 1.0
	âˆ‡âˆ‡transitionmatrix!(âˆ‡âˆ‡Aáµƒsilent, âˆ‡Aáµƒsilent, Aáµƒsilent, P)
	Sameacrosstrials(Aáµƒsilent=Aáµƒsilent,
					âˆ‡Aáµƒsilent=âˆ‡Aáµƒsilent,
					âˆ‡âˆ‡Aáµƒsilent=âˆ‡âˆ‡Aáµƒsilent,
					Aá¶œ=Aá¶œ,
					âˆ‡Aá¶œ=âˆ‡Aá¶œ,
					Î”t=options.Î”t,
					indexÎ¸_paâ‚=indexÎ¸_paâ‚,
					indexÎ¸_paâ‚œaâ‚œâ‚‹â‚=indexÎ¸_paâ‚œaâ‚œâ‚‹â‚,
					indexÎ¸_pcâ‚=indexÎ¸_pcâ‚,
					indexÎ¸_pcâ‚œcâ‚œâ‚‹â‚=indexÎ¸_pcâ‚œcâ‚œâ‚‹â‚,
					indexÎ¸_Ïˆ=indexÎ¸_Ïˆ,
					indexÎ¸_py=indexÎ¸_py,
					indexÎ¸_pY=indexÎ¸_pY,
					K=K,
					Ï€á¶œ=Ï€á¶œ,
					âˆ‡Ï€á¶œ=âˆ‡Ï€á¶œ,
					Î=Î)
end

"""
	Memoryforhessian(model)

EXAMPLE
```julia-repl
julia> using FHMDDM
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_05_05_test/data.mat"; randomize=true)
julia> S = FHMDDM.Sameacrosstrials(model)
julia> M = FHMDDM.Memoryforhessian(model, S)
```
"""
function Memoryforhessian(model::Model, S::Sameacrosstrials)
	@unpack options, Î¸native, Î¸real, trialsets = model
	@unpack Î”t, K, Î = options
	maxclicks = maximum_number_of_clicks(model)
	maxtimesteps = maximum_number_of_time_steps(model)
	maxneurons = maximum(map(trialset-> length(trialset.mpGLMs), trialsets))
	max_nÎ¸_pY=maximum(S.nÎ¸_pY)
	max_nÎ¸_py=maximum(map(n->maximum(n), S.nÎ¸_py))
	âˆ‡D=collect(zeros(S.nÎ¸_alltrialsets) for t=1:maxtimesteps)
	f = collect(zeros(Î,K) for t=1:maxtimesteps)
	âˆ‡f = collect(collect(zeros(Î,K) for q=1:S.nÎ¸_alltrialsets) for t=1:maxtimesteps)
	âˆ‡b = collect(zeros(Î,K) for q=1:S.nÎ¸_alltrialsets)
	Î» = collect(collect(zeros(Î,K) for t=1:maxtimesteps) for n = 1:maxneurons)
	âˆ‡logpy = collect(collect(collect(zeros(Î,K) for q=1:max_nÎ¸_py) for n=1:maxneurons) for t=1:maxtimesteps)
	âˆ‡âˆ‡logpy=map(1:maxtimesteps) do t
				map(1:maxneurons) do n
					map(CartesianIndices((max_nÎ¸_py,max_nÎ¸_py))) do q
						zeros(Î,K)
					end
				end
			end
	Aáµƒinput=map(1:maxclicks) do t
				A = zeros(Î,Î)
				A[1,1] = A[Î,Î] = 1.0
				return A
			end
	âˆ‡Aáµƒinput = collect(collect(zeros(Î,Î) for q=1:S.nÎ¸_paâ‚œaâ‚œâ‚‹â‚) for t=1:maxclicks)
	âˆ‡âˆ‡Aáµƒinput = map(1:maxclicks) do t
					map(CartesianIndices((S.nÎ¸_paâ‚œaâ‚œâ‚‹â‚,S.nÎ¸_paâ‚œaâ‚œâ‚‹â‚))) do ij
						zeros(Î,Î)
					end
				end
	âˆ‡paâ‚ = collect(zeros(Î) for q=1:S.nÎ¸_paâ‚)
	âˆ‡âˆ‡paâ‚ = map(CartesianIndices((S.nÎ¸_paâ‚,S.nÎ¸_paâ‚))) do q
				zeros(Î)
			end
	pY = collect(zeros(Î,K) for t=1:maxtimesteps)
	âˆ‡pY = collect(collect(zeros(Î,K) for q=1:max_nÎ¸_pY) for t=1:maxtimesteps)
	Memoryforhessian(Aáµƒinput=Aáµƒinput,
					âˆ‡Aáµƒinput=âˆ‡Aáµƒinput,
					âˆ‡âˆ‡Aáµƒinput=âˆ‡âˆ‡Aáµƒinput,
					f=f,
					âˆ‡f=âˆ‡f,
					âˆ‡b=âˆ‡b,
					D = zeros(maxtimesteps),
					âˆ‡D=âˆ‡D,
					âˆ‡â„“=zeros(S.nÎ¸_alltrialsets),
					âˆ‡âˆ‡â„“=zeros(S.nÎ¸_alltrialsets,S.nÎ¸_alltrialsets),
					âˆ‚pYğ‘‘_âˆ‚Ïˆ=zeros(Î,K),
					Î»=Î»,
					âˆ‡logpy=âˆ‡logpy,
					âˆ‡âˆ‡logpy=âˆ‡âˆ‡logpy,
					P = Probabilityvector(Î”t, Î¸native, Î),
					âˆ‡paâ‚=âˆ‡paâ‚,
					âˆ‡âˆ‡paâ‚=âˆ‡âˆ‡paâ‚,
					pY=pY,
					âˆ‡pY=âˆ‡pY)
end
