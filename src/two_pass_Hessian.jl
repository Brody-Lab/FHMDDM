
"""
	check_twopasshessian(model)

Compare the automatically computed and hand-coded gradients and hessians with respect to the parameters being fitted in their real space

ARGUMENT
-`model`: a structure containing the data, parameters, and hyperparameters of a factorial hidden-Markov drift-diffusion model

RETURN
-`absdiffâ„“`: absolute difference in the log-likelihood evaluted using the algorithm bein automatically differentiated and the hand-coded algorithm
-`absdiffâˆ‡`: absolute difference in the gradients
-`absdiffâˆ‡âˆ‡`: absolute difference in the hessians

EXAMPLE
```julia-repl
julia> using FHMDDM
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_04_14_test/data.mat"; randomize=true)
julia> absdiffâ„“, absdiffâˆ‡, absdiffâˆ‡âˆ‡ = FHMDDM.check_twopasshessian(model)
```
"""
function check_twopasshessian(model::Model)
	concatenatedÎ¸, indexÎ¸ = concatenateparameters(model)
	â„“hand, âˆ‡hand, âˆ‡âˆ‡hand = FHMDDM.twopasshessian!(model,concatenatedÎ¸,indexÎ¸)
	f(x) = loglikelihood(x, indexÎ¸, model)
	â„“auto = f(concatenatedÎ¸)
	âˆ‡auto = ForwardDiff.gradient(f, concatenatedÎ¸)
	âˆ‡âˆ‡auto = ForwardDiff.hessian(f, concatenatedÎ¸)
	return abs(â„“auto-â„“hand), abs.(âˆ‡auto .- âˆ‡hand), abs.(âˆ‡âˆ‡auto .- âˆ‡âˆ‡hand)
end

"""
	twopasshessian!(model, concatenatedÎ¸, indexÎ¸)

Sort a vector of parameters and compute the log-likelihood, its gradient, and its hessian

ARGUMENT
-`model`: a structure containing the data and hyperparameters of the factorial hidden drift-diffusion model
-`concatenatedÎ¸`: values of the parameters in real space concatenated as a vector
-`indexÎ¸`: a structure for sorting (i.e., un-concatenating) the parameters

RETURN
-`â„“`: log-likelihood
-`âˆ‡â„“`: gradient of the log-likelihood with respect to fitted parameters in real space
-`âˆ‡âˆ‡â„“`: Hessian matrix of the log-likelihood with respect to fitted parameters in real space

EXAMPLE
```julia-repl
julia> using FHMDDM
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_04_14_test/data.mat"; randomize=true)
julia> concatenatedÎ¸, indexÎ¸ = FHMDDM.concatenateparameters(model)
julia> â„“, âˆ‡â„“, âˆ‡âˆ‡â„“ = FHMDDM.twopasshessian!(model, concatenatedÎ¸, indexÎ¸)
```
"""
function twopasshessian!(model::Model, concatenatedÎ¸::Vector{<:Real}, indexÎ¸::IndexÎ¸)
	sortparameters!(model, concatenatedÎ¸, indexÎ¸)
	â„“, âˆ‡â„“, âˆ‡âˆ‡â„“ = twopasshessian(model)
	native2real!(âˆ‡â„“, âˆ‡âˆ‡â„“, indexÎ¸.latentÎ¸, model)
	âˆ‡â„“ = sortparameters(indexÎ¸.latentÎ¸, âˆ‡â„“)
	âˆ‡âˆ‡â„“ = sortparameters(indexÎ¸.latentÎ¸, âˆ‡âˆ‡â„“)
	return â„“, âˆ‡â„“, âˆ‡âˆ‡â„“
end

"""
	twopasshessian!(model)

Compute the hessian as the Jacobian of the expectation conjugate gradient

ARGUMENT
-`model`: a structure containing the parameters, data, and hyperparameters of a factorial hidden Markov drift-diffusion model

RETURN
-`â„“`: log-likelihood
-`âˆ‡â„“`: gradient of the log-likelihood with respect to fitted parameters in real space
-`âˆ‡âˆ‡â„“`: Hessian matrix of the log-likelihood with respect to fitted parameters in real space

EXAMPLE
```julia-repl
julia> using FHMDDM
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_04_14_test/data.mat"; randomize=true)
julia> â„“, âˆ‡â„“, âˆ‡âˆ‡â„“ = FHMDDM.twopasshessian(model)
```
"""
function twopasshessian(model::Model)
	@unpack trialsets = model
	sameacrosstrials = FHMDDM.Sameacrosstrials(model)
	memoryforhessian = FHMDDM.Memoryforhessian(model, sameacrosstrials)
	for s in eachindex(trialsets)
		glmÎ¸s = collect(trialsets[s].mpGLMs[n].Î¸ for n = 1:length(trialsets[s].mpGLMs))
		for m in eachindex(trialsets[s].trials)
			twopasshessian!(memoryforhessian, glmÎ¸s, s, sameacrosstrials, model.Î¸native, trialsets[s].trials[m])
		end
	end
	@unpack â„“, âˆ‡â„“, âˆ‡âˆ‡â„“ = memoryforhessian
	for i = 1:size(âˆ‡âˆ‡â„“,1)
		for j = i+1:size(âˆ‡âˆ‡â„“,2)
			âˆ‡âˆ‡â„“[j,i] = âˆ‡âˆ‡â„“[i,j]
		end
	end
	return â„“[1], âˆ‡â„“, âˆ‡âˆ‡â„“
end

"""
	twopasshessian!

Compute the hessian for one trial as the Jacobian of the expectation conjugate gradient

MODIFIED ARGUMENT
-`â„“`: log-likelihood
-`âˆ‡â„“`: gradient of the log-likelihood with respect to fitted parameters in real space
-`âˆ‡âˆ‡â„“`: Hessian matrix of the log-likelihood with respect to fitted parameters in real space
-`P`: structure containing pre-allocated memory fro computing the transition matrix

UNMODIFIED ARGUMENT
-`glmÎ¸s`: a vector whose each element is a structure containing the parameters of of the generalized linear model of a neuron
-`s`: index of the trialset
-`memoryforhessian`: a structure containing quantities used in each trial
-`Î¸native`: a structure containing parameters specifying the latent variables in their native space
-`trial`: a structure containing information on the sensory stimuli, spike trains, input to each neuron's GLM, and behavioral choice
"""
function twopasshessian!(memoryforhessian::Memoryforhessian,
						 glmÎ¸s::Vector{<:GLMÎ¸},
						 s::Integer,
						 sameacrosstrials::Sameacrosstrials,
 						 Î¸native::LatentÎ¸,
						 trial::Trial)
	@unpack â„“, âˆ‡â„“, âˆ‡âˆ‡â„“, f, âˆ‡f, D, âˆ‡D, âˆ‡b, âˆ‡Î· = memoryforhessian
	@unpack P, âˆ‡paâ‚, âˆ‡âˆ‡paâ‚, Aáµƒinput, âˆ‡Aáµƒinput, âˆ‡âˆ‡Aáµƒinput = memoryforhessian
	@unpack L, Î», âˆ‡logpy, âˆ‡âˆ‡logpy, pY, âˆ‡pY, âˆ‚pYğ‘‘_âˆ‚Ïˆ = memoryforhessian
	@unpack dğ›_dB, Î”t, K, Î = sameacrosstrials
	@unpack Aáµƒsilent, âˆ‡Aáµƒsilent, âˆ‡âˆ‡Aáµƒsilent, Aá¶œ, Aá¶œáµ€, âˆ‡Aá¶œ, âˆ‡Aá¶œáµ€, Ï€á¶œ, Ï€á¶œáµ€, âˆ‡Ï€á¶œ, âˆ‡Ï€á¶œáµ€ = sameacrosstrials
	@unpack indexÎ¸_paâ‚, indexÎ¸_paâ‚œaâ‚œâ‚‹â‚, indexÎ¸_pcâ‚, indexÎ¸_pcâ‚œcâ‚œâ‚‹â‚, indexÎ¸_Ïˆ,  nÎ¸_paâ‚, nÎ¸_paâ‚œaâ‚œâ‚‹â‚, nÎ¸_pcâ‚, nÎ¸_pcâ‚œcâ‚œâ‚‹â‚, nÎ¸_Ïˆ, index_paâ‚_in_Î¸, index_paâ‚œaâ‚œâ‚‹â‚_in_Î¸, index_pcâ‚_in_Î¸, index_pcâ‚œcâ‚œâ‚‹â‚_in_Î¸, index_Ïˆ_in_Î¸ = sameacrosstrials
	indexÎ¸_py = sameacrosstrials.indexÎ¸_py[s]
	nÎ¸_py = sameacrosstrials.nÎ¸_py[s]
	indexÎ¸_pY = sameacrosstrials.indexÎ¸_pY[s]
	nÎ¸_pY = sameacrosstrials.nÎ¸_pY[s]
	index_pY_in_Î¸ = sameacrosstrials.index_pY_in_Î¸[s]
	indexÎ¸_trialset = sameacrosstrials.indexÎ¸_trialset[s]
	nÎ¸_trialset = sameacrosstrials.nÎ¸_trialset[s]
	nneurons = length(trial.spiketrainmodels)
	@unpack clicks, spiketrainmodels = trial
	adaptedclicks = FHMDDM.âˆ‡âˆ‡adapt(clicks, Î¸native.k[1], Î¸native.Ï•[1])
	# conditional likelihood of population spiking and its gradient; gradient and Hessian of the conditional log-likelihood of individual neurons' spiking
	for n in eachindex(L)
		FHMDDM.conditional_linear_predictor!(L[n], dğ›_dB, spiketrainmodels[n], glmÎ¸s[n])
		for t = 1:trial.ntimesteps
			FHMDDM.conditionalrate!(Î»[n][t], L[n][t])
			FHMDDM.âˆ‡âˆ‡conditional_log_likelihood!(âˆ‡logpy[t][n], âˆ‡âˆ‡logpy[t][n], L[n][t], Î»[n][t], Î”t, dğ›_dB, t, trial.spiketrainmodels[n])
		end
	end
	for t = 1:trial.ntimesteps
		for jk in eachindex(pY[t])
			pY[t][jk] = FHMDDM.Poissonlikelihood(Î»[1][t][jk]*Î”t, spiketrainmodels[1].ğ²[t])
			for n=2:nneurons
				pY[t][jk] *= FHMDDM.Poissonlikelihood(Î»[n][t][jk]*Î”t, spiketrainmodels[n].ğ²[t])
			end
		end
		q = 0
		for n=1:nneurons
			for i in eachindex(âˆ‡logpy[t][n])
				q+=1
				for j=1:Î
					for k=1:K
						âˆ‡pY[t][q][j,k] = âˆ‡logpy[t][n][i][j,k]*pY[t][j,k]
					end
				end
			end
		end
	end
	differentiate_pYğ‘‘_wrt_Ïˆ!(âˆ‚pYğ‘‘_âˆ‚Ïˆ, pY[trial.ntimesteps], trial.choice)
	conditionallikelihood!(pY[trial.ntimesteps], trial.choice, Î¸native.Ïˆ[1])
	for i in eachindex(âˆ‡pY[trial.ntimesteps])
		conditionallikelihood!(âˆ‡pY[trial.ntimesteps][i], trial.choice, Î¸native.Ïˆ[1])
	end
	# first forward step
	for q in eachindex(âˆ‡f[1])
		âˆ‡f[1][q] .= 0
	end
	FHMDDM.âˆ‡âˆ‡priorprobability!(âˆ‡âˆ‡paâ‚, âˆ‡paâ‚, P, trial.previousanswer)
	paâ‚ = copy(P.ğ›‘) # save for later
	pYâ‚â¨€pcâ‚ = pY[1] .* Ï€á¶œáµ€
	paâ‚â¨€pcâ‚ = paâ‚ .* Ï€á¶œáµ€
	for i = 1:nÎ¸_pY
		for j=1:Î
			for k = 1:K
				âˆ‡f[1][indexÎ¸_pY[i]][j,k] = âˆ‡pY[1][i][j,k]*paâ‚â¨€pcâ‚[j,k]
			end
		end
	end
	for i = 1:nÎ¸_paâ‚
		for j=1:Î
			for k = 1:K
				âˆ‡f[1][indexÎ¸_paâ‚[i]][j,k] = pYâ‚â¨€pcâ‚[j,k]*âˆ‡paâ‚[i][j]
			end
		end
	end
	for i = 1:nÎ¸_pcâ‚
		for j=1:Î
			for k = 1:K
				âˆ‡f[1][indexÎ¸_pcâ‚[1]][j,k] = pY[1][j,k]*paâ‚[j]*âˆ‡Ï€á¶œ[i][k]
			end
		end
	end
	for j=1:Î
		for k = 1:K
			f[1][j,k] = pY[1][j,k] * paâ‚â¨€pcâ‚[j,k]
		end
	end
	D[1] = sum(f[1])
	forward!(âˆ‡D[1], f[1], âˆ‡f[1], â„“, âˆ‡â„“, D[1])
	for t=2:trial.ntimesteps
		if t âˆˆ clicks.inputtimesteps
			update_for_âˆ‡âˆ‡transition_probabilities!(P, adaptedclicks, clicks, t)
			âˆ‡âˆ‡transitionmatrix!(âˆ‡âˆ‡Aáµƒinput[t], âˆ‡Aáµƒinput[t], Aáµƒinput[t], P)
			Aáµƒ = Aáµƒinput[t]
			âˆ‡Aáµƒ = âˆ‡Aáµƒinput[t]
			âˆ‡âˆ‡Aáµƒ = âˆ‡âˆ‡Aáµƒinput[t]
		else
			Aáµƒ = Aáµƒsilent
			âˆ‡Aáµƒ = âˆ‡Aáµƒsilent
			âˆ‡âˆ‡Aáµƒ = âˆ‡âˆ‡Aáµƒsilent
		end
		fâ¨‰Aá¶œáµ€ = f[t-1] * Aá¶œáµ€
		Aáµƒâ¨‰fâ¨‰Aá¶œáµ€ = Aáµƒ * fâ¨‰Aá¶œáµ€
		Aáµƒâ¨‰f = Aáµƒ * f[t-1]
		for q in indexÎ¸_trialset
			i_aâ‚œ = index_paâ‚œaâ‚œâ‚‹â‚_in_Î¸[q]
			i_câ‚œ = index_pcâ‚œcâ‚œâ‚‹â‚_in_Î¸[q]
			i_y = index_pY_in_Î¸[q]
			i_Ïˆ = index_Ïˆ_in_Î¸[q]
			if i_aâ‚œ > 0
				âˆ‡f[t][q] = pY[t] .* (âˆ‡Aáµƒ[i_aâ‚œ] * fâ¨‰Aá¶œáµ€ .+ Aáµƒ * âˆ‡f[t-1][q] * Aá¶œáµ€)
			elseif i_câ‚œ > 0
				âˆ‡f[t][q] = pY[t] .* (Aáµƒâ¨‰f * âˆ‡Aá¶œáµ€[i_câ‚œ] .+ Aáµƒ * âˆ‡f[t-1][q] * Aá¶œáµ€)
			elseif i_y > 0
				âˆ‡f[t][q] = âˆ‡pY[t][i_y] .* Aáµƒâ¨‰fâ¨‰Aá¶œáµ€ .+ pY[t] .* (Aáµƒ * âˆ‡f[t-1][q] * Aá¶œáµ€)
			elseif i_Ïˆ > 0
				if t==trial.ntimesteps
					âˆ‡f[t][q] = âˆ‚pYğ‘‘_âˆ‚Ïˆ .* Aáµƒâ¨‰fâ¨‰Aá¶œáµ€
				else
					âˆ‡f[t][q] .= 0.0
				end
			else
				âˆ‡f[t][q] = pY[t] .* (Aáµƒ * âˆ‡f[t-1][q] * Aá¶œáµ€)
			end
		end
		f[t] = pY[t] .* Aáµƒâ¨‰fâ¨‰Aá¶œáµ€
		D[t] = sum(f[t])
		forward!(âˆ‡D[t], f[t], âˆ‡f[t], â„“, âˆ‡â„“, D[t])
	end
	bâ‚œ = ones(Î,K)
	for t = trial.ntimesteps:-1:1
		Î³ = f[t] # resuse memory
		âˆ‡Î³ = âˆ‡f[trial.ntimesteps] # resuse memory
		if t == trial.ntimesteps
			for q in indexÎ¸_trialset
				âˆ‡b[q] .= 0
			end
			# the p(ğ‘‘ âˆ£ aâ‚œ, câ‚œ) term
			q = indexÎ¸_Ïˆ[1]
			âˆ‡â„“[q] += FHMDDM.expectation_derivative_logpğ‘‘_wrt_Ïˆ(trial.choice, Î³, Î¸native.Ïˆ[1])
			âˆ‡âˆ‡â„“[q,q] += FHMDDM.expectation_second_derivative_logpğ‘‘_wrt_Ïˆ(trial.choice, Î³, Î¸native.Ïˆ[1])
			for r in indexÎ¸_trialset
				if r < q
					âˆ‡âˆ‡â„“[r,q] += FHMDDM.expectation_derivative_logpğ‘‘_wrt_Ïˆ(trial.choice, âˆ‡Î³[r], Î¸native.Ïˆ[1])
				else
					âˆ‡âˆ‡â„“[q,r] += FHMDDM.expectation_derivative_logpğ‘‘_wrt_Ïˆ(trial.choice, âˆ‡Î³[r], Î¸native.Ïˆ[1])
				end
			end
		else
			if t+1 âˆˆ clicks.inputtimesteps
				Aáµƒâ‚œâ‚Šâ‚ = Aáµƒinput[t+1]
				âˆ‡Aáµƒâ‚œâ‚Šâ‚ = âˆ‡Aáµƒinput[t+1]
			else
				Aáµƒâ‚œâ‚Šâ‚ = Aáµƒsilent
				âˆ‡Aáµƒâ‚œâ‚Šâ‚ = âˆ‡Aáµƒsilent
			end
			Aáµƒâ‚œâ‚Šâ‚áµ€ = transpose(Aáµƒâ‚œâ‚Šâ‚)
			bâ‚œâ‚Šâ‚ = bâ‚œ # rename
			bâ‚œâ‚Šâ‚â¨€pYâ‚œâ‚Šâ‚ = bâ‚œâ‚Šâ‚ .* pY[t+1]
			bâ‚œ = transpose(Aáµƒâ‚œâ‚Šâ‚) * (bâ‚œâ‚Šâ‚â¨€pYâ‚œâ‚Šâ‚./D[t+1]) * Aá¶œ
			for q in indexÎ¸_trialset
				i_aâ‚œ = index_paâ‚œaâ‚œâ‚‹â‚_in_Î¸[q]
				i_câ‚œ = index_pcâ‚œcâ‚œâ‚‹â‚_in_Î¸[q]
				i_y = index_pY_in_Î¸[q]
				i_Ïˆ = index_Ïˆ_in_Î¸[q]
				if i_aâ‚œ > 0
					dAáµƒâ‚œâ‚Šâ‚áµ€_dÎ¸ = transpose(âˆ‡Aáµƒâ‚œâ‚Šâ‚[i_aâ‚œ])
					âˆ‡b[q] = ((dAáµƒâ‚œâ‚Šâ‚áµ€_dÎ¸*bâ‚œâ‚Šâ‚â¨€pYâ‚œâ‚Šâ‚ .+ Aáµƒâ‚œâ‚Šâ‚áµ€*(âˆ‡b[q].*pY[t+1])) * Aá¶œ .- bâ‚œ.*âˆ‡D[t+1][q])./D[t+1]
				elseif i_câ‚œ > 0
					âˆ‡b[q] = (Aáµƒâ‚œâ‚Šâ‚áµ€ * (bâ‚œâ‚Šâ‚â¨€pYâ‚œâ‚Šâ‚*âˆ‡Aá¶œ[i_câ‚œ] .+ (âˆ‡b[q].*pY[t+1]) * Aá¶œ) .- bâ‚œ.*âˆ‡D[t+1][q])./D[t+1]
				elseif i_y > 0
					âˆ‡b[q] = (Aáµƒâ‚œâ‚Šâ‚áµ€ * (bâ‚œâ‚Šâ‚.*âˆ‡pY[t+1][i_y] .+ (âˆ‡b[q].*pY[t+1])) * Aá¶œ .- bâ‚œ.*âˆ‡D[t+1][q])./D[t+1]
				elseif (i_Ïˆ > 0) && (t == trial.ntimesteps-1)
					âˆ‡b[q] = (Aáµƒâ‚œâ‚Šâ‚áµ€ * (bâ‚œâ‚Šâ‚.*âˆ‚pYğ‘‘_âˆ‚Ïˆ) * Aá¶œ .- bâ‚œ.*âˆ‡D[t+1][q])./D[t+1]
				else
					âˆ‡b[q] = (Aáµƒâ‚œâ‚Šâ‚áµ€*(âˆ‡b[q].*pY[t+1])*Aá¶œ .- bâ‚œ.*âˆ‡D[t+1][q])./D[t+1]
				end
			end
			for q in indexÎ¸_trialset
				for ij in eachindex(âˆ‡Î³[q])
					âˆ‡Î³[q][ij] = âˆ‡f[t][q][ij]*bâ‚œ[ij] + f[t][ij]*âˆ‡b[q][ij]
				end
			end
			Î³ .*= bâ‚œ # modify Î³ only after modifying âˆ‡Î³ because Î³ shares memory with f[t]
		end
		if t > 1
			if t âˆˆ clicks.inputtimesteps
				Aáµƒ = Aáµƒinput[t]
				âˆ‡Aáµƒ = âˆ‡Aáµƒinput[t]
				âˆ‡âˆ‡Aáµƒ = âˆ‡âˆ‡Aáµƒinput[t]
			else
				Aáµƒ = Aáµƒsilent
				âˆ‡Aáµƒ = âˆ‡Aáµƒsilent
				âˆ‡âˆ‡Aáµƒ = âˆ‡âˆ‡Aáµƒsilent
			end
			# the p(aâ‚œ âˆ£ aâ‚œâ‚‹â‚) term
			for i = 1:nÎ¸_paâ‚œaâ‚œâ‚‹â‚
				q = indexÎ¸_paâ‚œaâ‚œâ‚‹â‚[i]
				Î· = sum_product_over_states(D[t], f[t-1], bâ‚œ, pY[t], âˆ‡Aáµƒ[i], Aá¶œ)
				âˆ‡â„“[q] += Î·
				for r in indexÎ¸_trialset
 					if (r >= q) && r != indexÎ¸_Ïˆ[1]
						âˆ‡âˆ‡â„“[q,r] += sum_product_over_states(D[t], âˆ‡f[t-1][r], bâ‚œ, pY[t], âˆ‡Aáµƒ[i], Aá¶œ)
						âˆ‡âˆ‡â„“[q,r] += sum_product_over_states(D[t], f[t-1], âˆ‡b[r], pY[t], âˆ‡Aáµƒ[i], Aá¶œ)
						âˆ‡âˆ‡â„“[q,r] -= Î·*âˆ‡D[t][r]/D[t]
						j = index_pY_in_Î¸[r]
						if j > 0
							âˆ‡âˆ‡â„“[q,r] += sum_product_over_states(D[t], f[t-1], bâ‚œ, âˆ‡pY[t][j], âˆ‡Aáµƒ[i], Aá¶œ)
						end
						j = index_paâ‚œaâ‚œâ‚‹â‚_in_Î¸[r]
						if j > 0
							âˆ‡âˆ‡â„“[q,r] += sum_product_over_states(D[t], f[t-1], bâ‚œ, pY[t], âˆ‡âˆ‡Aáµƒ[i,j], Aá¶œ)
						end
						j = index_pcâ‚œcâ‚œâ‚‹â‚_in_Î¸[r]
						if j > 0
							âˆ‡âˆ‡â„“[q,r] += sum_product_over_states(D[t], f[t-1], bâ‚œ, pY[t], âˆ‡Aáµƒ[i], âˆ‡Aá¶œ[j])
						end
					end
				end
			end
			# the p(câ‚œ âˆ£ câ‚œâ‚‹â‚) term
			for i = 1:nÎ¸_pcâ‚œcâ‚œâ‚‹â‚
				q = indexÎ¸_pcâ‚œcâ‚œâ‚‹â‚[i]
				Î· = sum_product_over_states(D[t], f[t-1], bâ‚œ, pY[t], Aáµƒ, âˆ‡Aá¶œ[i])
				âˆ‡â„“[q] += Î·
				for r in indexÎ¸_trialset
 					if (r >= q) && r != indexÎ¸_Ïˆ[1]
						âˆ‡âˆ‡â„“[q,r] += sum_product_over_states(D[t], âˆ‡f[t-1][r], bâ‚œ, pY[t], Aáµƒ, âˆ‡Aá¶œ[i])
						âˆ‡âˆ‡â„“[q,r] += sum_product_over_states(D[t], f[t-1], âˆ‡b[r], pY[t], Aáµƒ, âˆ‡Aá¶œ[i])
						âˆ‡âˆ‡â„“[q,r] -= Î·*âˆ‡D[t][r]/D[t]
						j = index_pY_in_Î¸[r]
						if j > 0
							âˆ‡âˆ‡â„“[q,r] += sum_product_over_states(D[t], f[t-1], bâ‚œ, âˆ‡pY[t][j], Aáµƒ, âˆ‡Aá¶œ[i])
						end
						j = index_paâ‚œaâ‚œâ‚‹â‚_in_Î¸[r]
						if j > 0
							âˆ‡âˆ‡â„“[q,r] += sum_product_over_states(D[t], f[t-1], bâ‚œ, pY[t], âˆ‡Aáµƒ[j], âˆ‡Aá¶œ[i])
						end
					end
				end
			end
		end
		# the p(yâ‚™â‚œ âˆ£ aâ‚œ, câ‚œ) term
		for n = 1:length(indexÎ¸_py)
			for i = 1:nÎ¸_py[n]
				q = indexÎ¸_py[n][i]
				âˆ‡â„“[q] += sum_product_over_states(Î³, âˆ‡logpy[t][n][i])
				for r in indexÎ¸_trialset
					if r >=q
						âˆ‡âˆ‡â„“[q,r] += sum_product_over_states(âˆ‡Î³[r], âˆ‡logpy[t][n][i])
					end
				end
				for j = i:nÎ¸_py[n]
					r = indexÎ¸_py[n][j]
					âˆ‡âˆ‡â„“[q,r] += sum_product_over_states(Î³, âˆ‡âˆ‡logpy[t][n][i,j])
				end
			end
		end
	end
	#last backward step
	t = 1
	# the p(aâ‚) term
	for i = 1:nÎ¸_paâ‚
		q = indexÎ¸_paâ‚[i]
		Î· = sum_product_over_states(D[t], bâ‚œ, pY[t], âˆ‡paâ‚[i], Ï€á¶œ)
		âˆ‡â„“[q] += Î·
		for r in indexÎ¸_trialset
			if (r >= q) && r != indexÎ¸_Ïˆ[1]
				âˆ‡âˆ‡â„“[q,r] += sum_product_over_states(D[t], âˆ‡b[r], pY[t], âˆ‡paâ‚[i], Ï€á¶œ)
				âˆ‡âˆ‡â„“[q,r] -= Î·*âˆ‡D[t][r]/D[t]
				j = index_pY_in_Î¸[r]
				if j > 0
					âˆ‡âˆ‡â„“[q,r] += sum_product_over_states(D[t], bâ‚œ, âˆ‡pY[t][j], âˆ‡paâ‚[i], Ï€á¶œ)
				end
				j = index_paâ‚_in_Î¸[r]
				if j > 0
					âˆ‡âˆ‡â„“[q,r] += sum_product_over_states(D[t], bâ‚œ, pY[t], âˆ‡âˆ‡paâ‚[i,j], Ï€á¶œ)
				end
				j = index_pcâ‚_in_Î¸[r]
				if j > 0
					âˆ‡âˆ‡â„“[q,r] += sum_product_over_states(D[t], bâ‚œ, pY[t], âˆ‡paâ‚[i], âˆ‡Ï€á¶œ[j])
				end
			end
		end
	end
	# the p(câ‚) term
	for i = 1:nÎ¸_pcâ‚
		q = indexÎ¸_pcâ‚[i]
		Î· = sum_product_over_states(D[t], bâ‚œ, pY[t], paâ‚, âˆ‡Ï€á¶œ[i])
		âˆ‡â„“[q] += Î·
		for r in indexÎ¸_trialset
			if (r >= q) && r != indexÎ¸_Ïˆ[1]
				âˆ‡âˆ‡â„“[q,r] += sum_product_over_states(D[t], âˆ‡b[r], pY[t], paâ‚, âˆ‡Ï€á¶œ[i])
				âˆ‡âˆ‡â„“[q,r] -= Î·*âˆ‡D[t][r]/D[t]
				j = index_pY_in_Î¸[r]
				if j > 0
					âˆ‡âˆ‡â„“[q,r] += sum_product_over_states(D[t], bâ‚œ, âˆ‡pY[t][j], paâ‚, âˆ‡Ï€á¶œ[i])
				end
				j = index_paâ‚_in_Î¸[r]
				if j > 0
					âˆ‡âˆ‡â„“[q,r] += sum_product_over_states(D[t], bâ‚œ, pY[t], âˆ‡paâ‚[j], âˆ‡Ï€á¶œ[i])
				end
			end
		end
	end
	return nothing
end

"""
	forward!(D,âˆ‡D,f,âˆ‡f,â„“,âˆ‡â„“,t)

Normalize the forward term and its first-order partial derivatives and update the log-likelihood and its first-partial derivatives

PRE-MODIFIED ARGUMENT
-`f`: un-normalized forward term. Element `f[i,j]` corresponds to the un-normalized forward term in the i-th accumulator state and j-th coupling state
-`âˆ‡f`: first-order partial derivatives of the un-normalized forward term. Element `âˆ‡f[q][i,j]` corresponds to the derivative of the un-normalized forward term in the i-th accumulator state and j-th coupling state with respect to the q-th parameter.
-`â„“`: log-likelihood for the data before the current time-step
-`âˆ‡â„“`: gradient of the log-likelihood for the data before the current time-step

POST-MODIFICATION ARGUMENT
-`âˆ‡D`: gradient of the past-conditioned emissions likelihood for the current time step
-`âˆ‡f`: first-order partial derivatives of the normalized forward term. Element `âˆ‡f[q][i,j]` corresponds to `âˆ‚p{a(t)=Î¾(i), c(t)=j âˆ£ ğ˜(1:t)} / âˆ‚Î¸(q)`
-`f`: normalized forward term, which represents the joint conditional probability of the latent variables given the elapsed emissions. Element `f[i,j]` corresponds to `p{a(t)=Î¾(i), c(t)=j âˆ£ ğ˜(1:t)}`
-`âˆ‡â„“`: gradient of the log-likelihood for the data up to the current time-step
-`â„“`: log-likelihood for the data before the up to the current time-step

UNMODIFIED ARGUMENT
-`D`: past-conditioned emissions likelihood: `p{ğ˜(t) âˆ£ ğ˜(1:t-1))`

"""
function forward!(âˆ‡D::Vector{<:Real},
				f::Matrix{<:Real},
				âˆ‡f::Vector{<:Matrix{<:Real}},
				â„“::Vector{<:Real},
				âˆ‡â„“::Vector{<:Real},
				D::Real)
	f ./= D
	for i in eachindex(âˆ‡D)
		âˆ‡D[i] = sum(âˆ‡f[i])
	end
	for i in eachindex(âˆ‡f)
		for jk in eachindex(âˆ‡f[i])
			âˆ‡f[i][jk] = (âˆ‡f[i][jk] - f[jk]*âˆ‡D[i])/D
		end
	end
	â„“[1] += log(D)
	# for i in eachindex(âˆ‡â„“)
	# 	âˆ‡â„“[i] += âˆ‡D[i]/D
	# end
end

"""
	conditionalrate!(Î», L)

MODIFIED ARGUMENT
-`Î»`: matrix whose element `Î»[i,j]` is the Poisson rate given the i-th accumulator state and j-th coupling state

UNMODIFIED ARGUMENT
-`L`: matrix whose element `L[i,j]` is the linear predictor given the i-th accumulator state and j-th coupling state
"""
function conditionalrate!(Î»::Matrix{<:Real}, L::Matrix{<:Real})
	Î = size(Î»,1)
	for i = 1:Î
		Î»[i,1] = softplus(L[i,1])
	end
	Î»[:,2] .= Î»[cld(Î,2),1]
	return nothing
end

"""
	Poissonlikelihood(Î»Î”t, y)

Likelihood of observation `y` given Poisson rate `Î»Î”t`
"""
function Poissonlikelihood(Î»Î”t::Real, y::Integer)
	if y==0
		exp(-Î»Î”t)
	elseif y==1
		Î»Î”t/exp(Î»Î”t)
	elseif y == 2
		Î»Î”t^2 / exp(Î»Î”t) / 2
	else
		Î»Î”t^y / exp(Î»Î”t) / factorial(y)
	end
end


"""
	conditional_linear_predictor!(L, dğ›_dB, glmio, glmÎ¸)

MODIFIED ARGUMENT
-`L`: A vector of matrices whose element `L[t][i,j]` corresponds to the linear predictor at time t given the i-th accumulator state and j-th coupling state

UNMODFIED ARGUMENT
-`dğ›_dB`: normalized values of the accumulator
-`glmio`: input and observations of a neuron's glm
-`glmÎ¸`: parameters of a neuron's glm
"""
function conditional_linear_predictor!(L::Vector{<:Matrix{<:Real}},
									dğ›_dB::Vector{<:Real},
									glmio::SpikeTrainModel,
									glmÎ¸::GLMÎ¸)
	@unpack ğ”, ğš½ = glmio
	@unpack ğ®, ğ¯ = glmÎ¸
	ğ”ğ® = ğ”*ğ®
	ğš½ğ¯ = ğš½*ğ¯
	Î = length(dğ›_dB)
	zeroindex = cld(Î,2)
	for t = 1:length(ğ”ğ®)
		for i=1:Î-1
			L[t][i,1] = ğ”ğ®[t] + ğš½ğ¯[t]*dğ›_dB[i]
		end
		L[t][zeroindex,1] = ğ”ğ®[t]
		for i=zeroindex+1:Î
			L[t][i,1] = ğ”ğ®[t] + ğš½ğ¯[t]*dğ›_dB[i]
		end
		L[t][:,2] .= L[t][zeroindex,1]
	end
	return nothing
end

"""
	âˆ‡âˆ‡conditional_log_likelihood!(âˆ‡logpy, âˆ‡âˆ‡logpy, L, Î», Î”t, dğ›_dB, t, glmio)

Gradient and Hessian of the conditional log-likelihood

MODIFIED ARGUMENT
-`âˆ‡logpy`: Gradient of the conditional log-likelihood of a neuron's response at a single time. Element âˆ‡logpy[i][j,k] correponds to the partial derivative of log p{y(n,t) âˆ£ a(t)=Î¾(j), c(t)=k} with respect to the i-th parameter of the neuron's GLM
-`âˆ‡âˆ‡logpy`: Hessian of the conditional log-likelihood. Element âˆ‡logpy[i.j][k,l] correponds to the partial derivative of log p{y(n,t) âˆ£ a(t)=Î¾(k), c(t)=l} with respect to the i-th and j-th parameters of the neuron's GLM

UNMODIFIED ARGUMENT
-`L`: Conditional linear predictor whose element L[i,j] corresponds to a(t)=Î¾(i), c(t)=j
-`Î»`: Conditional Poisson whose element Î»[i,j] corresponds to a(t)=Î¾(i), c(t)=j
-`Î”t`: width of time step
-`dğ›_dB`: normalized value into which the accumulator is discretzed
-`t` time step
-`glmio`: input and observations of a neuron's Poisson mixture GLM
"""
function âˆ‡âˆ‡conditional_log_likelihood!(âˆ‡logpy::Vector{<:Matrix{<:Real}},
									âˆ‡âˆ‡logpy::Matrix{<:Matrix{<:Real}},
									L::Matrix{<:Real},
									Î»::Matrix{<:Real},
									Î”t::Real,
									dğ›_dB::Vector{<:Real},
									t::Integer,
									glmio::SpikeTrainModel)
	@unpack ğ”, ğš½, ğ² = glmio
	nğ® = size(ğ”,2)
	nğ¯ = size(ğš½,2)
	dL_dğ¯ = zeros(nğ¯)
	Î = size(L,1)
	zeroindex = cld(Î,2)
	for i = 1:Î
		dlogp_dL, dÂ²logp_dL = differentiate_twice_loglikelihood_wrt_linearpredictor(Î”t, L[i,1], Î»[i,1], ğ²[t])
		for j=1:nğ®
			âˆ‡logpy[j][i,1] = dlogp_dL*ğ”[t,j]
		end
		for j=1:nğ¯
			dL_dğ¯[j] = ğš½[t,j]*dğ›_dB[i]
			âˆ‡logpy[j+nğ®][i,1] = dlogp_dL*dL_dğ¯[j]
		end
		for j=1:nğ®
			for k=j:nğ®
				âˆ‡âˆ‡logpy[j,k][i,1] = dÂ²logp_dL*ğ”[t,j]*ğ”[t,k]
			end
			for k=1:nğ¯
				âˆ‡âˆ‡logpy[j,k+nğ®][i,1] = dÂ²logp_dL*ğ”[t,j]*dL_dğ¯[k]
			end
		end
		for j=1:nğ¯
			for k=j:nğ¯
				âˆ‡âˆ‡logpy[j+nğ®,k+nğ®][i,1] = dÂ²logp_dL * dL_dğ¯[j] * dL_dğ¯[k]
			end
		end
	end
	nğ®ğ¯ = nğ®+nğ¯
	for j = 1:nğ®ğ¯
		âˆ‡logpy[j][:,2] .= âˆ‡logpy[j][zeroindex,1]
		for k = j:nğ®ğ¯
			âˆ‡âˆ‡logpy[j,k][:,2] .= âˆ‡âˆ‡logpy[j,k][zeroindex,1]
		end
	end
	return nothing
end

"""
    differentiate_twice_loglikelihood_wrt_linearpredictor

Differentiate the log-likelihood of a Poisson GLM with respect to the linear predictor

The Poisson GLM is assumed to have a a softplus nonlinearity

ARGUMENT
-`Î”t`: duration of time step
-`L`: linear predictor at one time step
-`Î»`: Poisson rate
-`y`: observation at that time step

RETURN
-the first derivative with respect to the linear predictor
-the second derivative with respect to the linear predictor

EXAMPLE
```julia-repl
julia> using FHMDDM, ForwardDiff, LogExpFunctions
julia> Î”t = 0.01
julia> y = 2
julia> f(x) = let Î»Î”t = softplus(x[1])*Î”t; y*log(Î»Î”t)-Î»Î”t+log(factorial(y)); end
julia> x = rand(1)
julia> d1auto = ForwardDiff.gradient(f, x)
julia> d2auto = ForwardDiff.hessian(f, x)
julia> d1hand, d2hand = FHMDDM.differentiate_twice_loglikelihood_wrt_linearpredictor(Î”t, x[1], softplus(x[1]), y)
julia> abs(d1hand - d1auto[1])
julia> abs(d2hand - d2auto[1])
```
"""
function differentiate_twice_loglikelihood_wrt_linearpredictor(Î”t::AbstractFloat, L::Real, Î»::Real, y::Integer)
	dÎ»_dL = logistic(L)
	dÂ²Î»_dLdL = dÎ»_dL*(1-dÎ»_dL)
    if y > 0
        if L > -100.0
            dâ„“_dL = dÎ»_dL*(y/Î» - Î”t)
        else
            dâ„“_dL = y - dÎ»_dL*Î”t  # the limit of `dÎ»_dL/Î»` as x goes to -âˆ is 1
        end
		if L > -50.0
			dÂ²â„“_dLdL = y*(Î»*dÂ²Î»_dLdL - dÎ»_dL^2)/Î»^2 - dÂ²Î»_dLdL*Î”t # the limit of first second term is 0 as L goes to -âˆ
		else
			dÂ²â„“_dLdL = -dÂ²Î»_dLdL*Î”t
		end
    else
        dâ„“_dL = -dÎ»_dL*Î”t
		dÂ²â„“_dLdL = -dÂ²Î»_dLdL*Î”t
    end
	return dâ„“_dL, dÂ²â„“_dLdL
end

"""
	conditionallikelihood!(pY, ğ‘‘, Ïˆ)

Multiply the conditional likelihood of the choice to the conditional likelihood of spiking

ARGUMENT
-`pY`: a matrix whose element `pY[i,j]` corresponds to the i-th accumulator state and j-th coupling state and represents p{Y âˆ£ a(T)=Î¾(i), c(T)=j}
-`ğ‘‘`: left (false) or right (true) choice of the animal
-`Ïˆ`: lapse rate

MODIFIED ARGUMENT
-`pY`: p{Y, ğ‘‘ âˆ£ a(T)=Î¾(i), c(T)=j}
"""
function conditionallikelihood!(pY::Matrix{<:Real}, ğ‘‘::Bool, Ïˆ::Real)
	if ğ‘‘
		pğ‘‘_Î¾â» = Ïˆ/2
		pğ‘‘_Î¾âº = 1-Ïˆ/2
	else
		pğ‘‘_Î¾â» = 1-Ïˆ/2
		pğ‘‘_Î¾âº = Ïˆ/2
	end
	Î,K = size(pY)
	zeroindex = cld(Î,2)
	for j = 1:K
		for i = 1:zeroindex-1
			pY[i,j] *= pğ‘‘_Î¾â»
		end
		pY[zeroindex,j] *= 0.5
		for i = zeroindex+1:Î
			pY[i,j] *= pğ‘‘_Î¾âº
		end
	end
	return nothing
end

"""
	sum_product_over_states(D,fâ‚œâ‚‹â‚,bâ‚œ,Y,A,C)

Multiply terms across different states of the latent variables at consecutive time step and sum

ARGUMENT
-`Y`: similar to Î·, element Y[i,j] corresponds to i-th state of the accumulator at time t and the j-th state of the coupling at time t
-`A`: element A[i,j] corresponds to i-th state of the accumulator at time t and the j-th state of the accumulator at time t-1
-`C`: element C[i,j] corresponds to i-th state of the coupling at time t and the j-th state of the coupling at time t-1

RETURN
-`s`: sum of the product across all states of the two latent variables at two consecutive time steps
"""
function sum_product_over_states(D::Real, fâ‚œâ‚‹â‚::Matrix{<:Real}, bâ‚œ::Matrix{<:Real}, Y::Matrix{<:Real}, A::Matrix{<:Real}, C::Matrix{<:Real})
	s = 0.0
	Î,K = size(fâ‚œâ‚‹â‚)
	for iaâ‚œ = 1:Î
		for icâ‚œ = 1:K
			for iaâ‚œâ‚‹â‚ = 1:Î
				for icâ‚œâ‚‹â‚ = 1:K
					s += fâ‚œâ‚‹â‚[iaâ‚œâ‚‹â‚,icâ‚œâ‚‹â‚]*bâ‚œ[iaâ‚œ,icâ‚œ]*Y[iaâ‚œ,icâ‚œ]*A[iaâ‚œ,iaâ‚œâ‚‹â‚]*C[icâ‚œ, icâ‚œâ‚‹â‚]
				end
			end
		end
	end
	return s/D
end

"""
	sum_product_over_states(D, b,Y,A,C)

Multiply terms across different states of the latent variables at a single time step and sum

ARGUMENT
-`Y`: similar to Î·, element Y[i,j] corresponds to i-th state of the accumulator at time t and the j-th state of the coupling at time t
-`A`: element A[i] corresponds to the i-th state of the accumulator at time t
-`C`: element C[j] corresponds to the j-th state of the coupling at time t-1

RETURN
-`s`: sum of the product across all states of the two latent variables at a single time step
"""
function sum_product_over_states(D::Real, b::Matrix{<:Real}, Y::Matrix{<:Real}, A::Vector{<:Real}, C::Vector{<:Real})
	s = 0.0
	Î,K = size(b)
	for iaâ‚œ = 1:Î
		for icâ‚œ = 1:K
			s += b[iaâ‚œ,icâ‚œ]*Y[iaâ‚œ,icâ‚œ]*A[iaâ‚œ]*C[icâ‚œ]
		end
	end
	return s/D
end

"""
	sum_product_over_states(Î³,Y)

Multiply terms across different states of the latent variables at consecutive time step and sum

ARGUMENT
-`Î³`: element Î³[i,j] corresponds to i-th state of the accumulator at time t and the j-th state of the coupling at time t
-`Y`: similar to Î·, element Y[i,j] corresponds to i-th state of the accumulator at time t and the j-th state of the coupling at time t

RETURN
-`s`: sum of the product across all states of the two latent variables at two consecutive time steps
"""
function sum_product_over_states(Î³::Matrix{<:Real}, Y::Matrix{<:Real})
	s = 0.0
	Î,K = size(Î³)
	for j = 1:Î
		for k = 1:K
			s+= Î³[j,k]*Y[j,k]
		end
	end
	return s
end

"""
	expectation_derivative_logpğ‘‘_wrt_Ïˆ(choice, Î³, Ïˆ)

Expectation of the derivative of the choice log-likelihood with respect to the lapse rate

ARGUMENT
-`choice`: a Boolean specifying whether the choice was to the right
-`Î³`: a matrix or vector of representing the posterior probability of the latent variables at the end of the trial (i.e., last time step). Element `Î³_end[i,j]` = p(aáµ¢=1, câ±¼=1 âˆ£ ğ˜, d). Rows correspond to states of the accumulator state variable ğš, and columns to states of the coupling variable ğœ.
-`Ïˆ`: a floating-point number specifying the lapse rate

RETURN
-the expectation under the posterior probability of the late variables
"""
function expectation_derivative_logpğ‘‘_wrt_Ïˆ(choice::Bool, Î³::Array{<:Real}, Ïˆ::Real)
	if choice
		dlogpğ‘‘_dÏˆ_Î¾â» = 1/Ïˆ
		dlogpğ‘‘_dÏˆ_Î¾âº = 1/(Ïˆ-2)
	else
		dlogpğ‘‘_dÏˆ_Î¾â» = 1/(Ïˆ-2)
		dlogpğ‘‘_dÏˆ_Î¾âº = 1/Ïˆ
	end
	Î = size(Î³,1)
	zeroindex = cld(Î,2)
	edll = 0.0 # expectation of the derivative of the log-likelihood
	for j = 1:size(Î³,2)
		for i = 1:zeroindex-1
			edll += Î³[i,j]*dlogpğ‘‘_dÏˆ_Î¾â»
		end
		for i = zeroindex+1:Î
			edll += Î³[i,j]*dlogpğ‘‘_dÏˆ_Î¾âº
		end
	end
	return edll
end

"""
	expectation_second_derivative_logpğ‘‘_wrt_Ïˆ(choice, Î³, Ïˆ)

Expectation of the second derivative of the choice log-likelihood with respect to the lapse rate

ARGUMENT
-`choice`: a Boolean specifying whether the choice was to the right
-`Î³`: a matrix or vector of representing the posterior probability of the latent variables at the end of the trial (i.e., last time step). Element `Î³_end[i,j]` = p(aáµ¢=1, câ±¼=1 âˆ£ ğ˜, d). Rows correspond to states of the accumulator state variable ğš, and columns to states of the coupling variable ğœ.
-`Ïˆ`: a floating-point number specifying the lapse rate

RETURN
-the expectation under the posterior probability of the late variables
"""
function expectation_second_derivative_logpğ‘‘_wrt_Ïˆ(choice::Bool, Î³::Array{<:Real}, Ïˆ::Real)
	if choice
		dÂ²logpğ‘‘_dÏˆdÏˆ_Î¾â» = -Ïˆ^-2
		dÂ²logpğ‘‘_dÏˆdÏˆ_Î¾âº = -(Ïˆ-2)^-2
	else
		dÂ²logpğ‘‘_dÏˆdÏˆ_Î¾â» = -(Ïˆ-2)^-2
		dÂ²logpğ‘‘_dÏˆdÏˆ_Î¾âº = -Ïˆ^-2
	end
	Î = size(Î³,1)
	zeroindex = cld(Î,2)
	edÂ²ll = 0.0 # expectation of the second derivative of the log-likelihood
	for j = 1:size(Î³,2)
		for i = 1:zeroindex-1
			edÂ²ll += Î³[i,j]*dÂ²logpğ‘‘_dÏˆdÏˆ_Î¾â»
		end
		for i = zeroindex+1:Î
			edÂ²ll += Î³[i,j]*dÂ²logpğ‘‘_dÏˆdÏˆ_Î¾âº
		end
	end
	return edÂ²ll
end

"""
	Memoryforhessian(model)

EXAMPLE
```julia-repl
julia> using FHMDDM
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_04_15/data.mat"; randomize=true)
julia> S = FHMDDM.Sameacrosstrials(model)
julia> M = FHMDDM.Memoryforhessian(model, S)
```
"""
function Memoryforhessian(model::Model, S::Sameacrosstrials)
	@unpack options, Î¸native, Î¸real, trialsets = model
	@unpack Î”t, K, Î = options
	maxtimesteps=maximum(map(trialset->maximum(map(trial->trial.ntimesteps, trialset.trials)), trialsets))
	max_nÎ¸_pY=maximum(S.nÎ¸_pY)
	max_nÎ¸_py=maximum(map(n->maximum(n), S.nÎ¸_py))
	maxneurons = maximum(map(trialset-> length(trialset.mpGLMs), trialsets))
	âˆ‡D=collect(zeros(S.nÎ¸_alltrialsets) for t=1:maxtimesteps)
	f = collect(zeros(Î,K) for t=1:maxtimesteps)
	âˆ‡f = collect(collect(zeros(Î,K) for q=1:S.nÎ¸_alltrialsets) for t=1:maxtimesteps)
	âˆ‡b = collect(zeros(Î,K) for q=1:S.nÎ¸_alltrialsets)
	âˆ‡Î· = collect(zeros(Î,K) for q=1:S.nÎ¸_alltrialsets)
	L = collect(collect(zeros(Î,K) for t=1:maxtimesteps) for n = 1:maxneurons)
	Î» = collect(collect(zeros(Î,K) for t=1:maxtimesteps) for n = 1:maxneurons)
	âˆ‡logpy = collect(collect(collect(zeros(Î,K) for q=1:max_nÎ¸_py) for n=1:maxneurons) for t=1:maxtimesteps)
	âˆ‡âˆ‡logpy=map(1:maxtimesteps) do t
				map(1:maxneurons) do n
					map(CartesianIndices((max_nÎ¸_py,max_nÎ¸_py))) do q
						zeros(Î,K)
					end
				end
			end
	Aáµƒinput=map(1:maxtimesteps) do t
				A = zeros(Î,Î)
				A[1,1] = A[Î,Î] = 1.0
				return A
			end
	âˆ‡Aáµƒinput = collect(collect(zeros(Î,Î) for q=1:S.nÎ¸_paâ‚œaâ‚œâ‚‹â‚) for t=1:maxtimesteps)
	âˆ‡âˆ‡Aáµƒinput = map(1:maxtimesteps) do t
					map(CartesianIndices((S.nÎ¸_paâ‚œaâ‚œâ‚‹â‚,S.nÎ¸_paâ‚œaâ‚œâ‚‹â‚))) do ij
						zeros(Î,Î)
					end
				end
	âˆ‡paâ‚ = collect(zeros(Î) for q=1:S.nÎ¸_paâ‚)
	âˆ‡âˆ‡paâ‚ = map(CartesianIndices((S.nÎ¸_paâ‚,S.nÎ¸_paâ‚))) do q
				zeros(Î)
			end
	pY = collect(zeros(Î,K) for t=1:maxtimesteps)
	âˆ‡pY = collect(collect(zeros(Î,K) for q=1:max_nÎ¸_pY) for t=1:maxtimesteps)
	Memoryforhessian(Aáµƒinput=Aáµƒinput,
					âˆ‡Aáµƒinput=âˆ‡Aáµƒinput,
					âˆ‡âˆ‡Aáµƒinput=âˆ‡âˆ‡Aáµƒinput,
					f=f,
					âˆ‡f=âˆ‡f,
					âˆ‡b=âˆ‡b,
					âˆ‡Î·=âˆ‡Î·,
					D = zeros(maxtimesteps),
					âˆ‡D=âˆ‡D,
					âˆ‡â„“=zeros(S.nÎ¸_alltrialsets),
					âˆ‡âˆ‡â„“=zeros(S.nÎ¸_alltrialsets,S.nÎ¸_alltrialsets),
					âˆ‚pYğ‘‘_âˆ‚Ïˆ=zeros(Î,K),
					L=L,
					Î»=Î»,
					âˆ‡logpy=âˆ‡logpy,
					âˆ‡âˆ‡logpy=âˆ‡âˆ‡logpy,
					P = Probabilityvector(Î”t, Î¸native, Î),
					âˆ‡paâ‚=âˆ‡paâ‚,
					âˆ‡âˆ‡paâ‚=âˆ‡âˆ‡paâ‚,
					pY=pY,
					âˆ‡pY=âˆ‡pY)
end
