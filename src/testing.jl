"""
    learn_ğ›ƒ_from_sample(model; niterations, tolerance, startfromgenerative)

Learn of the GLM filters ğ›ƒ from a sample of the factorial hidden-Markov drift-diffusion model

During learning, other parameters of the model are fixed to the values used to generate the sample of the results

ARGUMENT
-`generativemodel`: an existing model whose data provide the organizational basis of the simulated data, whose options are copied, and whose parameters are used as the generative parameters

OPTIONAL ARGUMENT
-`niterations`: number of iterations of the expectation-maximization algorithm
-`tolerance`: the smallest change in the objective value of the expectation-maximization algorithm before convergence is considered to be successful
-`startfromgenerative`: whether the initial values of the learning process is set the values used in model sampling. This is useful for testing whether a local minimum might be an issue
"""
function learn_ğ›ƒ_from_sample(generativemodel::FHMDDM;
                            niterations::Integer=100,
                            startfromgenerative::Bool=false,
                            tolerance::AbstractFloat = 1e-9)
    model = sample(generativemodel)
    @unpack data, mpGLM, options, Î¸ = model
    @unpack Aá¶», ğ›ƒ, Ï€á¶», Ïˆ, Î¸â‚ = Î¸
    if !startfromgenerative
        for i in eachindex(mpGLM)
            for n in eachindex(mpGLM[i])
                mpGLM[i][n].ğ® .= 10 .- 20rand(size(mpGLM[i][n].ğ”,2))
                for k in eachindex(mpGLM[i][n].ğ¯)
                    mpGLM[i][n].ğ¯[k] .= 10 .- 20rand(size(mpGLM[i][n].ğ•[1],2))
                end
                ğ›ƒ[i][n] .= vcat(mpGLM[i][n].ğ®, collect(Iterators.flatten(mpGLM[i][n].ğ¯)))
                options.Î¸â‚€.ğ›ƒ[i][n] .= ğ›ƒ[i][n]
            end
        end
    end
    quantities = EMquantities(data, options)
    @unpack Î³ = quantities
    ğ’¬_ğ›ƒ_previous = 0.0
    for j = 1:niterations
        E_step!(quantities, data, mpGLM, options, Aá¶», Î¸â‚, Ï€á¶», Ïˆ)
        estimatefilters!(mpGLM, Î³)
        ğ’¬_ğ›ƒ = expectation(mpGLM, Î³)
        println("Iteration ", j)
        println("ğ’¬_ğ›ƒ = ", ğ’¬_ğ›ƒ)
        println("ğ›ƒ[1][1] = ", regression_coefficients(mpGLM)[1][1])
        if j > 1 && abs(ğ’¬_ğ›ƒ - ğ’¬_ğ›ƒ_previous)/(1+abs(ğ’¬_ğ›ƒ_previous)) < tolerance
            break
        else
            ğ’¬_ğ›ƒ_previous = ğ’¬_ğ›ƒ
        end
    end
    return model
end

"""
    learn_Î¸â‚_from_sample(model; niterations, startfromgenerative, tolerance)

Learn of the parameters controlling the drift-diffusion dynamics (Î¸â‚) from a sample of the factorial hidden-Markov drift-diffusion model

During learning, other parameters of the model are fixed to the values used to generate the sample of the results

ARGUMENT
-`generativemodel`: an existing model whose data provide the organizational basis of the simulated data, whose options are copied, and whose parameters are used as the generative parameters

OPTIONAL ARGUMENT
-`compute_posterior_with_generative`: whether to compute the posterior probabilities using the parameter values used to generate the data
-`niterations`: number of iterations of the expectation-maximization algorithm
-`startfromgenerative`: whether the initial values of the learning process is set the values used in model sampling. This is useful for testing whether a local minimum might be an issue
-`tolerance`: the smallest change in the objective value of the expectation-maximization algorithm before convergence is considered to be successful
"""
function learn_Î¸â‚_from_sample(generativemodel::FHMDDM;
                              compute_posterior_with_generative::Bool=false,
                              niterations::Integer=100,
                              startfromgenerative::Bool=false,
                              tolerance::AbstractFloat = 1e-9)
    model = sample(generativemodel)
    if !startfromgenerative
        Î¸â‚â‚€ = Î˜â‚(model.options.Î¸â‚€.Î¸â‚,
                 model.options.Î¸â‚isfit,
                 model.options.Î¸â‚lowerbound,
                 model.options.Î¸â‚upperbound)
        Î¸â‚€ = Î¸FHMDDM(Aá¶»=model.Î¸.Aá¶»,
                     ğ›ƒ=model.Î¸.ğ›ƒ,
                     Ï€á¶»=model.Î¸.Ï€á¶»,
                     Ïˆ=model.Î¸.Ïˆ,
                     Î¸â‚=Î¸â‚â‚€)
    else
        Î¸â‚€ = model.Î¸
    end
    optionsdict = Dict(model.options)
    if haskey(optionsdict, "theta_0")
        optionsdict["theta_0"] = Dict(Î¸â‚€)
    else
        optionsdict = merge(optionsdict, Dict("theta_0" => Dict(Î¸â‚€)))
    end
    model = FHMDDM(data=model.data,
                   mpGLM=model.mpGLM,
                   options=FHMDDMoptions(optionsdict),
                   Î¸=Î¸â‚€)
    @unpack data, mpGLM, options, Î¸ = model
    @unpack Aá¶», ğ›ƒ, Ï€á¶», Ïˆ, Î¸â‚ = Î¸
    quantities = EMquantities(data, options)
    @unpack Aáµƒ, Ï‡áµƒ, Ï‡áµƒnb, Ï‡á¶», fb, Î³, Î³áµƒ, Î³á¶», pğ˜ğ‘‘ = quantities
    â„“previous = NaN
    println("Initial Î¸â‚ = ", Î¸â‚)
    for j = 1:niterations
        if compute_posterior_with_generative
            E_step!(quantities, data, mpGLM, options, Aá¶», generativemodel.Î¸.Î¸â‚, Ï€á¶», Ïˆ)
        else
            E_step!(quantities, data, mpGLM, options, Aá¶», Î¸â‚, Ï€á¶», Ïˆ)
        end
        Î¸â‚ = estimateÎ¸â‚(Ï‡áµƒnb, data, Î³áµƒ, options, Î¸â‚; minAáµƒ=eps())
        â„“ = loglikelihood!(quantities, model, Ïˆ, Î¸â‚)
        println("Iteration ", j, ": log-likelihood = ", â„“)
        println("Î¸â‚ = ", Î¸â‚)
        if j > 1 &&
           (â„“ - â„“previous)/abs(â„“previous) < tolerance # stops if the log-likelihood is no longer being maximized
           if â„“ < â„“previous
               println("The log-likelihood decreased from the previous iteration!")
           end
            break
        else
            â„“previous = â„“
        end
    end
    Î¸ = Î¸FHMDDM(Aá¶»=Aá¶», ğ›ƒ =ğ›ƒ, Ï€á¶»=Ï€á¶», Ïˆ =Ïˆ, Î¸â‚=Î¸â‚)
    FHMDDM(data=data, mpGLM=mpGLM, options=options, Î¸=Î¸)
end

"""
    learn_ğ›ƒ_Î¸â‚_from_sample(generativemodel; niterations, startfromgenerative, tolerance)


Learn the linear filters (ğ›ƒ) and the parameters controlling the drift-diffusion dynamics (Î¸â‚) from a sample of the factorial hidden-Markov drift-diffusion model

During learning, other parameters of the model are fixed to the values used to generate the sample of the results

ARGUMENT
-`generativemodel`: an existing model whose data provide the organizational basis of the simulated data, whose options are copied, and whose parameters are used as the generative parameters

OPTIONAL ARGUMENT
-`niterations`: number of iterations of the expectation-maximization algorithm
-`startfromgenerative`: whether the initial values of the learning process is set the values used in model sampling. This is useful for testing whether a local minimum might be an issue
-`tolerance`: the smallest change in the objective value of the expectation-maximization algorithm before convergence is considered to be successful
"""
function learn_ğ›ƒ_Î¸â‚_from_sample(generativemodel::FHMDDM;
                              niterations::Integer=100,
                              startfromgenerative::Bool=false,
                              tolerance::AbstractFloat = 1e-9)
    model = initialize_ğ›ƒ_Î¸â‚(sample(generativemodel))
    @unpack data, mpGLM, options, Î¸ = model
    @unpack Aá¶», ğ›ƒ, Ï€á¶», Ïˆ, Î¸â‚ = Î¸
    quantities = EMquantities(data, options)
    @unpack Î³áµƒ, Ï‡áµƒnb, Î³ = quantities
    ğ’¬previous = 0.
    ğ’¬ = 0.
    println("Initial Î¸â‚ = ", Î¸â‚)
    println("Initial ğ›ƒ[1][1] = ", regression_coefficients(mpGLM)[1][1])
    for j = 1:niterations
        E_step!(quantities, data, mpGLM, options, Aá¶», Î¸â‚, Ï€á¶», Ïˆ)
        estimatefilters!(mpGLM, Î³)
        ğ’¬_ğ›ƒ = expectation(mpGLM, Î³)
        Î¸â‚, ğ’¬_Î¸â‚ = estimateÎ¸â‚(Ï‡áµƒnb, data, Î³áµƒ, options, Î¸â‚)
        ğ’¬ = ğ’¬_ğ›ƒ + ğ’¬_Î¸â‚
        println("Iteration ", j, ": ğ’¬ = ", ğ’¬)
        println("Î¸â‚ = ", Î¸â‚)
        println("ğ›ƒ[1][1] = ", regression_coefficients(mpGLM)[1][1])
        if j > 1 && abs(ğ’¬ - ğ’¬previous)/(1+abs(ğ’¬previous)) < tolerance
            break
        else
            ğ’¬previous = ğ’¬
        end
    end
    Î¸ = Î¸FHMDDM(Aá¶»=Aá¶», ğ›ƒ=regression_coefficients(mpGLM), Ï€á¶»=Ï€á¶», Ïˆ =Ïˆ, Î¸â‚=Î¸â‚)
    FHMDDM(data=data, mpGLM=mpGLM, options=options, Î¸=Î¸)
end

"""
    initialize_ğ›ƒ_Î¸â‚(model; startfromgenerative)

Initialize the linear filters (ğ›ƒ) and the parameters controlling the drift-diffusion dynamics (Î¸â‚) and document them in the model's options

ARGUMENT
-`model`: an instance of the factorial hidden Markov drift-diffusion model

OPTIONAL ARGUMENT
-`startfromgenerative`: whether the initial values of the learning process is set the values used in model sampling. This is useful for testing whether a local minimum might be an issue

RETURN
-an instance of the factorial hidden Markov drift-diffusion model with the parameters in ğ›ƒ and Î¸â‚ initialized
"""
function initialize_ğ›ƒ_Î¸â‚(model::FHMDDM; startfromgenerative::Bool=false)
    @unpack data, mpGLM, options, Î¸ = model
    @unpack Aá¶», ğ›ƒ, Ï€á¶», Ïˆ, Î¸â‚ = Î¸
    if !startfromgenerative
        Î¸â‚â‚€ = Î˜â‚(options.Î¸â‚€.Î¸â‚,
                 options.Î¸â‚isfit,
                 options.Î¸â‚lowerbound,
                 options.Î¸â‚upperbound)
        for i in eachindex(mpGLM)
            for n in eachindex(mpGLM[i])
                mpGLM[i][n].ğ® .= 10 .- 20rand(size(mpGLM[i][n].ğ”,2))
                    for k in eachindex(mpGLM[i][n].ğ¯)
                        mpGLM[i][n].ğ¯[k] .= 10 .- 20rand(size(mpGLM[i][n].ğ•[1],2))
                    end
                ğ›ƒ[i][n] .= vcat(mpGLM[i][n].ğ®, collect(Iterators.flatten(mpGLM[i][n].ğ¯)))
            end
        end
        Î¸â‚€ = Î¸FHMDDM(Aá¶»=model.Î¸.Aá¶»,
                     ğ›ƒ=ğ›ƒ,
                     Ï€á¶»=model.Î¸.Ï€á¶»,
                     Ïˆ=model.Î¸.Ïˆ,
                     Î¸â‚=Î¸â‚â‚€)
    else
        Î¸â‚€ = model.Î¸
    end
    optionsdict = Dict(model.options)
    if haskey(optionsdict, "theta_0")
        optionsdict["theta_0"] = Dict(Î¸â‚€)
    else
        optionsdict = merge(optionsdict, Dict("theta_0" => Dict(Î¸â‚€)))
    end
    FHMDDM(data=data,
           mpGLM=mpGLM,
           options=FHMDDMoptions(optionsdict),
           Î¸=Î¸â‚€)
end

"""
    assess_mpGLM_iterative_optimization(model; nrepeats)

Check whether the learning of the linear filters of the mixture of Poisson generalized linear model (mpGLM) is correct, when the posteriors of the latent variable are computed from both the generative parameters and previous values of the linear filters.

ARGUMENT
-`generativemodel`: an existing model whose data provide the organizational basis of the simulated data, whose options are copied, and whose parameters are used as the generative parameters

OPTIONAL ARGUMENT

SAVED TO DISK
-the simulated data are saved in the file `dirname(model.options.datapath)*"/simulateddata.mat"`
-for the `i`-th simulation, a file `dirname(model.options.resultspath)*"/simulatedresults.mat"`

RETURN
-`nothing`
"""
function assess_mpGLM_optimization(generativemodel::FHMDDM;
                                   niterations::Integer=100,
                                   nrepeats::Integer=2,
                                   tolerance::AbstractFloat = 1e-9,
                                   generativeposterior::Bool=false,
                                   startfromgenerative::Bool=false)
    model = sample(generativemodel)
    ğ²generative= predict_expected_spiketrain(generativemodel, pğš, pğ³) # the spike rate used for the simulations
    optionsdict = Dict(generativemodel.options)
    optionsdict["datapath"] = dirname(generativemodel.options.datapath)*"/simulateddata.mat"
    optionsdict = merge(optionsdict, Dict("theta_generative" => Dict(generativemodel.Î¸)))
    dict = Dict("data" => map(trialset->Dict(trialset), simulateddata),
                "options" => optionsdict,
                "ygenerative" => ğ²generative)
    matwrite(optionsdict["datapath"], dict)

    for i = 1:nrepeats
        if startfromgenerative || generativeposterior
            mpGLM = MixturePoissonGLM(generativemodel.Î¸.ğ›ƒ, simulateddata, FHMDDMoptions(optionsdict))
        else
            mpGLM = MixturePoissonGLM(simulateddata, FHMDDMoptions(optionsdict)) # initialize with random coefficients
        end
        ğ›ƒâ‚€ = regression_coefficients(mpGLM)
        Î¸â‚€ = Î¸FHMDDM(Aá¶»= copy(generativemodel.Î¸.Aá¶»),
                     ğ›ƒ = ğ›ƒâ‚€,
                     Ï€á¶»= copy(generativemodel.Î¸.Ï€á¶»),
                     Ïˆ = copy(generativemodel.Î¸.Ïˆ),
                     Î¸â‚= Î˜â‚(pulse_input_DDM.flatten(generativemodel.Î¸.Î¸â‚)...))
        if haskey(optionsdict, "theta_0")
            optionsdict["theta_0"] = Dict(Î¸â‚€)
        else
            optionsdict = merge(optionsdict, Dict("theta_0" => Dict(Î¸â‚€)))
        end
        optionsdict["resultspath"] = dirname(generativemodel.options.resultspath)*"/simulatedresults"*string(i)*".mat"
        options = FHMDDMoptions(optionsdict)

        ğ’¬_ğ›ƒ_previous = 0.0
        for j = 1:niterations
            quantities = EMquantities(simulateddata, options)
            @unpack Î³ = quantities
            println("Repeat ", i, " iteration ", j)
            println("beginning of loop")
            println("ğ›ƒ[1][1] = ", regression_coefficients(mpGLM)[1][1])
            if generativeposterior
                mpGLM_Estep = mpGLM_generative
            else
                mpGLM_Estep = mpGLM
            end
            E_step!(quantities,
                    simulateddata,
                    mpGLM_Estep,
                    options,
                    generativemodel.Î¸.Aá¶»,
                    generativemodel.Î¸.Î¸â‚,
                    generativemodel.Î¸.Ï€á¶»,
                    generativemodel.Î¸.Ïˆ)
            estimatefilters!(mpGLM, Î³)
            ğ’¬_ğ›ƒ = expectation(mpGLM, Î³)
            println("end of loop")
            println("ğ’¬_ğ›ƒ = ", ğ’¬_ğ›ƒ)
            println("ğ›ƒ[1][1] = ", regression_coefficients(mpGLM)[1][1])
            if j > 1 && abs(ğ’¬_ğ›ƒ - ğ’¬_ğ›ƒ_previous)/(1+abs(ğ’¬_ğ›ƒ_previous)) < tolerance
                break
            else
                ğ’¬_ğ›ƒ_previous = ğ’¬_ğ›ƒ
            end
        end

        Î¸ = Î¸FHMDDM(Aá¶»= copy(generativemodel.Î¸.Aá¶»),
                    ğ›ƒ = regression_coefficients(mpGLM), # only parameters that are learned
                    Ï€á¶»= copy(generativemodel.Î¸.Ï€á¶»),
                    Ïˆ = copy(generativemodel.Î¸.Ïˆ),
                    Î¸â‚= Î˜â‚(pulse_input_DDM.flatten(generativemodel.Î¸.Î¸â‚)...))
        simulatedmodel = FHMDDM(data=simulateddata,
                                mpGLM=mpGLM,
                                options=options,
                                Î¸=Î¸)
        save(simulatedmodel)
    end
    return nothing
end

"""
    assess_Î¸â‚_optimization(model; nrepeats)

Check whether the learning of the drift-diffusion parameters is correct

ARGUMENT
-`generativemodel`: an existing model whose data provide the organizational basis of the simulated data, whose options are copied, and whose parameters are used as the generative parameters

OPTIONAL ARGUMENT
-`niterations`: number of iterations in the learning process. If `niterations` is less than 1, then `Î¸â‚` are learned using posteriors computed with the generative `Î¸â‚`
-`nrepeats`: number of repeated simulations starting from random initial values

SAVED TO DISK
-the simulated data are saved in the file `dirname(model.options.datapath)*"/simulateddata.mat"`
-for the `i`-th simulation, a file `dirname(model.options.resultspath)*"/simulatedresults.mat"`

RETURN
-`nothing`
"""
function assess_Î¸â‚_optimization(generativemodel::FHMDDM; niterations::Integer=100, nrepeats::Integer=2, tolerance::AbstractFloat = 1e-9)
    data, pğš, pğ³ = simulatedata(generativemodel)
    ğ²generative= predict_expected_spiketrain(generativemodel, pğš, pğ³) # the spike rate used for the simulations
    optionsdict = Dict(generativemodel.options)
    optionsdict["datapath"] = dirname(generativemodel.options.datapath)*"/simulateddata.mat"
    optionsdict = merge(optionsdict, Dict("theta_generative" => Dict(generativemodel.Î¸)))
    dict = Dict("data" => map(trialset->Dict(trialset), data),
                "options" => optionsdict,
                "ygenerative" => ğ²generative)
    matwrite(optionsdict["datapath"], dict)
    mpGLM_generative = MixturePoissonGLM(generativemodel.Î¸.ğ›ƒ, data, FHMDDMoptions(optionsdict))
    for i = 1:nrepeats
        Î¸â‚ = Î˜â‚(generativemodel.options.Î¸â‚€.Î¸â‚,
                generativemodel.options.Î¸â‚isfit,
                generativemodel.options.Î¸â‚lowerbound,
                generativemodel.options.Î¸â‚upperbound)
        println("randomly initialized Î¸â‚ = ", Î¸â‚)
        Î¸â‚€ = Î¸FHMDDM(Aá¶»= generativemodel.options.Î¸â‚€.Aá¶»,
                     ğ›ƒ = generativemodel.options.Î¸â‚€.ğ›ƒ,
                     Ï€á¶»= generativemodel.options.Î¸â‚€.Ï€á¶»,
                     Ïˆ = generativemodel.options.Î¸â‚€.Ïˆ,
                     Î¸â‚= Î¸â‚)
        if haskey(optionsdict, "theta_0")
            optionsdict["theta_0"] = Dict(Î¸â‚€)
        else
            optionsdict = merge(optionsdict, Dict("theta_0" => Dict(Î¸â‚€)))
        end
        optionsdict["resultspath"] = dirname(generativemodel.options.resultspath)*"/simulatedresults"*string(i)*".mat"
        options = FHMDDMoptions(optionsdict)

        quantities = EMquantities(data, options)
        @unpack Î³áµƒ, Ï‡áµƒnb = quantities
        if niterations < 1
            E_step!(quantities,
                    data,
                    mpGLM_generative,
                    options,
                    generativemodel.Î¸.Aá¶»,
                    generativemodel.Î¸.Î¸â‚,
                    generativemodel.Î¸.Ï€á¶»,
                    generativemodel.Î¸.Ïˆ)
            Î¸â‚, ğ’¬ = estimateÎ¸â‚(Ï‡áµƒnb, data, Î³áµƒ, options, Î¸â‚)
            println("Î¸â‚ = ", Î¸â‚)
        end
        ğ’¬previous = 0.
        for j = 1:niterations
            E_step!(quantities,
                    data,
                    mpGLM_generative,
                    options,
                    generativemodel.Î¸.Aá¶»,
                    Î¸â‚,
                    generativemodel.Î¸.Ï€á¶»,
                    generativemodel.Î¸.Ïˆ)
            Î¸â‚, ğ’¬ = estimateÎ¸â‚(Ï‡áµƒnb, data, Î³áµƒ, options, Î¸â‚)
            println("Repetition ", i, " iteration ", j, ": ğ’¬ = ", ğ’¬)
            println("Î¸â‚ = ", Î¸â‚)
            if j > 1
                if abs(ğ’¬ - ğ’¬previous)/(1+abs(ğ’¬previous)) < tolerance
                    break
                else
                    ğ’¬previous = ğ’¬
                end
            else
                ğ’¬previous = ğ’¬
            end
        end
        Î¸ = Î¸FHMDDM(Aá¶»= generativemodel.Î¸.Aá¶»,
                    ğ›ƒ = generativemodel.Î¸.ğ›ƒ,
                    Ï€á¶»= generativemodel.Î¸.Ï€á¶»,
                    Ïˆ = generativemodel.Î¸.Ïˆ,
                    Î¸â‚= Î¸â‚) # only parameters that are learned
        simulatedmodel = FHMDDM(data=data,
                                mpGLM=mpGLM_generative,
                                options=options,
                                Î¸=Î¸)
        save(simulatedmodel)
    end
    return nothing
end

"""
    loglikelihood(model)

Compute the (incomplete-data) log-likelihood log p(ğ˜, d âˆ£ Î¸)

INPUT
-`model`: an instance of FHM-DDM

OPTIONAL INPUT
-`quantities`: a structure containing quantities used in computing the posterior probabilities

RETURN
-a scalar representing the log-likelihood of the model
"""
function loglikelihood(model::FHMDDM;
                       quantities=EMquantities(model.data, model.options))
    @unpack data, mpGLM, options, Î¸ = model
    @unpack Aá¶», Î¸â‚, Ï€á¶», Ïˆ = Î¸
    @unpack Aáµƒ, Aáµƒsilent, D, f, pğ’‚, pğ˜ğ‘‘ = quantities
    accumulatortransitions!(Aáµƒ, Aáµƒsilent, data, options, Î¸â‚; minAáµƒ=0.)
    emissionslikelihood!(pğ˜ğ‘‘, data, mpGLM, Ïˆ) # `pğ˜ğ‘‘` is the conditional likelihood p(ğ˜â‚œ, d âˆ£ aâ‚œ, zâ‚œ)
    â„“ = 0.0 # log-likelihood
    for i in eachindex(data)
       for m in eachindex(data[i].trials)
           forward!(f[i][m], D[i][m], Aáµƒ[i][m], Aá¶», Ï€á¶», pğ’‚, pğ˜ğ‘‘[i][m])
           âˆD = 1.0
           for t in eachindex(f[i][m])
               âˆD *= D[i][m][t]
               f_unscaled = f[i][m][t]*âˆD # for t < T, f_unscaled = p(ğ˜â‚œ, aâ‚œ, zâ‚œ), and for t=T, f_unscaled = p(ğ˜â‚œ, d, aâ‚œ, zâ‚œ)
               â„“ += log(sum(f_unscaled))
           end
       end
    end
    return â„“
end

"""
"""
function testf1(X)
    pmap(x->xâ‹…x, X)
end

"""
"""
function testf2(X)
    pmap(t->X[t]â‹…X[t], 1:length(X))
end

"""
"""
function testf3(X)
    map(x->xâ‹…x, X)
end
