"""
    EKCrossValidation(trialsets)

Return an object containing cross-validated exponential kernel models

ARGUMENT
-`trialsets`: A vector whose each element is an instance of `Trialset`

OPTIONAL ARGUMENT
-`Î”t`: duration, in seconds, of each time step
-`kfold`: number of cross-validation folds
-`latency`: number of time steps the clicks are shifted
-`T`: number of time steps
"""
function EKCrossValidation(trialsets::Vector{<:Trialset}; Î”t::AbstractFloat=0.01, kfold::Integer=10, T::Integer=75, latency::Integer=1)
    trials = collect((trialset.trials for trialset in trialsets)...)
    stimulus_duration_s = map(trials) do trial
        if isempty(trial.clicks.time)
            0
        else
            maximum(trial.clicks.time)
        end
    end
    min_stimulus_duration_s = T*Î”t
    trials = trials[stimulus_duration_s.>=min_stimulus_duration_s]
    ğ = BitArray(collect(trial.choice for trial in trials))
    ğ›Œ = collect(FHMDDM.gamma2lambda(trial.Î³) for trial in trials)
    ğ„ = map(ğ›Œ,trials) do Î», trial
            leftpulses = collect(length(left) for left in trial.clicks.left)
            rightpulses = collect(length(right) for right in trial.clicks.right)
            ğœ = rightpulses .- leftpulses
            ğœ[1+latency:T+latency] .- Î»*Î”t
        end
    testingtrials, trainingtrials = cvpartition(kfold, length(trials))
    testingmodels = map(testingtrials) do testingtrials
                        ExponentialKernelModel(ğ = ğ[testingtrials],
                                            ğ„ = ğ„[testingtrials],
                                            Î”t = Î”t,
                                            ğ›Œ=ğ›Œ[testingtrials],
                                            T=T)
                    end
    trainingmodels = map(trainingtrials) do trainingtrials
                        ExponentialKernelModel(ğ = ğ[trainingtrials],
                                            ğ„ = ğ„[trainingtrials],
                                            Î”t = Î”t,
                                            ğ›Œ=ğ›Œ[trainingtrials],
                                            T=T)
                    end
    EKCrossValidation(ğ=ğ,
                    kfold=kfold,
                    ğ›Œ=ğ›Œ,
                    testingmodels=testingmodels,
                    trainingmodels=trainingmodels,
                    testingtrials=testingtrials,
                    trainingtrials=trainingtrials)
end

"""
    gamma2lambda(Î³)

Transform the log-odds of the right and left pulse input rate (Î³'s) to the difference between the input rates

OPTIONAL ARGUMENT
-`f`: total pulse rate per second
"""
function gamma2lambda(Î³::Real; f::Real=40)
    f * (exp(Î³)-exp(-Î³)) / (2+exp(Î³)+exp(-Î³))
end


"""
    crossvalidate!(ekcv)

Make out-of-sample predictions
"""
function crossvalidate!(ekcv::EKCrossValidation)
    for trainingmodel in ekcv.trainingmodels
        maximizeposterior!(trainingmodel)
    end
    for k = 1:ekcv.kfold
        ekcv.testingmodels[k].ğ›ƒ .= ekcv.trainingmodels[k].ğ›ƒ
    end
    FHMDDM.predict!(ekcv)
    return nothing
end

"""
    maximizeposterior!(ekmodel)

Find the maximum-a-posteriori parameters of an exponential kernel model.

The optimization algorithm is done using the quasi-Newton algorithm L-BFGS. The gradient of the log-likelihood is computed using automatic differentiation.
"""
function maximizeposterior!(ekmodel::ExponentialKernelModel; Î±::AbstractFloat=0.5, nrepeat::Integer=5, show_trace::Bool=false)
    f(ğ›ƒ) = -loglikelihood(ğ›ƒ, ekmodel) + Î±*dot(ğ›ƒ,ğ›ƒ)
    minloss = Inf
    ğ›ƒML = copy(ekmodel.ğ›ƒ)
    for i = 1:nrepeat
        ğ›ƒâ‚€ = rand(length(ekmodel.ğ›ƒ))
        od = OnceDifferentiable(f, ğ›ƒâ‚€; autodiff = :forward);
        optimizer = LBFGS(linesearch = LineSearches.BackTracking())
        options = Optim.Options(show_trace=show_trace)
        optimizationresults = optimize(od, ğ›ƒâ‚€, optimizer, options)
        show_trace && println(optimizationresults)
        thisloss = Optim.minimum(optimizationresults)
        if thisloss < minloss
            ğ›ƒML = Optim.minimizer(optimizationresults)
            minloss = thisloss
        end
    end
    ekmodel.ğ›ƒ .= ğ›ƒML
    return nothing
end
"""
    loglikelihood(ğ›ƒ, ekmodel)

Log-likelihood of the parameters `ğ›ƒ` given the data contained in the object `ekmodel`
"""
function loglikelihood(ğ›ƒ::Vector{<:Real}, ekmodel::ExponentialKernelModel)
    @unpack ğ, Î”t, ğ„, ğ›Œ, ğ›Œabs, T, ğ›• = ekmodel
    TÎ”t = T*Î”t
    â„“ = 0
    for i = 1:length(ğ)
        p = probabilityright(ğ›ƒ, ğ„[i], TÎ”t, ğ›Œ[i], ğ›Œabs[i], ğ›•)
        â„“ += ğ[i] ? log(p) : log(1-p)
    end
    return â„“
end

"""
    probabilityright(ğ›ƒ, TÎ”t, Î», Î»abs, ğ›•)

Probability of a right choice

ARGUMENT
-`ğ›ƒ`: parameters
-`ğ`: excess pulse input
-`TÎ”t`: stimulus duration, in seconds
-`Î»`: expected input rate
-`Î»abs`: absolute value of the expected input rate
-`ğ›•`: time, in seconds, of each tiem step from stimulus onset
"""
function probabilityright(ğ›ƒ::Vector{<:Real}, ğ::Vector{<:Real}, TÎ”t::Real, Î»::Real, Î»abs::Real, ğ›•::Vector{<:Real})
    k = ğ›ƒ[4]*Î»abs
    y = dot(exp.(k.*ğ›•),ğ)
    bâ»Â¹ = k/(exp(k*TÎ”t)-1)
    x = ğ›ƒ[1] + ğ›ƒ[2]TÎ”t*Î» + ğ›ƒ[3]*y*bâ»Â¹
    return logistic(x)
end

"""
    predict!(ekcv)

Make out-of-sample predictions
"""
function predict!(ekcv::EKCrossValidation)
    ğ©right = map(ekcv.testingmodels) do testingmodel
                @unpack ğ›ƒ, ğ, Î”t, ğ„, ğ›Œ, ğ›Œabs, T, ğ›• = testingmodel
                TÎ”t = T*Î”t
                collect(probabilityright(ğ›ƒ, ğ, TÎ”t, Î», Î»abs, ğ›•) for (ğ, Î», Î»abs) in zip(ğ„, ğ›Œ,ğ›Œabs))
            end
    ğ©right = vcat(ğ©right...)
    sortindices = sortperm(vcat(ekcv.testingtrials...))
    ekcv.ğ©right .= ğ©right[sortindices]
    for i = 1:length(ekcv.â„“)
        p = ekcv.ğ©right[i]
        ekcv.â„“[i] = ekcv.ğ[i] ? log(p) : log(1-p)
    end
    â„“bernoulli = map(ekcv.testingmodels, ekcv.trainingmodels) do testingmodel, trainingmodel
                    p = mean(trainingmodel.ğ)
                    collect(d ? log(p) : log(1-p) for d in testingmodel.ğ)
                end
    â„“bernoulli = vcat(â„“bernoulli...)
    ekcv.â„“bernoulli .= â„“bernoulli[sortindices]
    for k = 1:ekcv.kfold
        ekcv.trainingmodels[k].hessian .= hessian(ekcv.trainingmodels[k])
    end
    return nothing
end

"""
    save(ekcv, filepath)

Save the cross-validation results of an exponential kernel model
"""
function save(ekcv::EKCrossValidation, filepath::String)
    dict = Dict("betas"=>collect(trainingmodel.ğ›ƒ for trainingmodel in ekcv.trainingmodels),
                "lambda"=>ekcv.ğ›Œ,
                "choice"=>ekcv.ğ,
                "pright"=>ekcv.ğ©right,
                "loglikelihood"=>ekcv.â„“,
                "loglikelihood_bernoulli"=>ekcv.â„“bernoulli,
                "times_s"=>ekcv.trainingmodels[1].ğ›•,
                "hessians"=>collect(trainingmodel.hessian for trainingmodel in ekcv.trainingmodels))
    matwrite(filepath, dict)
end

"""
    hessian(ekmodel)

Hessian matrix of the exponential kernel model
"""
function hessian(ekmodel::ExponentialKernelModel)
    f(ğ›ƒ) = -loglikelihood(ğ›ƒ, ekmodel)
    ForwardDiff.hessian(f, ekmodel.ğ›ƒ)
end
