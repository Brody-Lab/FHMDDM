"""
    Model(datapath; fit_to_choices)

Load a factorial hidden Markov drift diffusion model from a MATLAB file.

If the model has already been optimized, a results file is expected.

ARGUMENT
- `datapath`: full path of the data file

OPTIONAL ARGUMENT
-`fit_to_choices`: whether the initial values the model parameters are learned by fitting to the choices alone

RETURN
- a structure containing information for a factorial hidden Markov drift-diffusion model

EXAMPLE
```julia-repl
julia> using FHMDDM
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_04_14_test/data.mat"; randomize=true);
```
"""
function Model(datapath::String; randomize::Bool=true)
    dataMAT = matopen(datapath);
    options = Options(read(dataMAT, "options"))
    trialsets = vec(map(trialset->Trialset(options, trialset), read(dataMAT, "data")))
    if isfile(options.resultspath)
        Model(options, options.resultspath, trialsets)
    else
        Model(options, trialsets; randomize=randomize)
    end
end

"""
    Model(options, resultspath, trialsets)

Load a previously fitted factorial hidden Markov drift-diffusion model

ARGUMENT
-`options`: model settings
-`resultspath`: full path of the file containing the learned values of the parameters
-`trialsets`: data used to constrain the model

RETURN
- a structure containing information for a factorial hidden Markov drift-diffusion model
"""
function Model(options::Options,
		 		resultspath::String,
		 		trialsets::Vector{<:Trialset})
    resultsMAT = matopen(resultspath)
	glmŒ∏ = read(resultsMAT, "thetaglm")
	for i in eachindex(trialsets)
		for n in eachindex(trialsets[i].mpGLMs)
			trialsets[i].mpGLMs[n].Œ∏.ùêÆ .= glmŒ∏[i][n]["u"]
			for k in eachindex(glmŒ∏[i][n]["v"])
				trialsets[i].mpGLMs[n].Œ∏.ùêØ[k] .= glmŒ∏[i][n]["v"][k]
			end
		end
	end
	Model(options=options,
		   Œ∏native=LatentŒ∏(read(resultsMAT, "theta_native")),
		   Œ∏real=LatentŒ∏(read(resultsMAT, "theta_real")),
		   Œ∏‚ÇÄnative=LatentŒ∏(read(resultsMAT, "theta0_native")),
		   trialsets=trialsets)
end

"""
    Model(options, trialsets)

Create a factorial hidden Markov drift-diffusion model

ARGUMENT
-`options`: model settings
-`trialsets`: data used to constrain the model

RETURN
- a structure containing information for a factorial hidden Markov drift-diffusion model
"""
function Model(options::Options,
				trialsets::Vector{<:Trialset}; randomize::Bool=false)
	Œ∏native = randomize ? randomlyinitialize(options) : initializeparameters(options)
	Œ∏‚ÇÄnative = LatentŒ∏(([getfield(Œ∏native, f)...] for f in fieldnames(typeof(Œ∏native)))...) # just making a deep copy
	Model(options=options,
		   Œ∏native=Œ∏native,
		   Œ∏real=native2real(options, Œ∏native),
		   Œ∏‚ÇÄnative=Œ∏‚ÇÄnative,
		   trialsets=trialsets)
end

"""
    Clicks(a_latency_s, L, R, Œît, ntimesteps)

Create an instance of `Clicks` to compartmentalize variables related to the times of auditory clicks in one trial

The stereoclick is excluded.

ARGUMENT
-`a_latency_s`: latency of the accumulator with respect to the clicks
-`Œît`: duration, in seconds, of each time step
-`L`: a vector of floating-point numbers specifying the times of left clicks, in seconds. Does not need to be sorted.
-`ntimesteps`: number of time steps in the trial. Time is aligned to the stereoclick. The first time window is `[-Œît, 0.0)`, and the last time window is `[ntimesteps*Œît, (ntimesteps+1)*Œît)`, defined such that `t‚Çò‚Çí·µ•‚Çë - (ntimesteps+1)*Œît < Œît`, where `t‚Çò‚Çí·µ•‚Çë` is the time when movement away from the center port was first detected.
-`R`: a vector of floating-point numbers specifying the times of right clicks, in seconds. Does not need to be sorted.

RETURN
-an instance of the type `Clicks`
"""
function Clicks(a_latency_s::AbstractFloat,
				Œît::AbstractFloat,
                L::Vector{<:AbstractFloat},
                ntimesteps::Integer,
                R::Vector{<:AbstractFloat})
    L = L[.!isapprox.(L, 0.0)] #excluding the stereoclick
    R = R[.!isapprox.(R, 0.0)]
	L .+= a_latency_s
	R .+= a_latency_s
	rightmost_edge_s = (ntimesteps-1)*Œît
	L = L[L.<rightmost_edge_s]
	R = R[R.<rightmost_edge_s]
    clicktimes = [L;R]
    indices = sortperm(clicktimes)
    clicktimes = clicktimes[indices]
    isright = [falses(length(L)); trues(length(R))]
    isright = isright[indices]
    is_in_timestep =
        map(1:ntimesteps) do t
            ((t-2)*Œît .<= clicktimes) .& (clicktimes .< (t-1)*Œît) # the right edge of the first time step is defined as 0.0, the time of the stereoclick
        end
    right = map(is_in_timestep) do I
                findall(I .& isright)
            end
    isleft = .!isright
    left =  map(is_in_timestep) do I
                findall(I .& isleft)
            end
	inputtimesteps=findall(sum.(is_in_timestep).>0)
	inputindex = map(t->findall(inputtimesteps .== t), 1:ntimesteps)
    Clicks(time=clicktimes,
		   inputtimesteps=inputtimesteps,
		   inputindex=inputindex,
           source=isright,
           left=left,
           right=right)
end

"""
    Trialset(options, trialset)

Parse data exported from MATLAB to create a structure containing data for one trial-set

INPUT
-`trialset`: a dictionary contain MATLAB-exported data corresponding to a single trial-set
-`options`: model settings

OUTPUT
-an instance of `trialsetdata`
"""
function Trialset(options::Options, trialset::Dict)
    rawtrials = vec(trialset["trials"])
    ntimesteps = map(x->convert(Int64, x["ntimesteps"]), rawtrials)
	units = vec(trialset["units"])
    ùêò = map(x->convert.(Int64, vec(x["y"])), units)
    @assert sum(ntimesteps) == length(ùêò[1])
	@unpack K, Œû = options
	dùõè_dB = (2collect(1:Œû) .- Œû .- 1)./(Œû-2)
	ùêï, Œ¶ = temporal_bases_values(options, ntimesteps)
	ùêî‚ÇÄ = ones(size(trialset["Xtiming"],1))
	mpGLMs = map(units, ùêò) do unit, ùê≤
				ùêó=hcat(ùêî‚ÇÄ, unit["Xautoreg"], trialset["Xtiming"], ùêï)
				MixturePoissonGLM(Œît=options.Œît,
  								dùõè_dB=dùõè_dB,
								max_spikehistory_lag = size(unit["Xautoreg"],2),
								Œ¶=Œ¶,
								Œ∏=GLMŒ∏(K, ùêó, ùêï),
								ùêï=ùêï,
								ùêó=ùêó,
								ùê≤=ùê≤)
			 end
	rawclicktimes = map(x->x["clicktimes"], rawtrials)
    L = map(rawclicktimes) do x
			leftclicks = x["L"]
			typeof(leftclicks)<:AbstractFloat ? [leftclicks] : vec(leftclicks)
		end
	R = map(rawclicktimes) do x
			rightclicks = x["R"]
			typeof(rightclicks)<:AbstractFloat ? [rightclicks] : vec(rightclicks)
		end
	@assert typeof(trialset["lagged"]["lag"])==Float64  && trialset["lagged"]["lag"] == -1.0
    previousanswer = vec(convert.(Int64, trialset["lagged"]["answer"]))
    clicks = map((L,R,ntimesteps)->Clicks(options.a_latency_s, options.Œît,L,ntimesteps,R), L, R, ntimesteps)
    trials = map(clicks, rawtrials, ntimesteps, previousanswer) do clicks, rawtrial, ntimesteps, previousanswer
                Trial(clicks=clicks,
                      choice=rawtrial["choice"],
                      ntimesteps=ntimesteps,
                      previousanswer=previousanswer)
             end

    Trialset(mpGLMs=mpGLMs, trials=trials)
end

"""
	initializeparameters(options)

Initialize the value of each model parameters in native space with defaults

RETURN
-values of model parameter in native space
"""
function initializeparameters(options::Options)
	LatentŒ∏(A·∂ú‚ÇÅ‚ÇÅ=[options.q_A·∂ú‚ÇÅ‚ÇÅ],
			A·∂ú‚ÇÇ‚ÇÇ=[options.q_A·∂ú‚ÇÇ‚ÇÇ],
			B=[options.q_B],
			k=[options.q_k],
			Œª=options.fit_Œª ? [-1e-2] : zeros(1),
			Œº‚ÇÄ=zeros(1),
			œï=[options.q_œï],
			œÄ·∂ú‚ÇÅ=[options.q_œÄ·∂ú‚ÇÅ],
			œà=[options.q_œà],
			œÉ¬≤‚Çê=[options.q_œÉ¬≤‚Çê],
			œÉ¬≤·µ¢=[options.q_œÉ¬≤·µ¢],
			œÉ¬≤‚Çõ=[options.q_œÉ¬≤‚Çõ],
			w‚Çï=zeros(1))
end

"""
	initializeparameters(options)

Initialize the value of each model parameters in native space by sampling from a Uniform random variable

RETURN
-values of model parameter in native space
"""
function randomlyinitialize(options::Options)
	LatentŒ∏(A·∂ú‚ÇÅ‚ÇÅ=options.K==2 ? [options.bound_z + rand()*(1-2*options.bound_z)] : [options.q_A·∂ú‚ÇÅ‚ÇÅ],
			A·∂ú‚ÇÇ‚ÇÇ=options.K==2 ? [options.bound_z + rand()*(1-2*options.bound_z)] : [options.q_A·∂ú‚ÇÇ‚ÇÇ],
			B=options.fit_B ? 2options.q_B*rand(1) : [options.q_B],
			k=options.fit_k ? rand(1) : [options.q_k],
			Œª=options.fit_Œª ? [1-2rand()] : zeros(1),
			Œº‚ÇÄ=options.fit_Œº‚ÇÄ ? [1-2rand()] : zeros(1),
			œï=options.fit_œï ? rand(1) : [options.q_œï],
			œÄ·∂ú‚ÇÅ=options.K==2 ? [options.bound_z + rand()*(1-2*options.bound_z)] : [options.q_œÄ·∂ú‚ÇÅ],
			œà=options.fit_œà ? [options.bound_œà + rand()*(1-2*options.bound_œà)] : [options.q_œà],
			œÉ¬≤‚Çê=options.fit_œÉ¬≤‚Çê ? rand(1) : [options.q_œÉ¬≤‚Çê],
			œÉ¬≤·µ¢=options.fit_œÉ¬≤·µ¢ ? rand(1) : [options.q_œÉ¬≤·µ¢],
			œÉ¬≤‚Çõ=options.fit_œÉ¬≤‚Çõ ? rand(1)/10 : [options.q_œÉ¬≤‚Çõ],
			w‚Çï=options.fit_w‚Çï ? [1-2rand()] : zeros(1))
end

"""
	initializeparameters!(model)

Initialize the values of a subset of the parameters by maximizing the likelihood of only the choices.

The parameters specifying the transition probability of the coupling variable are not modified. The weights of the GLM are computed by maximizing the expectation of complete-data log-likelihood across accumulator states, assuming a coupled state.

MODIFIED ARGUMENT
-`model`: an instance of the factorial hidden Markov drift-diffusion model

EXAMPLE
```julia-repl
julia> using FHMDDM
julia> datapath = "/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_05_05_test/data.mat"
julia> model = Model(datapath; randomize=true)
julia> FHMDDM.initializeparameters!(model)
```
"""
function initializeparameters!(model::Model)
	maximize_choice_posterior!(model)
	learn_state_independent_filters!(model)
	return nothing
end

"""
	initialize_for_stochastic_transition!(model)

Initialize the parameters of the model such that the state transitions over time

MODIFIED ARGUMENT
-`model`: a structure containing the data, parameters, and the hyperparameters of the model. The prior and transition probabilities of the coupling variable are modified. The drift-diffusion parameters are optimized to maximize the posterior likelihood of the choices. The state-independent filters are optimized by maximizing the likelihood of each neuron's GLM as though it does not dependent on the accumulator. The filters of the accumulator dependent input are optimized by performing a single M-step using the posterior probabilities of the latent variable conditioned on only the choices. The data are split such that a different subset of the time steps in each trial used to optimize the filters in each state. The first subset of the time steps are used to optimize the filters of the accumulator in the first state, and the last subset of time steps are used to optimize the filters in the last state.

EXAMPLE
```julia-repl
julia> using FHMDDM
julia> datapath = "/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_05_05_test/data.mat"
julia> model = Model(datapath; randomize=true)
julia> FHMDDM.initialize_for_stochastic_transition!(model)
```
"""
function initialize_for_stochastic_transition!(model::Model)
	@unpack options, Œ∏native, Œ∏‚ÇÄnative, Œ∏real, trialsets = model
	@unpack K = model.options
	Œ∏‚ÇÄnative.œÄ·∂ú‚ÇÅ[1] = Œ∏native.œÄ·∂ú‚ÇÅ[1] = options.q_œÄ·∂ú‚ÇÅ
	Œ∏‚ÇÄnative.A·∂ú‚ÇÅ‚ÇÅ[1] = Œ∏native.A·∂ú‚ÇÅ‚ÇÅ[1] = min(0.95, options.q_A·∂ú‚ÇÅ‚ÇÅ)
	Œ∏‚ÇÄnative.A·∂ú‚ÇÇ‚ÇÇ[1] = Œ∏native.A·∂ú‚ÇÇ‚ÇÇ[1] = 1.0 - options.q_A·∂ú‚ÇÇ‚ÇÇ
	native2real!(Œ∏real, options, Œ∏native)
	learn_state_independent_filters!(model)
	maximize_choice_posterior!(model)
	Œ≥ = choiceposteriors(model)
	Œ≥ = map(Œ≥·µ¢->dropdims(sum(Œ≥·µ¢,dims=2),dims=2), Œ≥)
	max_ntimesteps = maximum_number_of_time_steps(model)
	ntimesteps = collect(trial.ntimesteps for trialset in trialsets for trial in trialset.trials)
	ùê≠ = round.(Int, collect(1:max_ntimesteps/K:max_ntimesteps+1))
	for k = 1:K
		t0 = ùê≠[k]
		t1 = ùê≠[k+1]-1
		indices = indices_for_subselection(model, t0, t1)
		Œ≥sub = subselect(indices, Œ≥)
		for i in eachindex(model.trialsets)
			for n in eachindex(model.trialsets[i].mpGLMs)
				mpGLM = subselect(indices[i], model.trialsets[i].mpGLMs[n])
				learn_state_dependent_filters!(mpGLM, Œ≥sub[i], k)
			end
		end
	end
end

"""
	indices_for_subselection(model, t0, t1)

ARGUMENT
-`model`: structure containing the data, parameters, and hyperparameters
-`t0`: first timestep in each trial to be included
-`t1`: last timestep to be included

RETURN
-`indices`: a vector of BitVectors indicating which time steps in the trialset are used. Element `indices[i][œÑ]` corresponds to the œÑ-th time step in the i-th trialset
"""
function indices_for_subselection(model::Model, t0::Integer, t1::Integer)
	indices = map(model.trialsets) do trialset
				falses(sum(collect(trial.ntimesteps for trial in trialset.trials)))
			end
	offset = 0
	for i in eachindex(model.trialsets)
		for j in eachindex(model.trialsets[i].trials)
			indices[i][offset.+(t0:min(t1, model.trialsets[i].trials[j].ntimesteps))] .= true
		end
	end
	return indices
end

"""
	subselect(indices, Œ≥)

Select the poster probabilities corresponding to a subset of time steps

ARGUMENT
-`indices`: a vector of BitVectors indicating which time steps in the trialset are used. Element `indices[i][œÑ]` corresponds to the œÑ-th time step in the i-th trialset
-`Œ≥`: posterior probabilities of each accumulator and coupling state. Element `Œ≥[i][j][œÑ]` corresponds to the posterior probability of the j-th accumulator state  in the œÑ-th time step in the i-th trialset.
-`k`: coupling state index

RETURN
-`Œ≥`: sub-selected posterior probabilities
"""
function subselect(indices::Vector{<:BitVector}, Œ≥::Vector{<:Vector{<:Vector{<:Real}}})
	map(indices, Œ≥) do indices, Œ≥
		map(Œ≥) do Œ≥
			Œ≥[indices]
		end
	end
end

"""
	subselect(indices, mpGLM)

Select the input and observations corresponding to a subset of time steps

ARGUMENT
-`indices`: a BitVector indicating which time steps in the trialset are used
-`mpGLM`: a mixture of Poisson generalized linear model

RETURN
-`mpGLM`: a new structure in which the input and the observations have been sub-selected. Note that other fields reference the same memory as the corresponding fields in the original structure.
"""
function subselect(indices::BitVector, mpGLM::MixturePoissonGLM)
	MixturePoissonGLM(Œît = mpGLM.Œît,
					dùõè_dB = mpGLM.dùõè_dB,
					max_spikehistory_lag = mpGLM.max_spikehistory_lag,
					Œ¶ = mpGLM.Œ¶,
					Œ∏ = mpGLM.Œ∏,
					ùêï = mpGLM.ùêï[indices,:],
					ùêó = mpGLM.ùêó[indices,:],
					ùê≤ = mpGLM.ùê≤[indices])
end

"""
	learn_state_dependent_filters!(mpGLM, Œ≥, k)

Learn the filters in the k-th state

MODIFIED ARGUMENT
-`mpGLM`: a structure containing the data and parameters of the mixture of Poisson GLM of one neuron

UNMODIFIED ARGUMENT
-`Œ≥`: posterior probability of the latent variables. Element `Œ≥[j][œÑ]` corresponds to the posterior probability of the j-th accumulator state  in the œÑ-th time step
-`k`: index of the coupling state
"""
function learn_state_dependent_filters!(mpGLM::MixturePoissonGLM, Œ≥::Vector{<:Vector{<:Real}}, k::Integer; show_trace::Bool=true, iterations::Integer=20)
	Q = fill(NaN,1)
	nùêØ = length(mpGLM.Œ∏.ùêØ[k])
	‚àáQ = fill(NaN, nùêØ)
	‚àá‚àáQ = fill(NaN, nùêØ, nùêØ)
	f(ùêØ‚Çñ) = -expectation_of_loglikelihood!(mpGLM,Q,‚àáQ,‚àá‚àáQ,Œ≥,k,ùêØ‚Çñ)
	‚àáf!(‚àá, ùêØ‚Çñ) = ‚àánegexpectation_of_loglikelihood!(‚àá,mpGLM,Q,‚àáQ,‚àá‚àáQ,Œ≥,k,ùêØ‚Çñ)
	‚àá‚àáf!(‚àá‚àá, ùêØ‚Çñ) = ‚àá‚àánegexpectation_of_loglikelihood!(‚àá‚àá,mpGLM,Q,‚àáQ,‚àá‚àáQ,Œ≥,k,ùêØ‚Çñ)
    results = Optim.optimize(f, ‚àáf!, ‚àá‚àáf!, copy(mpGLM.Œ∏.ùêØ[k]), NewtonTrustRegion(), Optim.Options(show_trace=show_trace, iterations=iterations))
	mpGLM.Œ∏.ùêØ[k] .= Optim.minimizer(results)
	return nothing
end

"""
	expectation_of_loglikelihood!(mpGLM,Q,‚àáQ,‚àá‚àáQ,Œ≥,k,ùêØ‚Çñ)

Expectation of the log-likelihood under the posterior probability of the latent variables

Only the component that depend on the state-dependent filters k-th state are included

MODIFIED ARGUMENT
-`mpGLM`: a structure containing the data and parameters of the mixture of Poisson GLM of one neuron
-`Q`: an one-element vector that quantifies the expectation
-`‚àáQ`: gradient of the expectation with respect to the filters in the k-th state
-`‚àá‚àáQ`: Hessian of the expectation with respect to the filters in the k-th state

UNMODIFIED ARGUMENT
-`Œ≥`: posterior probability of the latent variables. Element `Œ≥[j][œÑ]` corresponds to the posterior probability of the j-th accumulator state  in the œÑ-th time step
-`k`: index of the coupling state
-`ùêØ‚Çñ`: values of the filters of the accumulator-dependent input in the k-th state
"""
function expectation_of_loglikelihood!(mpGLM::MixturePoissonGLM, Q::Vector{<:Real}, ‚àáQ::Vector{<:Real}, ‚àá‚àáQ::Matrix{<:Real}, Œ≥::Vector{<:Vector{<:Real}}, k::Integer, ùêØ‚Çñ::Vector{<:Real})
	if (mpGLM.Œ∏.ùêØ[k] != ùêØ‚Çñ) || isnan(Q[1])
		mpGLM.Œ∏.ùêØ[k] .= ùêØ‚Çñ
		‚àá‚àáexpectation_of_loglikelihood!(Q,‚àáQ,‚àá‚àáQ,Œ≥,k,mpGLM)
	end
	Q[1]
end

"""
	‚àánegexpectation_of_loglikelihood!(‚àá,mpGLM,Q,‚àáQ,‚àá‚àáQ,Œ≥,k,ùêØ‚Çñ)

Gradient of the negative of the expectation of the log-likelihood under the posterior probability of the latent variables

MODIFIED ARGUMENT
-`‚àá`: gradient of the negative of the expectation
-`mpGLM`: a structure containing the data and parameters of the mixture of Poisson GLM of one neuron
-`Q`: an one-element vector that quantifies the expectation
-`‚àáQ`: gradient of the expectation with respect to the filters in the k-th state
-`‚àá‚àáQ`: Hessian of the expectation with respect to the filters in the k-th state

UNMODIFIED ARGUMENT
-`Œ≥`: posterior probability of the latent variables. Element `Œ≥[j][œÑ]` corresponds to the posterior probability of the j-th accumulator state  in the œÑ-th time step
-`k`: index of the coupling state
-`ùêØ‚Çñ`: values of the filters of the accumulator-dependent input in the k-th state
"""
function ‚àánegexpectation_of_loglikelihood!(‚àá::Vector{<:Real}, mpGLM::MixturePoissonGLM, Q::Vector{<:Real}, ‚àáQ::Vector{<:Real}, ‚àá‚àáQ::Matrix{<:Real}, Œ≥::Vector{<:Vector{<:Real}}, k::Integer, ùêØ‚Çñ::Vector{<:Real})
	if (mpGLM.Œ∏.ùêØ[k] != ùêØ‚Çñ) || isnan(Q[1])
		mpGLM.Œ∏.ùêØ[k] .= ùêØ‚Çñ
		‚àá‚àáexpectation_of_loglikelihood!(Q,‚àáQ,‚àá‚àáQ,Œ≥,k,mpGLM)
	end
	for i in eachindex(‚àá)
		‚àá[i] = -‚àáQ[i]
	end
	return nothing
end

"""
	‚àá‚àánegexpectation_of_loglikelihood!(‚àá‚àá,mpGLM,Q,‚àáQ,‚àá‚àáQ,Œ≥,k,ùêØ‚Çñ)

Hessian of the negative of the expectation of the log-likelihood under the posterior probability of the latent variables

MODIFIED ARGUMENT
-`‚àá‚àá`: Hessian of the negative of the expectation
-`mpGLM`: a structure containing the data and parameters of the mixture of Poisson GLM of one neuron
-`Q`: an one-element vector that quantifies the expectation
-`‚àáQ`: gradient of the expectation with respect to the filters in the k-th state
-`‚àá‚àáQ`: Hessian of the expectation with respect to the filters in the k-th state

UNMODIFIED ARGUMENT
-`Œ≥`: posterior probability of the latent variables. Element `Œ≥[j][œÑ]` corresponds to the posterior probability of the j-th accumulator state  in the œÑ-th time step
-`k`: index of the coupling state
-`ùêØ‚Çñ`: values of the filters of the accumulator-dependent input in the k-th state
"""
function ‚àá‚àánegexpectation_of_loglikelihood!(‚àá‚àá::Matrix{<:Real}, mpGLM::MixturePoissonGLM, Q::Vector{<:Real}, ‚àáQ::Vector{<:Real}, ‚àá‚àáQ::Matrix{<:Real}, Œ≥::Vector{<:Vector{<:Real}}, k::Integer, ùêØ‚Çñ::Vector{<:Real})
	if (mpGLM.Œ∏.ùêØ[k] != ùêØ‚Çñ) || isnan(Q[1])
		mpGLM.Œ∏.ùêØ[k] .= ùêØ‚Çñ
		‚àá‚àáexpectation_of_loglikelihood!(Q,‚àáQ,‚àá‚àáQ,Œ≥,k,mpGLM)
	end
	nùêØ = length(ùêØ‚Çñ)
	for i =1:nùêØ
		for j=i:nùêØ
			‚àá‚àá[i,j] = ‚àá‚àá[j,i] = -‚àá‚àáQ[i,j]
		end
	end
	return nothing
end

"""
	‚àá‚àáexpectation_of_loglikelihood!(Q,‚àáQ,‚àá‚àáQ,Œ≥,k,mpGLM)

Compute the expectation of the log-likelihood and its gradient and Hessian

ARGUMENT
-`Q`: expectation of the log-likelihood under the posterior probability of the latent variables. Only the component in the coupling state `k` is included
-`‚àáQ`: first-order derivatives of the expectation
-`‚àá‚àáQ`: second-order derivatives of the expectation

UNMODIFIED ARGUMENT
-`Œ≥`: posterior probabilities of the latent variables
-`k`: index of the coupling state
-`mpGLM`: a structure containing the data and parameters of the mixture of Poisson GLM of one neuron
"""
function ‚àá‚àáexpectation_of_loglikelihood!(Q::Vector{<:Real},
										‚àáQ::Vector{<:Real},
										‚àá‚àáQ::Matrix{<:Real},
										Œ≥::Vector{<:Vector{<:Real}},
										k::Integer,
										mpGLM::MixturePoissonGLM)
    @unpack Œît, ùêï, dùõè_dB, ùê≤ = mpGLM
	dùõè_dB¬≤ = dùõè_dB.^2
	Œû = size(Œ≥,1)
	T = length(ùê≤)
	‚àë·µ¢_dQ·µ¢‚Çñ_dL·µ¢‚Çñ‚®ÄdŒæ·µ¢_dB = zeros(T)
	‚àë·µ¢_d¬≤Q·µ¢‚Çñ_dL·µ¢‚Çñ¬≤‚®ÄdŒæ·µ¢_dB¬≤ = zeros(T)
	Q[1] = 0.0
	@inbounds for i = 1:Œû
		ùêã = linearpredictor(mpGLM,i,k)
		for t=1:T
			d¬≤‚Ñì_dL¬≤, d‚Ñì_dL, ‚Ñì = differentiate_loglikelihood_twice_wrt_linearpredictor(Œît, ùêã[t], ùê≤[t])
			Q[1] += Œ≥[i][t]*‚Ñì
			dQ·µ¢‚Çñ_dL·µ¢‚Çñ = Œ≥[i][t] * d‚Ñì_dL
			‚àë·µ¢_dQ·µ¢‚Çñ_dL·µ¢‚Çñ‚®ÄdŒæ·µ¢_dB[t] += Œ≥[i][t] * d‚Ñì_dL * dùõè_dB[i]
			‚àë·µ¢_d¬≤Q·µ¢‚Çñ_dL·µ¢‚Çñ¬≤‚®ÄdŒæ·µ¢_dB¬≤[t] += Œ≥[i][t] * d¬≤‚Ñì_dL¬≤ * dùõè_dB¬≤[i]
		end
	end
	ùêï·µÄ = transpose(ùêï)
	‚àáQ .= ùêï·µÄ*‚àë·µ¢_dQ·µ¢‚Çñ_dL·µ¢‚Çñ‚®ÄdŒæ·µ¢_dB
	‚àá‚àáQ .= ùêï·µÄ*(‚àë·µ¢_d¬≤Q·µ¢‚Çñ_dL·µ¢‚Çñ¬≤‚®ÄdŒæ·µ¢_dB¬≤.*ùêï)
	return nothing
end
