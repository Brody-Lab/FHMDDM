"""
	maximizeevidence!(model)

Learn both the parameters and hyperparameters by maximizing the evidence

This optimization procedure alternately fixes the hyperparameters and learn the parameters by maximizing the posterior, and fixes the parameters and learn the hyperparameters by maximizing the evidence.

MODIFIED ARGUMENT
-`model`: structure containing the parameters, hyperparameters, and data. The parameters and hyperparameters are updated.

OPTIONAL ARGUMET
-`iterations`: maximum number of iterations for alternating between maximizing the posterior probability and maximizing the evidence
-`MAP_iterations`: maximum number of iterations for optimizing the posterior probability of the parameters
-`store_trace`: should a trace of the optimization algorithm's state be saved at each iteration?
-`verbose`: whether to display messages
-`x_reltol`: the relative difference in the norm of the L2 penalty coefficients, in real space, below which the optimization procedure aborts
"""
function maximizeevidence!(model::Model;
						iterations::Int=2,
						MAP_iterations::Int=500,
						store_trace::Bool=true,
						verbose::Bool=true,
						x_reltol::Real=1e-1)
	@unpack indexğš½, ğ›‚min, ğ›‚max = model.gaussianprior
	memory = Memoryforgradient(model)
	indexğ›‰ = indexparameters(model)
	bestğ¸ = -Inf
	bestğ›‰ = concatenateparameters(model)
	bestğ›‚ = copy(model.gaussianprior.ğ›‚)
	for i = 1:iterations
		verbose && printseparator()
		verbose && println("Evidence optimization iteration: ", i, ": maximizing the log-posterior.")
	    Optim_results = maximizeposterior!(model;iterations=MAP_iterations)[1]
		MAP_values_converged = Optim.converged(Optim_results)
		ğ›‰â‚€ = concatenateparameters(model)
		stats = @timed âˆ‡âˆ‡loglikelihood(model)
		ğ‡ = stats.value[3]
		verbose && println("Evidence optimization iteration: ", i, ": computing the Hessian of the log-likelihood took ", stats.time, " seconds")
		if store_trace
			savetrace(MAP_values_converged, ğ‡, i, model)
			verbose && println("Evidence optimization iteration: ", i, ": saved trace")
		end
		if MAP_values_converged
			verbose && println("Evidence optimization iteration: ", i, ": the MAP values of the parameters converged")
		else
			verbose && println("Evidence optimization iteration: ", i, ": the MAP values of the parameters did not converge, and therefore the optimization procedure is aborting.")
			if i == 1
				bestğ›‰ = concatenateparameters(model)
			end
			break
		end
		ğ‡finite = ğ‡[indexğš½, indexğš½]
		ğ¸ = logevidence!(memory, model, ğ‡finite, ğ›‰â‚€)
		if ğ¸ > bestğ¸
			verbose && println("Evidence optimization iteration: ", i, ": the current log-evidence ( ", ğ¸, ") is greater than its previous value (", bestğ¸, ").")
			bestğ¸ = ğ¸
			bestğ›‰ = ğ›‰â‚€
			bestğ›‚ = copy(model.gaussianprior.ğ›‚)
		else
			verbose && println("Evidence optimization iteration: ", i, ": the current log-evidence ( ", ğ¸, ") is not greater than its previous value (", bestğ¸, "), and therefore the optimization procedure is aborting.")
			break
		end
		if i==iterations
			verbose && println("Evidence optimization iteration ", i, ": the last iteration has been reached, and the optimization procedure is aborting.")
			break
		end
		normÎ” = maximizeevidence!(memory, model, ğ‡finite, indexğ›‰, ğ›‰â‚€)
		verbose && println("Evidence optimization iteration ", i, ": new L2 penalty coefficients (ğ›‚) â†’ ", model.gaussianprior.ğ›‚)
		if normÎ” < x_reltol
			verbose && println("Evidence optimization iteration: ", i, ": optimization halted after the relative difference in the norm of the L2 penalty coefficients (in real space) decreased below ", x_reltol)
			break
		end
	end
	ğ›‰â‚€ = concatenateparameters(model)
	if ğ›‰â‚€ != bestğ›‰
		sortparameters!(model, bestğ›‰, indexğ›‰)
		real2native!(model.Î¸native, model.options, model.Î¸real)
	end
	if model.gaussianprior.ğ›‚ != bestğ›‚
		model.gaussianprior.ğ›‚ .= bestğ›‚
		precisionmatrix!(model.gaussianprior)
	end
	return nothing
end

"""
	logevidence!(memory, model, ğ‡, ğ›‰)

Log of the approximate marginal likelihood

MODIFIED ARGUMENT
-`memory`: a structure containing memory for in-place computation
-`model`: structure containing the parameters, hyperparameters, and data

UNMODIFIED ARGUMENT
-`ğ›‰`: posterior mode
"""
function logevidence!(memory::Memoryforgradient, model::Model, ğ‡::Matrix{<:Real}, ğ›‰::Vector{<:Real})
	loglikelihood!(model, memory, ğ›‰)
	ğ° = ğ›‰[model.gaussianprior.indexğš½]
	logevidence(ğ‡, memory.â„“[1], model.gaussianprior.ğš½, ğ°)
end

"""
	logevidence(ğ‡, â„“, ğš½, ğ°)

Evaluate the log-evidence

ARGUMENT
-`ğš½`: subset of the precision matrix corresponding to only the parametes with finite covariance
-`ğ‡`: subset of the Hessian of the log-likelihood evaluated at the MAP values of the parameters, containing only the elements with finite variance
-`â„“`: log-likelihood evaluated at the approximate posterior mode ğ°
-`ğ°`: subset of parameters corresponding to dimensions with finite covariance

RETURN
-log evidence
"""
function logevidence(ğ‡::Matrix{<:Real}, â„“::Real, ğš½::AbstractMatrix{<:Real}, ğ°::Vector{<:Real})
	ğŒ = I - (ğš½ \ ğ‡)
	logdetğŒ, signdetğŒ = logabsdet(ğŒ)
	if signdetğŒ < 0
		println("negative determinant")
		-Inf
	else
		â„“ - 0.5dot(ğ°, ğš½, ğ°) - 0.5logdetğŒ
	end
end

"""
	maximizeevidence!(memory, model, ğ‡, indexğ›‰, ğ›‰â‚€)

Learn hyperparameters by fixing the parameters of the model and maximizing the evidence

MODIFIED ARGUMENT
-`memory`: structure containing variables to be modified during computations
-`model`: structure containing the parameters, hyperparameters, and data. Only the hyperparameters are modified

UNMODIFIED ARGUMENT
-`indexğ›‰`: composite containing the indices of the model parameters if they were concatenated into a vector
-`ğ‡`: Hessian of the log-likelihood evaluated at the MAP solution `ğ›‰â‚€`, containing only the parameters associated with hyperparameters that are being optimized
-`ğ›‰â‚€`: exact MAP solution

RETURN
-Euclidean of the normalized difference in the log of the hyperparameters being optimized
"""
function maximizeevidence!(memory::Memoryforgradient,
						model::Model,
						ğ‡::Matrix{<:Real},
						indexğ›‰::IndexÎ¸,
						ğ›‰â‚€::Vector{<:Real};
						optimizationoptions::Optim.Options=Optim.Options(iterations=15, show_trace=false, show_every=1),
						optimizer::Optim.FirstOrderOptimizer=LBFGS(linesearch=LineSearches.BackTracking()))
	@unpack gaussianprior = model
	@unpack ğ›‚min, ğ›‚max, indexğš½, ğš½ = gaussianprior
	ğ°â‚€ = ğ›‰â‚€[indexğš½]
	ğâ‚€ğ°â‚€ = (ğš½-ğ‡)*ğ°â‚€
	ğ±â‚€ = native2real(gaussianprior)
	ğ›‰ = concatenateparameters(model)
	âˆ‡nâ„“ = similar(ğ›‰)
	f(ğ±) = -logevidence!(memory, model, ğ›‰, ğâ‚€ğ°â‚€, ğ‡, ğ±)
	g!(âˆ‡nğ¸, ğ±) = âˆ‡negativelogevidence!(memory, model, âˆ‡nğ¸, âˆ‡nâ„“, ğ›‰, ğâ‚€ğ°â‚€, ğ‡, ğ±)
	optimizationresults = Optim.optimize(f, g!, ğ±â‚€, optimizer, optimizationoptions)
	ğ±Ì‚ = Optim.minimizer(optimizationresults)
	normÎ” = 0.0
	for i in eachindex(ğ±Ì‚)
		normÎ” += (ğ±Ì‚[i]/ğ±â‚€[i] - 1.0)^2
	end
	real2native!(gaussianprior, ğ±Ì‚)
	precisionmatrix!(gaussianprior)
	sortparameters!(model, ğ›‰â‚€, indexğ›‰) #restore paramter values
	real2native!(model.Î¸native, model.options, model.Î¸real)
	return âˆšnormÎ”
end

"""
	logevidence!(memory, model, ğ›‰, ğâ‚€ğ°â‚€, ğ‡, ğ±)

Log of the marginal likelihood

MODIFIED ARGUMENT
-`memory`: a structure containing memory for in-place computation
-`model`: structure containing the parameters, hyperparameters, and data
-`ğ›‰`: preallocated memory for the parameters of the model

UNMODIFIED ARGUMENT
-`ğâ‚€ğ°â‚€`: Hessian of the log-posterior evalued at the MAP values of the parameters multiplied by the MAP value of the parameters, containing only the parameters associated with the hyperparameters being optimized
-`ğ‡`: Hessian of the log-likelihood evaluated at the MAP values of the parameters, containing only the parameters associated with the hyperparameters being optimized
-`ğ±`: concatenated values of the L2 penalties coefficients in real space

RETURN
-log of the evidence
"""
function logevidence!(memory::Memoryforgradient,
					model::Model,
					ğ›‰::Vector{<:Real},
					ğâ‚€ğ°â‚€::Vector{<:Real},
					ğ‡::Matrix{<:Real},
					ğ±::Vector{<:Real})
	FHMDDM.real2native!(model.gaussianprior, ğ±)
	FHMDDM.precisionmatrix!(model.gaussianprior)
	@unpack indexğš½, ğš½ = model.gaussianprior
    ğ° = (ğš½-ğ‡) \ ğâ‚€ğ°â‚€ # LAPACK.sysv! uses less memory but is slower
	ğ›‰[indexğš½] .= ğ°
	FHMDDM.loglikelihood!(model, memory, ğ›‰)
	FHMDDM.logevidence(ğ‡, memory.â„“[1], ğš½, ğ°)
end

"""
	âˆ‡negativelogevidence!(âˆ‡nğ¸, memory, model, ğ‡, ğâ‚€ğ›‰â‚€, ğ±)

Gradient of the negative log of the marginal likelihood with respect to the coefficients in real space

MODIFIED ARGUMENT
-`memory`: a structure containing memory for in-place computation
-`model`: structure containing the parameters, hyperparameters, and data
-`âˆ‡nğ¸`: memory for in-place computation of the gradient of the negative of the log-evidence
-`âˆ‡nâ„“`: memory for in-place computation of the gradient of the negative of the log-evidence
-`ğ›‰`: memory for in-place computation of the approximate posterior mode as a function of the hyperparameters

UNMODIFIED ARGUMENT
-`ğâ‚€ğ°â‚€`: Hessian of the log-posterior evalued at the MAP values of the parameters multiplied by the MAP value of the parameters, containing only the parameters associated with the hyperparameters being optimized
-`ğ‡`: Hessian of the log-likelihood evaluated at the MAP values of the parameters, containing only the parameters associated with the hyperparameters being optimized
-`ğ±`: concatenated values of the L2 penalties coefficients in real space
"""
function âˆ‡negativelogevidence!(memory::Memoryforgradient,
								model::Model,
								âˆ‡nğ¸::Vector{<:Real},
								âˆ‡nâ„“::Vector{<:Real},
								ğ›‰::Vector{<:Real},
								ğâ‚€ğ°â‚€::Vector{<:Real},
								ğ‡::Matrix{<:Real},
								ğ±::Vector{<:Real})
	real2native!(model.gaussianprior, ğ±)
	@unpack ğ€, ğ›‚, ğ›‚max, ğ›‚min, indexğš½, ğš½, indexğ€_in_indexğš½ = model.gaussianprior
	ğ = ğš½-ğ‡
	C = factorize(ğ)
	ğ° = C \ ğâ‚€ğ°â‚€
	ğ›‰[indexğš½] .= ğ°
	âˆ‡negativeloglikelihood!(âˆ‡nâ„“, memory, model, ğ›‰)
	ğ¦ = C \ (ğš½*ğ° + âˆ‡nâ„“[indexğš½])
	ğ›€ = (C \ (ğš½ \ ğ‡)')'
	@inbounds for j = 1:length(ğ›‚)
		ğ°â±¼ = ğ°[indexğ€_in_indexğš½[j]]
		ğ¦â±¼ = ğ¦[indexğ€_in_indexğš½[j]]
		ğ›€â±¼ = ğ›€[indexğ€_in_indexğš½[j],indexğ€_in_indexğš½[j]]
		ğ€â±¼ = ğ€[j]
		ğ°â±¼áµ€ğ€â±¼ = transpose(ğ°â±¼)*ğ€â±¼
		âˆ‡nğ¸[j] = 0.5*(ğ°â±¼áµ€ğ€â±¼*ğ°â±¼ + tr(ğ€â±¼*ğ›€â±¼)) - ğ°â±¼áµ€ğ€â±¼*ğ¦â±¼
		âˆ‡nğ¸[j] *= differentiate_native_wrt_real(ğ±[j], ğ›‚min[j], ğ›‚max[j])
	end
	return nothing
end

"""
	logevidence(ğ›‚, ğâ‚€ğ›‰â‚˜â‚â‚š, ğ‡, indexÎ¸, model)

ForwardDiff-computation evaluation of the log-evidence

ARGUMENT
-`ğ›‚`: precisions being learned
-`ğâ‚€ğ›‰â‚˜â‚â‚š`: Hessian of the log-posterior evalued at the MAP values of the parameters multiplied by the MAP value of the parameters
-`ğ‡`: Hessian of the log-likelihood evaluated at the MAP values of the parameters
-`indexÎ¸`: index of the parameters
-`model`: structure containing the parameters, hyperparameters, and data

RETURN
-log of the marginal likelihood
"""
function logevidence(ğâ‚€ğ°â‚€::Vector{<:Real},
					 ğ‡::Matrix{<:Real},
					 model::Model,
					 ğ±::Vector{type},) where{type<:Real}
	gaussianprior = real2native(model.gaussianprior, ğ±)
	@unpack indexğš½, ğš½ = gaussianprior
	ğ = ğš½-ğ‡
    ğ° = ğ \ ğâ‚€ğ°â‚€
	ğ›‰ = concatenateparameters(model)
	indexğ›‰ = indexparameters(model)
	ğ›‰ = ğ›‰ .- zero(type)
	ğ›‰[indexğš½] .= ğ°
	â„“ = loglikelihood(ğ›‰, indexğ›‰, model)
	logevidence(ğ‡, â„“, ğš½, ğ°)
end

"""
	savetrace(MAP_values_converged, ğ‡, iteration, model)

Save the hessian of the log-likelihood with the
"""
function savetrace(MAP_values_converged::Bool,
					ğ‡::Matrix{<:AbstractFloat},
					iteration::Integer,
					model::Model;
					folderpath::String=dirname(model.options.datapath))
	modelsummary = dictionary(ModelSummary(model))
	dict = Dict((key=>modelsummary[key] for key in keys(modelsummary))...,
				"MAP_values_converged"=>MAP_values_converged,
				"hessian_loglikelihood"=>ğ‡,
				"hessian_logposterior"=>ğ‡-modelsummary["precisionmatrix"])
	filename = "evidence_optimization_iteration_"*string(iteration)*".mat"
	filepath = joinpath(folderpath, filename)
	matwrite(filepath, dict)
end
