"""
    likelihood(mpGLM, j, k)

Conditional likelihood of the spike train, given the index of the state of the accumulator `j` and the state of the coupling `k`, and also by the prior likelihood of the regression weights

UNMODIFIED ARGUMENT
-`mpGLM`: the mixture of Poisson generalized linear model for one neuron
-`j`: state of accumulator variable
-`k`: state of the coupling variable

RETURN
-`ğ©`: a vector by which the conditional likelihood of the spike train and the prior likelihood of the regression weights are multiplied against
"""
function likelihood(mpGLM::MixturePoissonGLM, j::Integer, k::Integer)
    @unpack Î”t, ğ² = mpGLM
    ğ‹ = linearpredictor(mpGLM, j, k)
    ğ© = ğ‹ # reuse memory
    @inbounds for i=1:length(ğ©)
        ğ©[i] = poissonlikelihood(Î”t, ğ‹[i], ğ²[i])
    end
    return ğ©
end

"""
    likelihood!(ğ©, mpGLM, j, k)

In-place multiplication of `ğ©` by the conditional likelihood of the spike train, given the index of the state of the accumulator `j` and the state of the coupling `k`, and also by the prior likelihood of the regression weights

MODIFIED ARGUMENT
-`ğ©`: a vector by which the conditional likelihood of the spike train and the prior likelihood of the regression weights are multiplied against

UNMODIFIED ARGUMENT
-`mpGLM`: the mixture of Poisson generalized linear model for one neuron
-`j`: state of accumulator variable
-`k`: state of the coupling variable

RETURN
-`nothing`
"""
function likelihood!(ğ©::Vector{<:Real}, mpGLM::MixturePoissonGLM, j::Integer, k::Integer)
    @unpack Î”t, ğ² = mpGLM
    ğ‹ = linearpredictor(mpGLM, j, k)
    @inbounds for i=1:length(ğ©)
		ğ©[i] *= poissonlikelihood(Î”t, ğ‹[i], ğ²[i])
    end
    return nothing
end

"""
	Poissonlikelihood(Î»Î”t, L, y)

Probability of a Poisson observation

ARGUMENT
-`Î»Î”t`: the expected value
-`y`: the observation
-`y!`: the factorial of the observation

OUTPUT
-the likelihood
"""
function poissonlikelihood(Î”t::Real, L::Real, y::Integer)
	Î»Î”t = softplus(L)*Î”t
	if y==0
		1/exp(Î»Î”t)
	elseif y==1
		Î»Î”t/exp(Î»Î”t)
	else
		Î»Î”t^y / exp(Î»Î”t) / factorial(y)
	end
end

"""
    linearpredictor(mpGLM, j, k)

Linear combination of the weights in the j-th accumulator state and k-th coupling state

ARGUMENT
-`mpGLM`: the mixture of Poisson generalized linear model of one neuron
-`j`: state of the accumulator variable
-`k`: state of the coupling variable

RETURN
-`ğ‹`: a vector whose element ğ‹[t] corresponds to the t-th time bin in the trialset
"""
function linearpredictor(mpGLM::MixturePoissonGLM, j::Integer, k::Integer)
    @unpack ğ—, dğ›_dB = mpGLM
    @unpack ğ®, ğ¯ = mpGLM.Î¸
	ğ—*vcat(ğ®, ğ¯[k].*dğ›_dB[j])
end

"""
    expectation_loglikelihood(Î³, mpGLM, x)

ForwardDiff-compatible computation of the expectation of the log-likelihood of the mixture of Poisson generalized model of one neuron

Ignores the log(y!) term, which does not depend on the parameters

ARGUMENT
-`Î³`: posterior probability of the latent variable
-`mpGLM`: the GLM of one neuron
-`x`: weights of the linear filters of the GLM concatenated as a vector of floating-point numbers

RETURN
-expectation of the log-likelihood of the spike train of one neuron
"""
function expectation_loglikelihood(concatenatedÎ¸::Vector{<:Real},
								   Î³::Matrix{<:Vector{<:AbstractFloat}},
								   mpGLM::MixturePoissonGLM)
	mpGLM = MixturePoissonGLM(concatenatedÎ¸, mpGLM)
    @unpack Î”t, ğ² = mpGLM
    T = length(ğ²)
    Î,K = size(Î³)
    Q = 0.0
    @inbounds for i = 1:Î
	    for k = 1:K
			ğ‹ = linearpredictor(mpGLM,i,k)
            for t = 1:T
				Q += Î³[i,k][t]*poissonloglikelihood(Î”t, ğ‹[t], ğ²[t])
            end
        end
    end
    return Q
end

"""
	expectation_âˆ‡loglikelihood!(âˆ‡Q, indexÎ¸, Î³, mpGLM)

Expectation under the posterior probability of the gradient of the log-likelihood

MODIFIED ARGUMENT
-`âˆ‡`: The gradient

UNMODIFIED ARGUMENT
-`Î³`: Joint posterior probability of the accumulator and coupling variable. Î³[i,k][t] corresponds to the i-th accumulator state and the k-th coupling state in the t-th time bin in the trialset.
-`mpGLM`: structure containing information for the mixture of Poisson GLM for one neuron

EXAMPLE
```julia-rep
julia> using FHMDDM, Random
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_05_05_test/data.mat"; randomize=true);
julia> mpGLM = model.trialsets[1].mpGLMs[1]
julia> Î³ = FHMDDM.randomposterior(mpGLM; rng=MersenneTwister(1234))
julia> âˆ‡Q = FHMDDM.GLMÎ¸(mpGLM.Î¸, eltype(mpGLM.Î¸.ğ®))
julia> FHMDDM.expectation_âˆ‡loglikelihood!(âˆ‡Q, Î³, mpGLM)
julia> ghand = FHMDDM.concatenateparameters(âˆ‡Q)[1]
julia> using ForwardDiff
julia> concatenatedÎ¸ = FHMDDM.concatenateparameters(mpGLM.Î¸)[1]
julia> f(x) = FHMDDM.expectation_loglikelihood(x, Î³, mpGLM)
julia> gauto = ForwardDiff.gradient(f, concatenatedÎ¸)
julia> maximum(abs.(gauto .- ghand))
```
"""
function expectation_âˆ‡loglikelihood!(âˆ‡Q::GLMÎ¸,
	                                Î³::Matrix{<:Vector{<:Real}},
	                                mpGLM::MixturePoissonGLM)
	@unpack Î”t, dğ›_dB, ğ•, ğ—, ğ² = mpGLM
	Î, K = size(Î³)
	T = length(ğ²)
	âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚– = collect(zeros(T) for k=1:K)
	âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–â¨€dÎ¾áµ¢_dB = collect(zeros(T) for k=1:K)
	@inbounds for i = 1:Î
		for k = 1:K
			ğ‹ = linearpredictor(mpGLM,i,k)
			for t=1:T
				dQáµ¢â‚–_dLáµ¢â‚– = Î³[i,k][t] * differentiate_loglikelihood_wrt_linearpredictor(Î”t, ğ‹[t], ğ²[t])
				âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–[k][t] += dQáµ¢â‚–_dLáµ¢â‚–
				âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–â¨€dÎ¾áµ¢_dB[k][t] += dQáµ¢â‚–_dLáµ¢â‚–*dğ›_dB[i]
			end
		end
	end
	q = size(ğ—,2)-size(ğ•,2)
	ğ”áµ€ = transpose(@view ğ—[:,1:q])
	ğ•áµ€ = transpose(ğ•)
	âˆ‡Q.ğ® .= ğ”áµ€*sum(âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–)
	@inbounds for k = 1:K
		âˆ‡Q.ğ¯[k] .= ğ•áµ€*âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–â¨€dÎ¾áµ¢_dB[k]
	end
	return nothing
end

"""
	learn_state_independent_filters!(model)

Learn the filters of the state-independent inputs of each neuron's GLM

EXAMPLE
```julia-repl
julia> using FHMDDM, Random
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_05_05_test/data.mat"; randomize=true);
julia> FHMDDM.learn_state_independent_filters!(model)
```
"""
function learn_state_independent_filters!(model::Model)
	q = length(model.trialsets[1].mpGLMs[1].Î¸.ğ®)
	Opt = PoissonGLMOptimization(ğ® = fill(NaN, q))
	for trialset in model.trialsets
		for mpGLM in trialset.mpGLMs
			learn_state_independent_filters!(mpGLM, Opt)
		end
	end
	return nothing
end

"""
    learn_state_independent_filters!(mpGLM, Opt)

Learn the filters of the state-independent inputs

ARGUMENT
-`mpGLM`: the Poisson mixture GLM of one neuron

OPTIONAL ARGUMENT
-`show_trace`: whether to show information about each step of the optimization

RETURN
-weights concatenated into a single vector

EXAMPLE
```julia-repl
julia> using FHMDDM, Random
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_05_05_test/data.mat"; randomize=true);
julia> mpGLM = model.trialsets[1].mpGLMs[1]
julia> Opt = FHMDDM.PoissonGLMOptimization(ğ® = fill(NaN, length(mpGLM.Î¸.ğ®)))
julia> FHMDDM.estimatefilters!(mpGLM, Opt)
```
"""
function learn_state_independent_filters!(mpGLM::MixturePoissonGLM,
										Opt::PoissonGLMOptimization,
										iterations::Integer=20,
										show_trace::Bool=false)
    f(ğ®) = -loglikelihood!(mpGLM,Opt,ğ®)
	g!(âˆ‡, ğ®) = âˆ‡negloglikelihood!(âˆ‡,mpGLM,Opt,ğ®)
	h!(âˆ‡âˆ‡, ğ®) = âˆ‡âˆ‡negloglikelihood!(âˆ‡âˆ‡,mpGLM,Opt,ğ®)
    results = Optim.optimize(f, g!, h!, copy(mpGLM.Î¸.ğ®), NewtonTrustRegion(), Optim.Options(show_trace=show_trace, iterations=iterations))
	mpGLM.Î¸.ğ® .= Optim.minimizer(results)
	return nothing
end

"""
	loglikelihood!(mpGLM, Opt, concatenatedÎ¸)

Compute the log-likelihood of Poisson GLM

ARGUMENT
-`mpGLM`: a structure containing the parameters, input, and observations a mixture of Poisson GLM
-`Opt`: a structure for maximizing the expectation of the log-likelihood of a mixture of Poisson GLM
-`concatenatedÎ¸`: parameters of a GLM concatenated into a vector

RETURN
-log-likelihood of a Poisson GLM
"""
function loglikelihood!(mpGLM::MixturePoissonGLM,
						Opt::PoissonGLMOptimization,
						ğ®::Vector{<:AbstractFloat})
	update!(mpGLM, Opt, ğ®)
	return Opt.â„“[1]
end

"""
	âˆ‡negloglikelihood!(g, mpGLM, Opt, Î³, concatenatedÎ¸)

Compute the gradient of the negative of the log-likelihood of a Poisson GLM

MODIFIED ARGUMENT
-`g`: gradient

ARGUMENT
-`mpGLM`: a structure containing the parameters, input, and observations a mixture of Poisson GLM
-`Opt`: a structure for maximizing the expectation of the log-likelihood of a mixture of Poisson GLM
-`concatenatedÎ¸`: parameters of a GLM concatenated into a vector
"""
function âˆ‡negloglikelihood!(g::Vector{<:AbstractFloat},
							mpGLM::MixturePoissonGLM,
							Opt::PoissonGLMOptimization,
							ğ®::Vector{<:AbstractFloat})
	update!(mpGLM, Opt, ğ®)
	for i in eachindex(g)
		g[i] = -Opt.âˆ‡â„“[i]
	end
	return nothing
end

"""
	âˆ‡âˆ‡negloglikelihood!(h, mpGLM, Opt, Î³, concatenatedÎ¸)

Compute the hessian of the negative of the log-likelihood of a Poisson GLM

MODIFIED ARGUMENT
-`h`: hessian

ARGUMENT
-`mpGLM`: a structure containing the parameters, input, and observations a mixture of Poisson GLM
-`Opt`: a structure for maximizing the expectation of the log-likelihood of a mixture of Poisson GLM
-`concatenatedÎ¸`: parameters of a GLM concatenated into a vector
"""
function âˆ‡âˆ‡negloglikelihood!(h::Matrix{<:AbstractFloat},
							mpGLM::MixturePoissonGLM,
							Opt::PoissonGLMOptimization,
							ğ®::Vector{<:AbstractFloat})
	update!(mpGLM, Opt, ğ®)
	for i in eachindex(h)
		h[i] = -Opt.âˆ‡âˆ‡â„“[i]
	end
	return nothing
end

"""
	update!(mpGLM, Opt, concatenatedÎ¸)

Update quantities for computing the log-likelihood of a Poisson GLM and its gradient and hessian

MODIFIED ARGUMENT
-`mpGLM`: a structure containing the parameters, input, and observations a mixture of Poisson GLM
-`Opt`: a structure for maximizing the expectation of the log-likelihood of a mixture of Poisson GLM

ARGUMENT
-`concatenatedÎ¸`: parameters of a GLM concatenated into a vector

EXAMPLE
```julia-rep
julia> using FHMDDM, Random
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_05_05_test/data.mat"; randomize=true);
julia> mpGLM = model.trialsets[1].mpGLMs[1]
julia> Opt = FHMDDM.PoissonGLMOptimization(ğ® = fill(NaN, length(mpGLM.Î¸.ğ®)))
julia> randğ® = copy(mpGLM.Î¸.ğ®)
julia> FHMDDM.update!(mpGLM, Opt, randğ®)
julia> using ForwardDiff
julia> f(ğ®) = FHMDDM.loglikelihood(mpGLM, ğ®)
julia> fauto = f(randğ®)
julia> gauto = ForwardDiff.gradient(f, randğ®)
julia> hauto = ForwardDiff.hessian(f, randğ®)
julia> abs(fauto - Opt.â„“[1])
julia> maximum(abs.(gauto .- Opt.âˆ‡â„“))
julia> maximum(abs.(hauto .- Opt.âˆ‡âˆ‡â„“))
```
"""
function update!(mpGLM::MixturePoissonGLM,
				Opt::PoissonGLMOptimization,
				ğ®::Vector{<:AbstractFloat})
	if ğ® != Opt.ğ®
		Opt.ğ® .= ğ®
		mpGLM.Î¸.ğ® .= ğ®
		âˆ‡âˆ‡loglikelihood!(Opt, mpGLM)
	end
end

"""
    âˆ‡âˆ‡loglikelihood!(Opt, mpGLM)

Compute the log-likelihood of a Poisson mixture GLM and its first and second derivatives

MODIFIED ARGUMENT
-`Opt`: a structure for maximizing the expectation of the log-likelihood of a mixture of Poisson GLM

UNMODIFIED ARGUMENT
-`mpGLM`: the Poisson mixture GLM of one neuron
```
"""
function âˆ‡âˆ‡loglikelihood!(Opt::PoissonGLMOptimization, mpGLM::MixturePoissonGLM)
	@unpack â„“, âˆ‡â„“, âˆ‡âˆ‡â„“ = Opt
    @unpack Î”t, ğ—, ğ² = mpGLM
	@unpack ğ® = mpGLM.Î¸
	ğ” = @view ğ—[:,1:length(ğ®)]
	ğ”áµ€ = transpose(ğ”)
	ğ‹ = ğ”*ğ®
	T = length(ğ²)
	dÂ²â„“_dLÂ², dâ„“_dL = zeros(T), zeros(T)
	â„“[1] = 0.0
	for t = 1:T
		dÂ²â„“_dLÂ²[t], dâ„“_dL[t], â„“â‚œ = differentiate_loglikelihood_twice_wrt_linearpredictor(Î”t, ğ‹[t], ğ²[t])
		â„“[1] += â„“â‚œ
	end
	âˆ‡â„“ .= ğ”áµ€*dâ„“_dL
	âˆ‡âˆ‡â„“ .= ğ”áµ€*(dÂ²â„“_dLÂ².*ğ”)
	return nothing
end

"""
	loglikelihood(mpGLM, concatenatedÎ¸)

Compute the log-likelihood of Poisson GLM

ARGUMENT
-`mpGLM`: a structure containing the parameters, input, and observations a mixture of Poisson GLM
-`ğ®`: filters of the state-independent inputs of the GLM

RETURN
-log-likelihood of a Poisson GLM
"""
function loglikelihood(mpGLM::MixturePoissonGLM, ğ®::Vector{type}) where {type<:Real}
	@unpack Î”t, ğ—, ğ² = mpGLM
	ğ” = @view ğ—[:,1:length(ğ®)]
	ğ”áµ€ = transpose(ğ”)
	ğ‹ = ğ”*ğ®
	T = length(ğ²)
	dÂ²â„“_dLÂ², dâ„“_dL = zeros(type, T), zeros(type, T)
	â„“ = 0.0
	for t = 1:T
		â„“ += poissonloglikelihood(Î”t, ğ‹[t], ğ²[t])
	end
	â„“
end

"""
    poissonloglikelihood

Differentiate the log-likelihood of a Poisson GLM with respect to the linear predictor

The Poisson GLM is assumed to have a a softplus nonlinearity

ARGUMENT
-`Î”t`: duration of time step
-`L`: linear predictor
-`y`: observation

RETURN
-log-likelihood
"""
function poissonloglikelihood(Î”t::AbstractFloat, L::Real, y::Integer)
    Î»Î”t = softplus(L)*Î”t
	if y == 0
		-Î»Î”t
	elseif y == 1
		log(Î»Î”t) - Î»Î”t
	else
		y*log(Î»Î”t) - Î»Î”t
	end
end

"""
    differentiate_loglikelihood_twice_wrt_linearpredictor

First derivative of the log-likelihood of a Poisson GLM with respect to the linear predictor

The Poisson GLM is assumed to have a a softplus nonlinearity

ARGUMENT
-`Î”t`: duration of time step
-`L`: linear predictor
-`y`: observation

RETURN
-first derivative of the log-likelihood with respect to the linear predictor
"""
function differentiate_loglikelihood_wrt_linearpredictor(Î”t::AbstractFloat, L::Real, y::Integer)
    fâ‚ = logistic(L)
	if y > 0
        if L > -100.0
			fâ‚€ = softplus(L)
            fâ‚*(y/fâ‚€ - Î”t)
        else
            y - fâ‚*Î”t # the limit of logistic(x)/softplus(x) as x goes to -âˆ is 1
        end
    else
        -fâ‚*Î”t
    end
end

"""
    differentiate_loglikelihood_twice_wrt_linearpredictor

Second derivative of the log-likelihood of a Poisson GLM with respect to the linear predictor

The Poisson GLM is assumed to have a a softplus nonlinearity

ARGUMENT
-`Î”t`: duration of time step
-`L`: linear predictor
-`y`: observation

RETURN
-second derivative of the log-likelihood with respect to the linear predictor
-first derivative of the log-likelihood with respect to the linear predictor
-log-likelihood
"""
function differentiate_loglikelihood_twice_wrt_linearpredictor(Î”t::AbstractFloat, L::Real, y::Integer)
	fâ‚€ = softplus(L)
    fâ‚ = logistic(L)
    fâ‚‚ = fâ‚*(1.0-fâ‚)
	Î»Î”t = fâ‚€*Î”t
	if y == 0
		â„“ = -Î»Î”t
	elseif y == 1
		â„“ = log(Î»Î”t) - Î»Î”t
	else
		â„“ = y*log(Î»Î”t) - Î»Î”t
	end
	if y > 0
        if L > -100.0
            dâ„“_dL= fâ‚*(y/fâ‚€ - Î”t)
        else
            dâ„“_dL = y - fâ‚*Î”t # the limit of logistic(x)/softplus(x) as x goes to -âˆ is 1
        end
    else
        dâ„“_dL = -fâ‚*Î”t
    end
    if y > 0 && L > -50.0
        dÂ²â„“_dLÂ² = y*(fâ‚€*fâ‚‚ - fâ‚^2)/fâ‚€^2 - fâ‚‚*Î”t # the limit of the second term is 0 as xw goes to -âˆ
    else
        dÂ²â„“_dLÂ² = -fâ‚‚*Î”t
    end
	return dÂ²â„“_dLÂ², dâ„“_dL, â„“
end

"""
	GLMÎ¸(K, ğ‡, ğ”, ğ•)

Randomly initiate the parameters for a mixture of Poisson generalized linear model

ARGUMENT
-`K`: number of coupling states
-`ğ—`: constant input, time-varying inputs from spike history, time-varying inputs from trial events
-`ğ•`: constant and time-varying inputs from the accumulator

OUTPUT
-an instance of `GLMÎ¸`
"""
function GLMÎ¸(K::Integer,
			ğ—::Matrix{<:AbstractFloat},
			ğ•::Matrix{<:AbstractFloat})
	nğ¯ =size(ğ•,2)
	nğ® = size(ğ—,2)-size(ğ•,2)
	if K == 1
		ğ¯ = [ones(nğ¯)]
	else
		ğ¯ = collect(i*ones(nğ¯) for i = -1:2/(K-1):1)
	end
	Î¸ = GLMÎ¸(ğ® = 1.0 .- 2.0.*rand(nğ®),
			 ğ¯ = ğ¯)
end

"""
	GLMÎ¸(glmÎ¸, elementtype)

Create an uninitialized instance of GLMÎ¸ with the given element type.

This is for using ForwardDiff

ARGUMENT
-`glmÎ¸`: an instance of GLMÎ¸
-`elementtype`: type of the element in each field of GLMÎ¸

RETURN
-an instance of GLMÎ¸
"""
function GLMÎ¸(glmÎ¸::GLMÎ¸, elementtype)
	GLMÎ¸(ğ® = zeros(elementtype, length(glmÎ¸.ğ®)),
		 ğ¯ = collect(zeros(elementtype, length(ğ¯)) for ğ¯ in glmÎ¸.ğ¯))
end

"""
    transformaccumulator

Nonlinearly transform the normalized values of the accumulator

ARGUMENT
-`Î¾`: value of the accumulator: expected to be between -1 and 1
-`b`: parameter specifying the transformation

RETURN
-transformed value of the accumulator
"""
function transformaccumulator(b::Real, Î¾::Real)
    if b == 0.0
        Î¾
    else
        if Î¾ < 0
            if b > 709.0 # 709 is close to which exp returns Inf for a 64-bit floating point number
                Î¾ == -1.0 ? -1.0 : 0.0
            else
                -expm1(-b*Î¾)/expm1(b)
                # (exp(-b*Î¾)-1.0)/(1.0-exp(b))
            end
        elseif Î¾ > 0
            if b > 709.0
                Î¾ == 1.0 ? 1.0 : 0.0
            else
                expm1(b*Î¾)/expm1(b)
                # (1.0-exp(b*Î¾))/(1.0-exp(b))
            end
        else
            0.0
        end
    end
end

"""
    dtransformaccumulator

Derivative of the nonlinear transformation of the normalized values of the accumulator with respect to b

ARGUMENT
-`Î¾`: value of the accumulator: expected to be between -1 and 1
-`b`: parameter specifying the transformation

RETURN
-transformed value of the accumulator
"""
function dtransformaccumulator(b::Real, Î¾::Real)
    if Î¾ == -1.0 || Î¾ == 0.0 || Î¾ == 1.0 || b > 709.0 # 709 is close to which exp returns Inf for a 64-bit floating point number
        0.0
    elseif abs(b) < 1e-6
        Î¾ < 0 ? (-Î¾^2-Î¾)/2 : (Î¾^2-Î¾)/2
    elseif Î¾ < 0
        eáµ‡ = exp(b)
        eáµ‡m1 = expm1(b)
        eâ»áµ‡Ë£ = exp(-b*Î¾)
        eâ»áµ‡Ë£m1 = expm1(-b*Î¾)
        if b < 1
            (Î¾*eâ»áµ‡Ë£*eáµ‡m1 + eâ»áµ‡Ë£m1*eáµ‡)/eáµ‡m1^2
        else
            Î¾*eâ»áµ‡Ë£/eáµ‡m1 + eâ»áµ‡Ë£m1/(eáµ‡-2+exp(-b))
        end
    elseif Î¾ > 0
        eáµ‡ = exp(b)
        eáµ‡m1 = expm1(b)
        eáµ‡Ë£ = exp(b*Î¾)
        eáµ‡Ë£m1 = expm1(b*Î¾)
        if b < 1
            Î¾*eáµ‡Ë£/eáµ‡m1 - eáµ‡Ë£m1*eáµ‡/eáµ‡m1^2
        else
            Î¾*eáµ‡Ë£/eáµ‡m1 - eáµ‡Ë£m1/(eáµ‡-2+exp(-b))
        end
    end
end

"""
	maximize_expectation_of_loglikelihood!(mpGLM, Î³)

Learn the filters of a Poisson mixture GLM by maximizing the expectation of the log-likelihood

MODIFIED ARGUMENT
-`mpGLM`: a structure containing the data and parameters of the mixture of Poisson GLM of one neuron

UNMODIFIED ARGUMENT
-`Î³`: posterior probability of the latent variables. Element `Î³[j][Ï„]` corresponds to the posterior probability of the j-th accumulator state  in the Ï„-th time step

EXAMPLE
```julia-repl
julia> using FHMDDM
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_05_05_test/data.mat");
julia> maximize_choice_posterior!(model)
julia> Î³ = choiceposteriors(model)[1]
julia> mpGLM = model.trialsets[1].mpGLMs[2]
julia> FHMDDM.maximize_expectation_of_loglikelihood!(mpGLM, Î³)
```
"""
function maximize_expectation_of_loglikelihood!(mpGLM::MixturePoissonGLM, Î³::Matrix{<:Vector{<:Real}}; show_trace::Bool=false, iterations::Integer=20)
	xâ‚€ = concatenateparameters(mpGLM.Î¸)
	nparameters = length(xâ‚€)
	Q = fill(NaN,1)
	âˆ‡Q = fill(NaN, nparameters)
	âˆ‡âˆ‡Q = fill(NaN, nparameters, nparameters)
	f(x) = -expectation_of_loglikelihood!(mpGLM,Q,âˆ‡Q,âˆ‡âˆ‡Q,Î³,x)
	âˆ‡f!(âˆ‡, x) = negexpectation_of_âˆ‡loglikelihood!(âˆ‡,mpGLM,Q,âˆ‡Q,âˆ‡âˆ‡Q,Î³,x)
	âˆ‡âˆ‡f!(âˆ‡âˆ‡, x) = negexpectation_of_âˆ‡âˆ‡loglikelihood!(âˆ‡âˆ‡,mpGLM,Q,âˆ‡Q,âˆ‡âˆ‡Q,Î³,x)
    results = Optim.optimize(f, âˆ‡f!, âˆ‡âˆ‡f!, xâ‚€, NewtonTrustRegion(), Optim.Options(show_trace=show_trace, iterations=iterations))
	sortparameters!(mpGLM.Î¸, Optim.minimizer(results))
	return nothing
end

"""
	expectation_of_loglikelihood!(mpGLM,Q,âˆ‡Q,âˆ‡âˆ‡Q,Î³,x)

Expectation of the log-likelihood under the posterior probability of the latent variables

MODIFIED ARGUMENT
-`mpGLM`: a structure containing the data and parameters of the mixture of Poisson GLM of one neuron
-`Q`: an one-element vector that quantifies the expectation
-`âˆ‡Q`: gradient of the expectation with respect to the filters in the k-th state
-`âˆ‡âˆ‡Q`: Hessian of the expectation with respect to the filters in the k-th state

UNMODIFIED ARGUMENT
-`Î³`: posterior probability of the latent variables. Element `Î³[j][Ï„]` corresponds to the posterior probability of the j-th accumulator state  in the Ï„-th time step
-`x`: filters
"""
function expectation_of_loglikelihood!(mpGLM::MixturePoissonGLM, Q::Vector{<:Real}, âˆ‡Q::Vector{<:Real}, âˆ‡âˆ‡Q::Matrix{<:Real}, Î³::Matrix{<:Vector{<:Real}}, x::Vector{<:Real})
	if (x != concatenateparameters(mpGLM.Î¸)[1]) || isnan(Q[1])
		sortparameters!(mpGLM.Î¸, x)
		expectation_of_âˆ‡âˆ‡loglikelihood!(Q,âˆ‡Q,âˆ‡âˆ‡Q,Î³,mpGLM)
	end
	Q[1]
end

"""
	negexpectation_of_âˆ‡loglikelihood!(âˆ‡,mpGLM,Q,âˆ‡Q,âˆ‡âˆ‡Q,Î³,x)

Gradient of the negative of the expectation of the log-likelihood under the posterior probability of the latent variables

MODIFIED ARGUMENT
-`âˆ‡`: gradient of the negative of the expectation
-`mpGLM`: a structure containing the data and parameters of the mixture of Poisson GLM of one neuron
-`Q`: an one-element vector that quantifies the expectation
-`âˆ‡Q`: gradient of the expectation with respect to the filters in the k-th state
-`âˆ‡âˆ‡Q`: Hessian of the expectation with respect to the filters in the k-th state

UNMODIFIED ARGUMENT
-`Î³`: posterior probability of the latent variables. Element `Î³[j][Ï„]` corresponds to the posterior probability of the j-th accumulator state  in the Ï„-th time step
-`x`: filters
"""
function negexpectation_of_âˆ‡loglikelihood!(âˆ‡::Vector{<:Real}, mpGLM::MixturePoissonGLM, Q::Vector{<:Real}, âˆ‡Q::Vector{<:Real}, âˆ‡âˆ‡Q::Matrix{<:Real}, Î³::Matrix{<:Vector{<:Real}}, x::Vector{<:Real})
	if (x != concatenateparameters(mpGLM.Î¸)[1]) || isnan(Q[1])
		sortparameters!(mpGLM.Î¸, x)
		expectation_of_âˆ‡âˆ‡loglikelihood!(Q,âˆ‡Q,âˆ‡âˆ‡Q,Î³,mpGLM)
	end
	for i in eachindex(âˆ‡)
		âˆ‡[i] = -âˆ‡Q[i]
	end
	return nothing
end

"""
	negexpectation_of_âˆ‡âˆ‡loglikelihood!(âˆ‡âˆ‡,mpGLM,Q,âˆ‡Q,âˆ‡âˆ‡Q,Î³,x)

Hessian of the negative of the expectation of the log-likelihood under the posterior probability of the latent variables

MODIFIED ARGUMENT
-`âˆ‡âˆ‡`: Hessian of the negative of the expectation
-`mpGLM`: a structure containing the data and parameters of the mixture of Poisson GLM of one neuron
-`Q`: an one-element vector that quantifies the expectation
-`âˆ‡Q`: gradient of the expectation with respect to the filters in the k-th state
-`âˆ‡âˆ‡Q`: Hessian of the expectation with respect to the filters in the k-th state

UNMODIFIED ARGUMENT
-`Î³`: posterior probability of the latent variables. Element `Î³[j][Ï„]` corresponds to the posterior probability of the j-th accumulator state  in the Ï„-th time step
-`x`: filters
"""
function negexpectation_of_âˆ‡âˆ‡loglikelihood!(âˆ‡âˆ‡::Matrix{<:Real}, mpGLM::MixturePoissonGLM, Q::Vector{<:Real}, âˆ‡Q::Vector{<:Real}, âˆ‡âˆ‡Q::Matrix{<:Real}, Î³::Matrix{<:Vector{<:Real}}, x::Vector{<:Real})
	if (x != concatenateparameters(mpGLM.Î¸)[1]) || isnan(Q[1])
		sortparameters!(mpGLM.Î¸, x)
		expectation_of_âˆ‡âˆ‡loglikelihood!(Q,âˆ‡Q,âˆ‡âˆ‡Q,Î³,mpGLM)
	end
	nparameters = length(x)
	for i =1:nparameters
		for j=i:nparameters
			âˆ‡âˆ‡[i,j] = âˆ‡âˆ‡[j,i] = -âˆ‡âˆ‡Q[i,j]
		end
	end
	return nothing
end

"""
	expectation_of_âˆ‡âˆ‡loglikelihood!(Q,âˆ‡Q,âˆ‡âˆ‡Q,Î³,k,mpGLM)

Compute the expectation of the log-likelihood and its gradient and Hessian

ARGUMENT
-`Q`: expectation of the log-likelihood under the posterior probability of the latent variables. Only the component in the coupling state `k` is included
-`âˆ‡Q`: first-order derivatives of the expectation
-`âˆ‡âˆ‡Q`: second-order derivatives of the expectation

UNMODIFIED ARGUMENT
-`Î³`: posterior probabilities of the latent variables
-`k`: index of the coupling state
-`mpGLM`: a structure containing the data and parameters of the mixture of Poisson GLM of one neuron

EXAMPLE
```julia-repl
julia> using FHMDDM, ForwardDiff, Random
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_05_05_test/data.mat");
julia> mpGLM = model.trialsets[1].mpGLMs[1]
julia> Î³ = FHMDDM.randomposterior(mpGLM; rng=MersenneTwister(1234))
julia> xâ‚€ = concatenateparameters(mpGLM.Î¸)
julia> nparameters = length(xâ‚€)
julia> fhand, ghand, hhand = fill(NaN,1), fill(NaN,nparameters), fill(NaN,nparameters,nparameters)
julia> FHMDDM.expectation_of_âˆ‡âˆ‡loglikelihood!(fhand, ghand, hhand, Î³, mpGLM)
julia> f(x) = FHMDDM.expectation_of_loglikelihood(Î³, mpGLM, x)
julia> fauto = f(xâ‚€)
julia> gauto = ForwardDiff.gradient(f, xâ‚€)
julia> hauto = ForwardDiff.hessian(f, xâ‚€)
julia> abs(fauto - fhand[1])
julia> maximum(abs.(gauto .- ghand))
julia> maximum(abs.(hauto .- hhand))
```
"""
function expectation_of_âˆ‡âˆ‡loglikelihood!(Q::Vector{<:Real},
										âˆ‡Q::Vector{<:Real},
										âˆ‡âˆ‡Q::Matrix{<:Real},
										Î³::Matrix{<:Vector{<:Real}},
										mpGLM::MixturePoissonGLM)
    @unpack Î”t, ğ•, ğ—, dğ›_dB, ğ² = mpGLM
	dğ›_dBÂ² = dğ›_dB.^2
	Î,K = size(Î³)
	T = length(ğ²)
	âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚– = collect(zeros(T) for k=1:K)
	âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–â¨€dÎ¾áµ¢_dB = collect(zeros(T) for k=1:K)
	âˆ‘áµ¢_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â² = collect(zeros(T) for k=1:K)
	âˆ‘áµ¢_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dB = collect(zeros(T) for k=1:K)
	âˆ‘áµ¢_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dBÂ² = collect(zeros(T) for k=1:K)
	Q[1] = 0.0
	âˆ‡Q .= 0.0
	âˆ‡âˆ‡Q .= 0.0
	@inbounds for i = 1:Î
		for k = 1:K
			ğ‹ = linearpredictor(mpGLM,i,k)
			for t=1:T
				dÂ²â„“_dLÂ², dâ„“_dL, â„“ = differentiate_loglikelihood_twice_wrt_linearpredictor(Î”t, ğ‹[t], ğ²[t])
				Q[1] += Î³[i,k][t]*â„“
				dQáµ¢â‚–_dLáµ¢â‚– = Î³[i,k][t] * dâ„“_dL
				âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–[k][t] += dQáµ¢â‚–_dLáµ¢â‚–
				âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–â¨€dÎ¾áµ¢_dB[k][t] += dQáµ¢â‚–_dLáµ¢â‚–*dğ›_dB[i]
				dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â² = Î³[i,k][t] * dÂ²â„“_dLÂ²
				âˆ‘áµ¢_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²[k][t] += dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²
				âˆ‘áµ¢_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dB[k][t] += dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²*dğ›_dB[i]
				âˆ‘áµ¢_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dBÂ²[k][t] += dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²*dğ›_dBÂ²[i]
			end
		end
	end
	nğ® = length(mpGLM.Î¸.ğ®)
	nğ¯â‚– = length(mpGLM.Î¸.ğ¯[1])
	indicesğ® = 1:nğ®
	ğ” = @view ğ—[:, indicesğ®]
	ğ”áµ€, ğ•áµ€ = transpose(ğ”), transpose(ğ•)
	âˆ‘áµ¢â‚–_dQáµ¢â‚–_dLáµ¢â‚– = sum(âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–)
	âˆ‘áµ¢â‚–_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â² = sum(âˆ‘áµ¢_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²)
	âˆ‡Q[indicesğ®] = ğ”áµ€*âˆ‘áµ¢â‚–_dQáµ¢â‚–_dLáµ¢â‚–
	âˆ‡âˆ‡Q[indicesğ®, indicesğ®] = ğ”áµ€*(âˆ‘áµ¢â‚–_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â².*ğ”)
	@inbounds for k=1:K
		indicesğ¯â‚– = nğ®+(k-1)*nğ¯â‚–+1:nğ®+k*nğ¯â‚–
		âˆ‡Q[indicesğ¯â‚–] = ğ•áµ€*âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–â¨€dÎ¾áµ¢_dB[k]
		âˆ‡âˆ‡Q[indicesğ®, indicesğ¯â‚–] = ğ”áµ€*(âˆ‘áµ¢_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dB[k].*ğ•)
		âˆ‡âˆ‡Q[indicesğ¯â‚–, indicesğ¯â‚–] = ğ•áµ€*(âˆ‘áµ¢_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dBÂ²[k].*ğ•)
		for i in indicesğ®
			for j in indicesğ¯â‚–
				âˆ‡âˆ‡Q[j,i] = âˆ‡âˆ‡Q[i,j]
			end
		end
	end
	return nothing
end

"""
    expectation_of_loglikelihood(Î³, mpGLM, x)

ForwardDiff-compatible computation of the expectation of the log-likelihood of the mixture of Poisson generalized model of one neuron

Ignores the log(y!) term, which does not depend on the parameters

ARGUMENT
-`Î³`: posterior probability of the latent variable
-`mpGLM`: the GLM of one neuron
-`x`: weights of the linear filters of the GLM concatenated as a vector of floating-point numbers

RETURN
-expectation of the log-likelihood of the spike train of one neuron
"""
function expectation_of_loglikelihood(Î³::Matrix{<:Vector{<:AbstractFloat}},
									   mpGLM::MixturePoissonGLM,
									   x::Vector{<:Real})
	mpGLM = MixturePoissonGLM(x, mpGLM)
    @unpack Î”t, ğ² = mpGLM
    T = length(ğ²)
    Î,K = size(Î³)
    Q = 0.0
    @inbounds for i = 1:Î
	    for k = 1:K
			ğ‹ = linearpredictor(mpGLM,i,k)
            for t = 1:T
				Q += Î³[i,k][t]*poissonloglikelihood(Î”t, ğ‹[t], ğ²[t])
            end
        end
    end
    return Q
end
