"""
    likelihood(mpGLM, j, k)

Conditional likelihood of the spike train, given the index of the state of the accumulator `j` and the state of the coupling `k`, and also by the prior likelihood of the regression weights

UNMODIFIED ARGUMENT
-`mpGLM`: the mixture of Poisson generalized linear model for one neuron
-`j`: state of accumulator variable
-`k`: state of the coupling variable

RETURN
-`ğ©`: a vector by which the conditional likelihood of the spike train and the prior likelihood of the regression weights are multiplied against
"""
function likelihood(mpGLM::MixturePoissonGLM, j::Integer, k::Integer)
    @unpack Î”t, ğ² = mpGLM
    ğ‹ = linearpredictor(mpGLM, j, k)
    ğ© = ğ‹ # reuse memory
    @inbounds for i=1:length(ğ©)
        ğ©[i] = poissonlikelihood(Î”t, ğ‹[i], ğ²[i])
    end
    return ğ©
end

"""
    likelihood!(ğ©, mpGLM, j, k)

In-place multiplication of `ğ©` by the conditional likelihood of the spike train, given the index of the state of the accumulator `j` and the state of the coupling `k`, and also by the prior likelihood of the regression weights

MODIFIED ARGUMENT
-`ğ©`: a vector by which the conditional likelihood of the spike train and the prior likelihood of the regression weights are multiplied against

UNMODIFIED ARGUMENT
-`mpGLM`: the mixture of Poisson generalized linear model for one neuron
-`j`: state of accumulator variable
-`k`: state of the coupling variable

RETURN
-`nothing`
"""
function likelihood!(ğ©::Vector{<:Real}, mpGLM::MixturePoissonGLM, j::Integer, k::Integer)
    @unpack Î”t, ğ² = mpGLM
    ğ‹ = linearpredictor(mpGLM, j, k)
    @inbounds for i=1:length(ğ©)
		ğ©[i] *= poissonlikelihood(Î”t, ğ‹[i], ğ²[i])
    end
    return nothing
end

"""
	Poissonlikelihood(Î»Î”t, y, y!)

Probability of a Poisson observation

ARGUMENT
-`Î»Î”t`: the expected value
-`y`: the observation
-`y!`: the factorial of the observation

OUTPUT
-the likelihood
"""
function poissonlikelihood(Î”t::Real, L::Real, y::Integer)
	Î»Î”t = softplus(L)*Î”t
	if y==0
		1/exp(Î»Î”t)
	elseif y==1
		Î»Î”t/exp(Î»Î”t)
	else
		Î»Î”t^y / exp(Î»Î”t) / factorial(y)
	end
end

"""
    linearpredictor(mpGLM, j, k)

Linear combination of the weights in the j-th accumulator state and k-th coupling state

ARGUMENT
-`mpGLM`: the mixture of Poisson generalized linear model of one neuron
-`j`: state of the accumulator variable
-`k`: state of the coupling variable

RETURN
-`ğ›Œ`: a vector whose element ğ›Œ[t] corresponds to the t-th time bin in the trialset
"""
function linearpredictor(mpGLM::MixturePoissonGLM, j::Integer, k::Integer)
    @unpack ğ‡, ğ”, ğ•, dğ›_dB = mpGLM
    @unpack ğ¡, ğ®, ğ¯, ğ° = mpGLM.Î¸
	ğ‡*ğ¡ .+ ğ”*ğ®[k] .+ ğ•*(ğ¯[k].*dğ›_dB[j]) .+ ğ°[k]
end

"""
    estimatefilters(Î³, Opt, mpGLM)

Estimate the filters of the Poisson mixture GLM of one neuron

ARGUMENT
-`Î³`: posterior probabilities of the latent
-`mpGLM`: the Poisson mixture GLM of one neuron

OPTIONAL ARGUMENT
-`show_trace`: whether to show information about each step of the optimization
-`fit_a`: whether to fit the asymmetric scaling factor
-`fit_b`: whether to fit the nonlinearity factor

RETURN
-weights concatenated into a single vector

EXAMPLE
```julia-repl
julia> using FHMDDM, Random
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_04_27_test/data.mat"; randomize=true);
julia> mpGLM = model.trialsets[1].mpGLMs[1]
julia> concatenatedÎ¸, indexÎ¸ = FHMDDM.concatenateparameters(mpGLM.Î¸)
julia> Opt = FHMDDM.MixturePoissonGLM_Optimization(concatenatedÎ¸=fill(NaN, length(concatenatedÎ¸)), indexÎ¸=indexÎ¸)
julia> Î³ = FHMDDM.randomposterior(mpGLM; rng=MersenneTwister(1234))
julia> FHMDDM.estimatefilters!(mpGLM, Opt, Î³)
```
"""
function estimatefilters!(mpGLM::MixturePoissonGLM,
						Opt::MixturePoissonGLM_Optimization,
						Î³::Matrix{<:Vector{<:AbstractFloat}};
						iterations::Integer=20,
						show_trace::Bool=true)
    xâ‚€ = concatenateparameters(mpGLM.Î¸)[1]
    f(x) = expectation_negloglikelihood!(mpGLM,Opt,Î³,x)
	g!(âˆ‡, x) = expectation_âˆ‡negloglikelihood!(âˆ‡,mpGLM,Opt,Î³,x)
	h!(âˆ‡âˆ‡, x) = expectation_âˆ‡âˆ‡negloglikelihood!(âˆ‡âˆ‡,mpGLM,Opt,Î³,x)
    results = Optim.optimize(f, g!, h!, xâ‚€, NewtonTrustRegion(), Optim.Options(show_trace=show_trace, iterations=iterations))
    show_trace && println("The model converged: ", Optim.converged(results))
	sortparameters!(mpGLM.Î¸, Optim.minimizer(results))
	return nothing
end

"""
	expectation_negloglikelihood!(mpGLM, Opt, Î³, concatenatedÎ¸)

Compute the negative of the expectation of the log-likelihood of a mixture of Poisson GLM

ARGUMENT
-`mpGLM`: a structure containing the parameters, input, and observations a mixture of Poisson GLM
-`Î³`: posterior probabilities of the latent variables
-`Opt`: a structure for maximizing the expectation of the log-likelihood of a mixture of Poisson GLM
-`concatenatedÎ¸`: parameters of a GLM concatenated into a vector

RETURN
-negative of the expectation of the log-likelihood of a mixture of Poisson GLM
"""
function expectation_negloglikelihood!(mpGLM::MixturePoissonGLM,
									Opt::MixturePoissonGLM_Optimization,
									Î³::Matrix{<:Vector{<:AbstractFloat}},
									concatenatedÎ¸::Vector{<:AbstractFloat})
	update!(mpGLM, Opt, Î³, concatenatedÎ¸)
	return -Opt.Q[1]
end

"""
	expectation_âˆ‡negloglikelihood!(g, mpGLM, Opt, Î³, concatenatedÎ¸)

Compute the gradient of the negative of the expectation of the log-likelihood of a mixture of Poisson GLM

MODIFIED ARGUMENT
-`g`: gradient

ARGUMENT
-`mpGLM`: a structure containing the parameters, input, and observations a mixture of Poisson GLM
-`Î³`: posterior probabilities of the latent variables
-`Opt`: a structure for maximizing the expectation of the log-likelihood of a mixture of Poisson GLM
-`concatenatedÎ¸`: parameters of a GLM concatenated into a vector
"""
function expectation_âˆ‡negloglikelihood!(g::Vector{<:AbstractFloat},
									mpGLM::MixturePoissonGLM,
									Opt::MixturePoissonGLM_Optimization,
									Î³::Matrix{<:Vector{<:AbstractFloat}},
									concatenatedÎ¸::Vector{<:AbstractFloat})
	update!(mpGLM, Opt, Î³, concatenatedÎ¸)
	for i in eachindex(g)
		g[i] = -Opt.âˆ‡Q[i]
	end
	return nothing
end

"""
	expectation_âˆ‡âˆ‡negloglikelihood!(h, mpGLM, Opt, Î³, concatenatedÎ¸)

Compute the hessian of the negative of the expectation of the log-likelihood of a mixture of Poisson GLM

MODIFIED ARGUMENT
-`h`: hessian

ARGUMENT
-`mpGLM`: a structure containing the parameters, input, and observations a mixture of Poisson GLM
-`Î³`: posterior probabilities of the latent variables
-`Opt`: a structure for maximizing the expectation of the log-likelihood of a mixture of Poisson GLM
-`concatenatedÎ¸`: parameters of a GLM concatenated into a vector
"""
function expectation_âˆ‡âˆ‡negloglikelihood!(h::Matrix{<:AbstractFloat},
									mpGLM::MixturePoissonGLM,
									Opt::MixturePoissonGLM_Optimization,
									Î³::Matrix{<:Vector{<:AbstractFloat}},
									concatenatedÎ¸::Vector{<:AbstractFloat})
	update!(mpGLM, Opt, Î³, concatenatedÎ¸)
	for i in eachindex(h)
		h[i] = -Opt.âˆ‡âˆ‡Q[i]
	end
	return nothing
end

"""
	update!(mpGLM, Opt, Î³, concatenatedÎ¸)

Update the expectation of the log-likelihood of a mixture of Poisson GLM and its gradient and hessian

MODIFIED ARGUMENT
-`mpGLM`: a structure containing the parameters, input, and observations a mixture of Poisson GLM
-`Opt`: a structure for maximizing the expectation of the log-likelihood of a mixture of Poisson GLM

ARGUMENT
-`Î³`: posterior probabilities of the latent variables
-`concatenatedÎ¸`: parameters of a GLM concatenated into a vector
"""
function update!(mpGLM::MixturePoissonGLM,
				Opt::MixturePoissonGLM_Optimization,
				Î³::Matrix{<:Vector{<:AbstractFloat}},
				concatenatedÎ¸::Vector{<:AbstractFloat})
	if concatenatedÎ¸ != Opt.concatenatedÎ¸
		sortparameters!(mpGLM.Î¸, concatenatedÎ¸)
		Opt.concatenatedÎ¸ .= concatenatedÎ¸
		expectation_âˆ‡âˆ‡loglikelihood!(Opt, Î³, mpGLM)
	end
end

"""
    expectation_loglikelihood(Î³, mpGLM, x)

ForwardDiff-compatible computation of the expectation of the log-likelihood of the mixture of Poisson generalized model of one neuron

Ignores the log(y!) term, which does not depend on the parameters

ARGUMENT
-`Î³`: posterior probability of the latent variable
-`mpGLM`: the GLM of one neuron
-`x`: weights of the linear filters of the GLM concatenated as a vector of floating-point numbers

RETURN
-expectation of the log-likelihood of the spike train of one neuron
"""
function expectation_loglikelihood(concatenatedÎ¸::Vector{<:Real},
								   mpGLM::MixturePoissonGLM,
								   Î³::Matrix{<:Vector{<:AbstractFloat}})
	mpGLM = MixturePoissonGLM(concatenatedÎ¸, mpGLM)
    @unpack Î”t, ğ² = mpGLM
    T = length(ğ²)
    Î,K = size(Î³)
    Q = 0.0
    @inbounds for i = 1:Î
	    for k = 1:K
			ğ‹ = linearpredictor(mpGLM,i,k)
            for t = 1:T
				Q += Î³[i,k][t]*poissonloglikelihood(Î”t, ğ‹[t], ğ²[t])
            end
        end
    end
    return Q
end

"""
	expectation_âˆ‡loglikelihood!(âˆ‡, Î³, mpGLM)

Expectation under the posterior probability of the gradient of the log-likelihood

MODIFIED ARGUMENT
-`âˆ‡`: The gradient

UNMODIFIED ARGUMENT
-`Î³`: Joint posterior probability of the accumulator and coupling variable. Î³[i,k][t] corresponds to the i-th accumulator state and the k-th coupling state in the t-th time bin in the trialset.
-`mpGLM`: structure containing information for the mixture of Poisson GLM for one neuron

EXAMPLE
```julia-rep
julia> using FHMDDM, Random
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_04_27_test/data.mat"; randomize=true);
julia> mpGLM = model.trialsets[1].mpGLMs[1]
julia> concatenatedÎ¸, indexÎ¸ = FHMDDM.concatenateparameters(mpGLM.Î¸)
julia> Î³ = FHMDDM.randomposterior(mpGLM; rng=MersenneTwister(1234))
julia> ghand = similar(concatenatedÎ¸)
julia> FHMDDM.expectation_âˆ‡loglikelihood!(ghand, indexÎ¸, Î³, mpGLM)
julia> using ForwardDiff
julia> f(x) = FHMDDM.expectation_loglikelihood(x, mpGLM, Î³)
julia> gauto = ForwardDiff.gradient(f, concatenatedÎ¸)
julia> maximum(abs.(gauto .- ghand))
```
"""
function expectation_âˆ‡loglikelihood!(âˆ‡Q::Vector{<:Real},
									indexÎ¸::GLMÎ¸,
	                                Î³::Matrix{<:Vector{<:Real}},
	                                mpGLM::MixturePoissonGLM)
	@unpack Î”t, ğ‡, ğ”, ğ•, dğ›_dB, Î¸, ğ² = mpGLM
	Î = size(Î³,1)
	K = length(mpGLM.Î¸.ğ¯)
	T = length(ğ²)
	âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚– = collect(zeros(T) for k=1:K)
	âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–â¨€dÎ¾áµ¢_dB = collect(zeros(T) for k=1:K)
	@inbounds for i = 1:Î
		for k = 1:K
			ğ‹ = linearpredictor(mpGLM,i,k)
			for t=1:T
				dQáµ¢â‚–_dLáµ¢â‚– = Î³[i,k][t] * differentiate_loglikelihood_wrt_linearpredictor(Î”t, ğ‹[t], ğ²[t])
				âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–[k][t] += dQáµ¢â‚–_dLáµ¢â‚–
				âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–â¨€dÎ¾áµ¢_dB[k][t] += dQáµ¢â‚–_dLáµ¢â‚–*dğ›_dB[i]
			end
		end
	end
	âˆ‘áµ¢â‚–_dQáµ¢â‚–_dLáµ¢â‚– = sum(âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–)
	ğ‡áµ€, ğ”áµ€, ğ•áµ€ = transpose(ğ‡), transpose(ğ”), transpose(ğ•)
	âˆ‡Q[indexÎ¸.ğ¡] = ğ‡áµ€*âˆ‘áµ¢â‚–_dQáµ¢â‚–_dLáµ¢â‚–
	@inbounds for k = 1:K
		âˆ‡Q[indexÎ¸.ğ®[k]] = ğ”áµ€*âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–[k]
		âˆ‡Q[indexÎ¸.ğ¯[k]] = ğ•áµ€*âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–â¨€dÎ¾áµ¢_dB[k]
		âˆ‡Q[indexÎ¸.ğ°[k]] = sum(âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–[k])
	end
	return nothing
end

"""
    expectation_âˆ‡âˆ‡loglikelihood(Opt, Î³, mpGLM)

Compute the log-likelihood of a Poisson mixture GLM and its first and second derivatives

MODIFIED ARGUMENT
-`âˆ‡âˆ‡Q`: Hessian matrix

UNMODIFIED ARGUMENT
-`Î³`: posterior probabilities of the latents
-`mpGLM`: the Poisson mixture GLM of one neuron
-`x`: filters of the Poisson mixture GLM

RETURN
-nothing

EXAMPLE
```julia-rep
julia> using FHMDDM, Random
julia> model = Model("/mnt/cup/labs/brody/tzluo/analysis_data/analysis_2022_04_27_test/data.mat"; randomize=true);
julia> mpGLM = model.trialsets[1].mpGLMs[1]
julia> concatenatedÎ¸, indexÎ¸ = FHMDDM.concatenateparameters(mpGLM.Î¸)
julia> Opt = FHMDDM.MixturePoissonGLM_Optimization(concatenatedÎ¸=fill(NaN, length(concatenatedÎ¸)), indexÎ¸=indexÎ¸)
julia> Î³ = FHMDDM.randomposterior(mpGLM; rng=MersenneTwister(1234))
julia> FHMDDM.expectation_âˆ‡âˆ‡loglikelihood!(Opt, Î³, mpGLM)
julia> using ForwardDiff
julia> f(x) = FHMDDM.expectation_loglikelihood(x, mpGLM, Î³)
julia> fauto = f(concatenatedÎ¸)
julia> gauto = ForwardDiff.gradient(f, concatenatedÎ¸)
julia> hauto = ForwardDiff.hessian(f, concatenatedÎ¸)
julia> abs(fauto - Opt.Q[1])
julia> maximum(abs.(gauto .- Opt.âˆ‡Q))
julia> maximum(abs.(hauto .- Opt.âˆ‡âˆ‡Q))
```
"""
function expectation_âˆ‡âˆ‡loglikelihood!(Opt::MixturePoissonGLM_Optimization,
									Î³::Matrix{<:Vector{<:AbstractFloat}},
									mpGLM::MixturePoissonGLM)
	@unpack indexÎ¸, Q, âˆ‡Q, âˆ‡âˆ‡Q = Opt
    @unpack Î”t, ğ‡, ğ”, ğ•, dğ›_dB, Î¸, ğ² = mpGLM
	dğ›_dBÂ² = dğ›_dB.^2
	Î,K = size(Î³)
	T = length(ğ²)
	âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚– = collect(zeros(T) for k=1:K)
	âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–â¨€dÎ¾áµ¢_dB = collect(zeros(T) for k=1:K)
	âˆ‘áµ¢_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â² = collect(zeros(T) for k=1:K)
	âˆ‘áµ¢_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dB = collect(zeros(T) for k=1:K)
	âˆ‘áµ¢_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dBÂ² = collect(zeros(T) for k=1:K)
	Q[1] = 0.0
	âˆ‡Q .= 0.0
	âˆ‡âˆ‡Q .= 0.0
	@inbounds for i = 1:Î
		for k = 1:K
			ğ‹ = linearpredictor(mpGLM,i,k)
			for t=1:T
				dÂ²â„“_dLÂ², dâ„“_dL, â„“ = differentiate_loglikelihood_twice_wrt_linearpredictor(Î”t, ğ‹[t], ğ²[t])
				Q[1] += Î³[i,k][t]*â„“
				dQáµ¢â‚–_dLáµ¢â‚– = Î³[i,k][t] * dâ„“_dL
				âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–[k][t] += dQáµ¢â‚–_dLáµ¢â‚–
				âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–â¨€dÎ¾áµ¢_dB[k][t] += dQáµ¢â‚–_dLáµ¢â‚–*dğ›_dB[i]
				dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â² = Î³[i,k][t] * dÂ²â„“_dLÂ²
				âˆ‘áµ¢_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²[k][t] += dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²
				âˆ‘áµ¢_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dB[k][t] += dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²*dğ›_dB[i]
				âˆ‘áµ¢_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dBÂ²[k][t] += dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²*dğ›_dBÂ²[i]
			end
		end
	end
	ğ‡áµ€, ğ”áµ€, ğ•áµ€ = transpose(ğ‡), transpose(ğ”), transpose(ğ•)
	âˆ‘áµ¢â‚–_dQáµ¢â‚–_dLáµ¢â‚– = sum(âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–)
	âˆ‡Q[indexÎ¸.ğ¡] = ğ‡áµ€*âˆ‘áµ¢â‚–_dQáµ¢â‚–_dLáµ¢â‚–
	@inbounds for k = 1:K
		âˆ‡Q[indexÎ¸.ğ®[k]] = ğ”áµ€*âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–[k]
		âˆ‡Q[indexÎ¸.ğ¯[k]] = ğ•áµ€*âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–â¨€dÎ¾áµ¢_dB[k]
		âˆ‡Q[indexÎ¸.ğ°[k]] = sum(âˆ‘áµ¢_dQáµ¢â‚–_dLáµ¢â‚–[k])
	end
	âˆ‘áµ¢â‚–_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â² = sum(âˆ‘áµ¢_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²)
	âˆ‡âˆ‡Q[indexÎ¸.ğ¡, indexÎ¸.ğ¡] = ğ‡áµ€*(âˆ‘áµ¢â‚–_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â².*ğ‡)
	@inbounds for k=1:K
		âˆ‡âˆ‡Q[indexÎ¸.ğ¡, indexÎ¸.ğ®[k]] = ğ‡áµ€*(âˆ‘áµ¢_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²[k].*ğ”)
		âˆ‡âˆ‡Q[indexÎ¸.ğ¡, indexÎ¸.ğ¯[k]] = ğ‡áµ€*(âˆ‘áµ¢_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dB[k].*ğ•)
		âˆ‡âˆ‡Q[indexÎ¸.ğ¡, indexÎ¸.ğ°[k]] = ğ‡áµ€*âˆ‘áµ¢_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²[k]
		âˆ‡âˆ‡Q[indexÎ¸.ğ®[k], indexÎ¸.ğ®[k]] = ğ”áµ€*(âˆ‘áµ¢_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²[k].*ğ”)
		âˆ‡âˆ‡Q[indexÎ¸.ğ®[k], indexÎ¸.ğ¯[k]] = ğ”áµ€*(âˆ‘áµ¢_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dB[k].*ğ•)
		âˆ‡âˆ‡Q[indexÎ¸.ğ®[k], indexÎ¸.ğ°[k]] = ğ”áµ€*(âˆ‘áµ¢_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²[k])
		âˆ‡âˆ‡Q[indexÎ¸.ğ¯[k], indexÎ¸.ğ¯[k]] = ğ•áµ€*(âˆ‘áµ¢_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dBÂ²[k].*ğ•)
		âˆ‡âˆ‡Q[indexÎ¸.ğ¯[k], indexÎ¸.ğ°[k]] = ğ•áµ€*(âˆ‘áµ¢_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²â¨€dÎ¾áµ¢_dB[k])
		âˆ‡âˆ‡Q[indexÎ¸.ğ°[k], indexÎ¸.ğ°[k]] = sum(âˆ‘áµ¢_dÂ²Qáµ¢â‚–_dLáµ¢â‚–Â²[k])
	end
	@inbounds for q=1:size(âˆ‡âˆ‡Q,1) # update the lower triangle
		for r=q+1:size(âˆ‡âˆ‡Q,2)
			âˆ‡âˆ‡Q[r,q] = âˆ‡âˆ‡Q[q,r]
		end
	end
	return nothing
end

"""
    poissonloglikelihood

Differentiate the log-likelihood of a Poisson GLM with respect to the linear predictor

The Poisson GLM is assumed to have a a softplus nonlinearity

ARGUMENT
-`Î”t`: duration of time step
-`L`: linear predictor
-`y`: observation

RETURN
-log-likelihood
"""
function poissonloglikelihood(Î”t::AbstractFloat, L::Real, y::Integer)
    Î» = softplus(L)
	if y == 0
		-Î»*Î”t
	elseif y == 1
		log(Î») - Î»*Î”t
	else
		y*log(Î») - Î»*Î”t
	end
end

"""
    differentiate_loglikelihood_twice_wrt_linearpredictor

First derivative of the log-likelihood of a Poisson GLM with respect to the linear predictor

The Poisson GLM is assumed to have a a softplus nonlinearity

ARGUMENT
-`Î”t`: duration of time step
-`L`: linear predictor
-`y`: observation

RETURN
-first derivative of the log-likelihood with respect to the linear predictor
"""
function differentiate_loglikelihood_wrt_linearpredictor(Î”t::AbstractFloat, L::Real, y::Integer)
    fâ‚ = logistic(L)
	if y > 0
        if L > -100.0
			fâ‚€ = softplus(L)
            fâ‚*(y/fâ‚€ - Î”t)
        else
            y - fâ‚*Î”t # the limit of logistic(x)/softplus(x) as x goes to -âˆ is 1
        end
    else
        -fâ‚*Î”t
    end
end

"""
    differentiate_loglikelihood_twice_wrt_linearpredictor

Second derivative of the log-likelihood of a Poisson GLM with respect to the linear predictor

The Poisson GLM is assumed to have a a softplus nonlinearity

ARGUMENT
-`Î”t`: duration of time step
-`L`: linear predictor
-`y`: observation

RETURN
-second derivative of the log-likelihood with respect to the linear predictor
-first derivative of the log-likelihood with respect to the linear predictor
-log-likelihood
"""
function differentiate_loglikelihood_twice_wrt_linearpredictor(Î”t::AbstractFloat, L::Real, y::Integer)
	fâ‚€ = softplus(L)
    fâ‚ = logistic(L)
    fâ‚‚ = fâ‚*(1.0-fâ‚)
	if y == 0
		â„“ = -fâ‚€*Î”t
	elseif y == 1
		â„“ = log(fâ‚€) - fâ‚€*Î”t
	else
		â„“ = y*log(fâ‚€) - fâ‚€*Î”t
	end
	if y > 0
        if L > -100.0
            dâ„“_dL= fâ‚*(y/fâ‚€ - Î”t)
        else
            dâ„“_dL = y - fâ‚*Î”t # the limit of logistic(x)/softplus(x) as x goes to -âˆ is 1
        end
    else
        dâ„“_dL = -fâ‚*Î”t
    end
    if y > 0 && L > -50.0
        dÂ²â„“_dLÂ² = y*(fâ‚€*fâ‚‚ - fâ‚^2)/fâ‚€^2 - fâ‚‚*Î”t # the limit of the second term is 0 as xw goes to -âˆ
    else
        dÂ²â„“_dLÂ² = -fâ‚‚*Î”t
    end
	return dÂ²â„“_dLÂ², dâ„“_dL, â„“
end

"""
	GLMÎ¸(K, ğ‡, ğ”, ğ•)

Randomly initiate the parameters for a mixture of Poisson generalized linear model

ARGUMENT
-`K`: number of coupling states
-`ğ‡`: time-varying inputs from spike history
-`ğ”`: time-varying inputs from trial events
-`ğ•`: time-varying inputs from the accumulator

OUTPUT
-an instance of `GLMÎ¸`
"""
function GLMÎ¸(K::Integer,
			ğ‡::Matrix{<:AbstractFloat},
			ğ”::Matrix{<:AbstractFloat},
			ğ•::Matrix{<:AbstractFloat})
	nğ¡ = size(ğ‡,2)
	nğ® = size(ğ”,2)
	nğ¯ = size(ğ•,2)
	Î¸ = GLMÎ¸(ğ¡ = 1.0 .- 2.0.*rand(nğ¡),
			 ğ° = 1.0 .- 2.0.*rand(K),
			 ğ® = collect(1.0 .- 2.0.*rand(nğ®) for k=1:K),
			 ğ¯ = collect(1.0 .- 2.0.*rand(nğ¯) for k=1:K))
end

"""
	GLMÎ¸(glmÎ¸, elementtype)

Create an uninitialized instance of GLMÎ¸ with the given element type.

This is for using ForwardDiff

ARGUMENT
-`glmÎ¸`: an instance of GLMÎ¸
-`elementtype`: type of the element in each field of GLMÎ¸

RETURN
-an instance of GLMÎ¸
"""
function GLMÎ¸(glmÎ¸::GLMÎ¸, elementtype)
	GLMÎ¸(ğ¡ = zeros(elementtype, length(glmÎ¸.ğ¡)),
		 ğ® = collect(zeros(elementtype, length(ğ®)) for ğ® in glmÎ¸.ğ®),
		 ğ¯ = collect(zeros(elementtype, length(ğ¯)) for ğ¯ in glmÎ¸.ğ¯),
		 ğ° = zeros(elementtype, length(glmÎ¸.ğ°)))
end

"""
    transformaccumulator

Nonlinearly transform the normalized values of the accumulator

ARGUMENT
-`Î¾`: value of the accumulator: expected to be between -1 and 1
-`b`: parameter specifying the transformation

RETURN
-transformed value of the accumulator
"""
function transformaccumulator(b::Real, Î¾::Real)
    if b == 0.0
        Î¾
    else
        if Î¾ < 0
            if b > 709.0 # 709 is close to which exp returns Inf for a 64-bit floating point number
                Î¾ == -1.0 ? -1.0 : 0.0
            else
                -expm1(-b*Î¾)/expm1(b)
                # (exp(-b*Î¾)-1.0)/(1.0-exp(b))
            end
        elseif Î¾ > 0
            if b > 709.0
                Î¾ == 1.0 ? 1.0 : 0.0
            else
                expm1(b*Î¾)/expm1(b)
                # (1.0-exp(b*Î¾))/(1.0-exp(b))
            end
        else
            0.0
        end
    end
end

"""
    dtransformaccumulator

Derivative of the nonlinear transformation of the normalized values of the accumulator with respect to b

ARGUMENT
-`Î¾`: value of the accumulator: expected to be between -1 and 1
-`b`: parameter specifying the transformation

RETURN
-transformed value of the accumulator
"""
function dtransformaccumulator(b::Real, Î¾::Real)
    if Î¾ == -1.0 || Î¾ == 0.0 || Î¾ == 1.0 || b > 709.0 # 709 is close to which exp returns Inf for a 64-bit floating point number
        0.0
    elseif abs(b) < 1e-6
        Î¾ < 0 ? (-Î¾^2-Î¾)/2 : (Î¾^2-Î¾)/2
    elseif Î¾ < 0
        eáµ‡ = exp(b)
        eáµ‡m1 = expm1(b)
        eâ»áµ‡Ë£ = exp(-b*Î¾)
        eâ»áµ‡Ë£m1 = expm1(-b*Î¾)
        if b < 1
            (Î¾*eâ»áµ‡Ë£*eáµ‡m1 + eâ»áµ‡Ë£m1*eáµ‡)/eáµ‡m1^2
        else
            Î¾*eâ»áµ‡Ë£/eáµ‡m1 + eâ»áµ‡Ë£m1/(eáµ‡-2+exp(-b))
        end
    elseif Î¾ > 0
        eáµ‡ = exp(b)
        eáµ‡m1 = expm1(b)
        eáµ‡Ë£ = exp(b*Î¾)
        eáµ‡Ë£m1 = expm1(b*Î¾)
        if b < 1
            Î¾*eáµ‡Ë£/eáµ‡m1 - eáµ‡Ë£m1*eáµ‡/eáµ‡m1^2
        else
            Î¾*eáµ‡Ë£/eáµ‡m1 - eáµ‡Ë£m1/(eáµ‡-2+exp(-b))
        end
    end
end
